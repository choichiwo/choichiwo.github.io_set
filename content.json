{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Starthihih Create a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/29/hello-world/"},{"title":"titanic","text":"Kaggle API 설치 Google Colab에서 Kaggle API를 불러오려면 다음 소스코드를 실행한다. 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) 1. Kaggle Token 다운로드 Kaggle에서 API Token을 다운로드 받는다. [Kaggle]-[My Account]-[API]-[Create New API Token]을 누르면 kaggle.json 파일이 다운로드 된다. 이 파일을 바탕화면에 옮긴 뒤, 아래 코드(토큰을 실행시키는 코드)를 실행 시킨다. 12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 66 bytes 아래 코드는 실행됬는지 확인하는 코드 1ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 1 2. Kaggle 데이터 불러오기 먼저 kaggle competition list를 불러온다. 1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 134 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 185 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 315 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2356 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 18058 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4536 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 390 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1184 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 152 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 1466 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False competitive-data-science-predict-future-sales 2020-12-31 23:59:00 Playground Kudos 9343 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 43 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 193 False hashcode-drone-delivery 2020-12-14 23:59:00 Playground Knowledge 79 False cdp-unlocking-climate-solutions 2020-12-02 23:59:00 Analytics $91,000 0 False lish-moa 2020-11-30 23:59:00 Research $30,000 3395 False google-football 2020-11-30 23:59:00 Featured $6,000 916 False conways-reverse-game-of-life-2020 2020-11-30 23:59:00 Playground Swag 131 False lyft-motion-prediction-autonomous-vehicles 2020-11-25 23:59:00 Featured $30,000 778 False 1!kaggle competitions download -c titanic Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) Downloading gender_submission.csv to /content 0% 0.00/3.18k [00:00&lt;?, ?B/s] 100% 3.18k/3.18k [00:00&lt;00:00, 6.86MB/s] Downloading test.csv to /content 0% 0.00/28.0k [00:00&lt;?, ?B/s] 100% 28.0k/28.0k [00:00&lt;00:00, 23.4MB/s] Downloading train.csv to /content 0% 0.00/59.8k [00:00&lt;?, ?B/s] 100% 59.8k/59.8k [00:00&lt;00:00, 52.2MB/s] ls는 디렉터리(파일,경로) 내의 데이터 파일을 보여주는 명령어 1!ls gender_submission.csv sample_data test.csv train.csv 현재 총 4개의 데이터를 다운로드 받았다. gender_submission.csv sample_data test.csv train.csv 3. 캐글 데이터 수집 및 EDA 우선 데이터를 수집하기에 앞서서 EDA에 관한 필수 패키지를 설치하자. 1234567891011import pandas as pd # 데이터 가공 변환(deploy)import pandas_profiling # 보고서 기능 import numpy as np # 수치연산 &amp; 배열 import matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use('fivethirtyeight')import warningswarnings.filterwarnings('ignore')%matplotlib inlinefrom IPython.core.display import display, HTML (1) 데이터 불러오기 지난 시간에 받은 데이터가 총 4개임을 확인했다. gender_submission.csv sample_data test.csv train.csv 여기에서는 우선 test.csv &amp; train.csv 파일을 받도록 한다. 123train = pd.read_csv('train.csv')test = pd.read_csv('test.csv')print(&quot;data import is done&quot;) data import is done (2) 데이터 확인 Kaggle 데이터를 불러오면 우선 확인해야 하는 것은 데이터셋의 크기다. 변수의 갯수 Numeric 변수 &amp; Categorical 변수의 개수 등을 파악해야 한다. Point 1 - train데이터에서 굳이 훈련데이터와 테스트 데이터를 구분할 필요는 없다. 보통 Kaggle에서는 테스트 데이터를 주기적으로 업데이트 해준다. Point 2 - 보통 test 데이터의 변수의 개수가 하나 더 작다. 1train.shape, test.shape ((891, 12), (418, 11)) 그 후 train데이터의 상위 5개의 데이터만 확인한다. 1display(train.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 아래 코드는 train.csv을 data로 변환 한다. 1data=pd.read_csv('train.csv') 1data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1data.isnull().sum() PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 The Age, Cabin and Embarked 에 대한 결측값을 구한다. 얼마나 살아남았나?1234567f,ax=plt.subplots(1,2,figsize=(18,8))data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)ax[0].set_title('Survived')ax[0].set_ylabel('')sns.countplot('Survived',data=data,ax=ax[1])ax[1].set_title('Survived')plt.show() 그 사고에서 살아남은 승객이 많지 않다는 것은 명백하다. 훈련장 891명 중 350명 정도만 살아남았다. 즉, 전체 훈련장 중 38.4%만이 추락사고에서 살아남았다. 우리는 데이터로부터 더 나은 통찰력을 얻고 어떤 범주의 승객들이 살아남았는지 그리고 누가 살아남지 않았는지 보기 위해 더 많은 정보를 캐내야 한다. 우리는 데이터 세트의 다른 특징들을 사용하여 생존율을 확인하도록 노력할 것이다. Sex, Port Of Embarcation, Age 등이 특징이다. 먼저 다양한 유형의 특징을 이해하십시오. 피쳐 유형 범주형 피쳐:범주형 변수는 두 개 이상의 범주를 가진 변수 중 하나이며, 해당 형상의 각 값은 범주별로 분류할 수 있다.예를 들어, 성별은 두 개의 범주(남성과 여성)를 갖는 범주형 변수다. 이제 우리는 그러한 변수들에 대해 어떠한 순서도 정렬할 수 없다. 이 변수들은 공칭 변수라고도 한다. 데이터 집합의 범주형 특징: 성별, 도착. 순서형 피쳐:순서형 변수는 범주형 값과 비슷하지만, 그 값들 사이의 차이는 우리가 상대적인 순서나 값들 사이의 정렬을 가질 수 있다는 것이다. 예를 들어: 높이에 높은 값, 중간 값, 짧은 값과 같은 특징이 있다면 높이는 서수 변수다. 여기서 우리는 변수에서 상대적인 분류법을 가질 수 있다. 데이터 집합의 순서형 기능: PClass 연속 기능:형상은 형상 열의 두 점 또는 최소값 또는 최대값 사이에 값을 취할 수 있는 경우 지속된다고 한다. 데이터 집합의 지속적인 기능: 나이 특성 분석성–&gt; 범주형 특징 1data.groupby(['Sex','Survived'])['Survived'].count() Sex Survived female 0 81 1 233 male 0 468 1 109 Name: Survived, dtype: int64 123456f,ax=plt.subplots(1,2,figsize=(18,8))data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Sex:Survived vs Dead')plt.show() 배에 타고 있는 남자들의 수가 여자들의 수보다 훨씬 많다. 그러나 여전히 구조된 여성의 수는 구조된 남성의 수보다 거의 두 배나 많다. 배에 타고 있는 여성의 생존율은 약 75%인 반면 남성은 약 18-19%이다. 이것은 모델링을 위해 매우 중요한 특징으로 보인다. 하지만 그게 최고일까? 다른 기능을 확인해 봅시다. Pclass –&gt; 순서형 피쳐1pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r') #T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row0_col0,#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row1_col1,#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row1_col2{ background-color: #ffff66; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row0_col1{ background-color: #cee666; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row0_col2{ background-color: #f4fa66; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row1_col0{ background-color: #f6fa66; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row2_col0{ background-color: #60b066; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row2_col1{ background-color: #dfef66; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row2_col2{ background-color: #90c866; color: #000000; }#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row3_col0,#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row3_col1,#T_ceb04e82_1e4a_11eb_b25f_0242ac1c0002row3_col2{ background-color: #008066; color: #f1f1f1; } Survived 0 1 All Pclass 1 80 136 216 2 97 87 184 3 372 119 491 All 549 342 891 1234567f,ax=plt.subplots(1,2,figsize=(18,8))data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])ax[0].set_title('Number Of Passengers By Pclass')ax[0].set_ylabel('Count')sns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Pclass:Survived vs Dead')plt.show() P클래스사람은 돈이 모든 것을 살 수 없다고 말한다. 그러나 우리는 구조하는 동안 P클래스 1의 승객이 매우 높은 우선순위를 부여받았다는 것을 분명히 알 수 있다. P클래스 3의 탑승객 수가 훨씬 더 많았지만, 여전히 탑승객들로부터 생존하는 사람들의 수는 약 25%로 매우 낮다. P클래스의 경우 1% 생존율이 약 63%인 반면 P클래스2의 경우 약 48%이다. 그래서 돈과 지위가 중요하다. 그런 물질적인 세계. 좀 더 자세히 살펴보고 다른 흥미로운 관찰을 확인해 봅시다. 성과 P클래스 통해 생존율을 확인해보자. 1pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r') #T_2327443c_1e4d_11eb_b25f_0242ac1c0002row0_col0,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row0_col1,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row0_col3,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row3_col2{ background-color: #ffff66; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row0_col2,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row1_col2{ background-color: #f1f866; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row1_col0{ background-color: #96cb66; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row1_col1{ background-color: #a3d166; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row1_col3{ background-color: #cfe766; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row2_col0{ background-color: #a7d366; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row2_col1,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row2_col3{ background-color: #85c266; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row2_col2{ background-color: #6eb666; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row3_col0{ background-color: #cde666; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row3_col1{ background-color: #f0f866; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row3_col3{ background-color: #f7fb66; color: #000000; }#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row4_col0,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row4_col1,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row4_col2,#T_2327443c_1e4d_11eb_b25f_0242ac1c0002row4_col3{ background-color: #008066; color: #f1f1f1; } Pclass 1 2 3 All Sex Survived female 0 3 6 72 81 1 91 70 72 233 male 0 77 91 300 468 1 45 17 47 109 All 216 184 491 891 12sns.factorplot('Pclass','Survived',hue='Sex',data=data) #성에 따른 Pclass 생존율plt.show() 이 경우 요인 그림은 범주형 값의 분리가 쉽기 때문에 요인 그림을 사용한다. 크로스탭과 인자 플롯을 보면 Pclass1 여성 94명 중 3명만이 사망했기 때문에 Pclass1 여성 생존율이 약 95~96%라고 쉽게 유추할 수 있다. Pclass와 관계 없이 구조하는 동안 여성에게 우선권이 주어졌다는 것은 명백하다. 심지어 Pclass1 출신의 남성들도 생존율이 매우 낮다. Pclass도 중요한 특징인 것 같다. 다른 특징을 분석해보자. 나이 –&gt; 지속적 특징123print('Oldest Passenger was of:',data['Age'].max(),'Years') #최대 나이 승객print('Youngest Passenger was of:',data['Age'].min(),'Years') #최소 나이 승객 print('Average Age on the ship:',data['Age'].mean(),'Years') #평균 나이 승객 Oldest Passenger was of: 80.0 Years Youngest Passenger was of: 0.42 Years Average Age on the ship: 29.69911764705882 Years 12345678f,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot(&quot;Pclass&quot;,&quot;Age&quot;, hue=&quot;Survived&quot;, data=data,split=True,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived') #제목ax[0].set_yticks(range(0,110,10)) sns.violinplot(&quot;Sex&quot;,&quot;Age&quot;, hue=&quot;Survived&quot;, data=data,split=True,ax=ax[1])ax[1].set_title('Sex and Age vs Survived') #제목ax[1].set_yticks(range(0,110,10))plt.show() 관측치:1)P클래스에 따라 자녀 수가 증가하고 10세 미만(즉, 자녀)의 패시네거 생존율은 P클래스에 관계없이 양호한 것으로 보인다. 2)Pclass1에서 20-50세의 승객의 생존 가능성은 높고 여성에게는 더욱 좋다. 3)남성의 경우 나이가 들수록 생존 가능성이 줄어든다. 앞에서 보았듯이 에이지 기능은 177개의 null 값을 가지고 있다. 이러한 NaN 값을 대체하기 위해 데이터 집합의 평균 연령을 할당할 수 있다. 그런데 문제는 나이 차이가 많은 사람들이 많았다는 겁니다. 우리는 단지 평균 나이 29세인 4살 아이를 배정할 수 없다. 승객이 어떤 연령대인지 알 수 있는 방법은 없을까? 이름 기능을 확인할 수 있어. 그 특징을 살펴보면, 우리는 그 이름들이 Mr. 또는 Mrs.와 같은 경례를 가지고 있다는 것을 알 수 있다. 따라서 우리는 Mr. Mrs와 Mrs의 평균값을 각 그룹에 할당할 수 있다. ‘’이름에 무엇이 있는가?”—&gt; 특징 :p123data['Initial']=0for i in data: data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #인사말을 꺼내자. 자, 이제 Regex를 사용합시다. [A-Za-z]+).. 즉, A-Z 또는 a-z 사이에 있는 문자열을 찾고, 그 뒤에 .(점)이 있는 문자열을 찾는 겁니다. 그래서 우리는 이름에서 이니셜을 성공적으로 추출했다. 1pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #성으로 이니셜 확인 #T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col0,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col1,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col3,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col4,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col5,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col7,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col8,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col12,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col15,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col16,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col2,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col6,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col9,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col10,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col11,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col13,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col14{ background-color: #ffff66; color: #000000; }#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col2,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col6,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col9,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col10,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col11,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col13,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row0_col14,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col0,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col1,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col3,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col4,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col5,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col7,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col8,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col12,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col15,#T_392ab74e_1e4f_11eb_b25f_0242ac1c0002row1_col16{ background-color: #008066; color: #f1f1f1; } Initial Capt Col Countess Don Dr Jonkheer Lady Major Master Miss Mlle Mme Mr Mrs Ms Rev Sir Sex female 0 0 1 0 1 0 1 0 0 182 2 1 0 125 1 0 0 male 1 2 0 1 6 1 0 2 40 0 0 0 517 0 0 6 1 좋아, Mlle이나 Mme와 같은 철자가 틀린 이니셜이 있는데 Miss를 나타낸다. 나는 그것들을 미스나 다른 가치에 대해서도 같은 것으로 대체할 것이다.아래 코드 참조 1data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True) 1data.groupby('Initial')['Age'].mean() #이니셜별 평균 연령 확인 Initial Master 4.574167 Miss 21.860000 Mr 32.739609 Mrs 35.981818 Other 45.888889 Name: Age, dtype: float64 NaN 연령 채우기 123456## 평균 연령의 Ceil 값에 NaN 값 할당data.loc[(data.Age.isnull())&amp;(data.Initial=='Mr'),'Age']=33data.loc[(data.Age.isnull())&amp;(data.Initial=='Mrs'),'Age']=36data.loc[(data.Age.isnull())&amp;(data.Initial=='Master'),'Age']=5data.loc[(data.Age.isnull())&amp;(data.Initial=='Miss'),'Age']=22data.loc[(data.Age.isnull())&amp;(data.Initial=='Other'),'Age']=46 1data.Age.isnull().any() #그래서 마침내 null 값이 남지 않게 되었다. False 12345678910f,ax=plt.subplots(1,2,figsize=(20,10))data[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')ax[0].set_title('Survived= 0')x1=list(range(0,85,5))ax[0].set_xticks(x1)data[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')ax[1].set_title('Survived= 1')x2=list(range(0,85,5))ax[1].set_xticks(x2)plt.show() 관측치:1)아기들 (&lt;5)은 대량으로 생존되었다. 2)가장 나이가 많은 승객은 생존했다(80년). 3)최대 사망자는 30~40세였다. 12sns.factorplot('Pclass','Survived',col='Initial',data=data)plt.show() 따라서 부녀자 우선 정책은 계층에 관계없이 적용된다. 승선 –&gt; 범주형 값1pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r') #T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row0_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row1_col2{ background-color: #fcfe66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row0_col1{ background-color: #d2e866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row0_col2{ background-color: #f2f866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row0_col3{ background-color: #d8ec66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row0_col4,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row2_col3{ background-color: #e8f466; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row1_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row3_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row3_col1,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row3_col2,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row3_col3,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row3_col4,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row4_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row4_col2,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row4_col3,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row4_col4{ background-color: #ffff66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row1_col1,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row6_col0{ background-color: #f9fc66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row1_col3,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row1_col4{ background-color: #fbfd66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row2_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row5_col1{ background-color: #e6f266; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row2_col1{ background-color: #f0f866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row2_col2{ background-color: #eef666; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row2_col4,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row7_col0{ background-color: #edf666; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row4_col1{ background-color: #fefe66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row5_col0{ background-color: #e3f166; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row5_col2{ background-color: #ecf666; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row5_col3{ background-color: #f8fc66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row5_col4{ background-color: #ebf566; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row6_col1{ background-color: #cde666; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row6_col2{ background-color: #e4f266; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row6_col3{ background-color: #bede66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row6_col4{ background-color: #dbed66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row7_col1{ background-color: #bdde66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row7_col2{ background-color: #d3e966; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row7_col3,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row8_col1{ background-color: #dcee66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row7_col4{ background-color: #d1e866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row8_col0{ background-color: #52a866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row8_col2{ background-color: #81c066; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row8_col3{ background-color: #b0d866; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row8_col4{ background-color: #9acc66; color: #000000; }#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row9_col0,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row9_col1,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row9_col2,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row9_col3,#T_40b4596e_1e5b_11eb_b25f_0242ac1c0002row9_col4{ background-color: #008066; color: #f1f1f1; } Sex female male All Survived 0 1 0 1 Embarked Pclass C 1 1 42 25 17 85 2 0 7 8 2 17 3 8 15 33 10 66 Q 1 0 1 1 0 2 2 0 2 1 0 3 3 9 24 36 3 72 S 1 2 46 51 28 127 2 6 61 82 15 164 3 55 33 231 34 353 All 81 231 468 109 889 항구별 생존 가능성 1234sns.factorplot('Embarked','Survived',data=data)fig=plt.gcf()fig.set_size_inches(5,3)plt.show() 포트 C의 생존 가능성은 0.55 전후로 가장 높은 반면 S의 생존 가능성은 가장 낮다. 1234567891011f,ax=plt.subplots(2,2,figsize=(20,15))sns.countplot('Embarked',data=data,ax=ax[0,0]) #sns = seabornax[0,0].set_title('No. Of Passengers Boarded')sns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])ax[0,1].set_title('Male-Female Split for Embarked')sns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])ax[1,0].set_title('Embarked vs Survived')sns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])ax[1,1].set_title('Embarked vs Pclass')plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show() 관측치:1)S에서 탑승한 최대 탑승객. 그들 대부분은 Pclass3 출신이다. 2)C에서 온 승객들은 상당부분 살아남은 것으로 보아 운이 좋은 것으로 보인다. 그 이유는 아마도 모든 Pclass1과 Pclass2 승객을 구조했을 것이다. 3)승객 S는 부자들의 대다수가 탑승한 항구를 바라본다. 여전히 생존 가능성은 낮은데, 그것은 81% 정도 되는 Pclass3의 많은 승객들이 살아남지금은 Pclass3의 많은 승객들이 살아남지 못했기 때문이다. 4)항구 Q의 탑승객은 거의 95%가 Pclass3 출신이었다. 12sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)plt.show() 관측치:1)Pclass1과 Pclass2의 여성은 Pclass와 관계없이 생존 확률은 거의 1이다. 2)남녀 모두의 생존율이 매우 낮기 때문에 Pclass3 Passenger에게는 포트S가 매우 불행해 보인다.(돈 문제) 3)포트 Q는 거의 모두가 Pclass 3에서 온 것처럼 남자에게는 가장 어울리지 않는 것 같다. 채우기 시작 NaN우리는 최대 승객들이 S항에서 탑승하는 것을 보았듯이 NaN을 S로 대체한다. 1data['Embarked'].fillna('S',inplace=True) 1data.Embarked.isnull().any()# Finally No NaN values False SibSip–&gt;구체적 특징이 특징은 사람이 혼자 있는지 가족과 함께 있는지 여부를 나타낸다. 형제 = 형제, 자매, 의붓동생, 의붓동생, 의붓동생 배우자 = 남편, 아내 1pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r') #T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row0_col0,#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row0_col1{ background-color: #008066; color: #f1f1f1; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row1_col0{ background-color: #c4e266; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row1_col1{ background-color: #77bb66; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row2_col0,#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row4_col0{ background-color: #f9fc66; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row2_col1{ background-color: #f0f866; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row3_col0,#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row3_col1{ background-color: #fbfd66; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row4_col1{ background-color: #fcfe66; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row5_col0,#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row5_col1,#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row6_col1{ background-color: #ffff66; color: #000000; }#T_728ddb78_1e5f_11eb_b25f_0242ac1c0002row6_col0{ background-color: #fefe66; color: #000000; } Survived 0 1 SibSp 0 398 210 1 97 112 2 15 13 3 12 4 4 15 3 5 5 0 8 7 0 1234567f,ax=plt.subplots(1,2,figsize=(20,8))sns.barplot('SibSp','Survived',data=data,ax=ax[0])ax[0].set_title('SibSp vs Survived')sns.factorplot('SibSp','Survived',data=data,ax=ax[1])ax[1].set_title('SibSp vs Survived')plt.close(2)plt.show() 1pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r') #T_83313358_1e5f_11eb_b25f_0242ac1c0002row0_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row0_col1,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row0_col2{ background-color: #008066; color: #f1f1f1; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row1_col0{ background-color: #7bbd66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row1_col1{ background-color: #8ac466; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row1_col2{ background-color: #c6e266; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row2_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row4_col2{ background-color: #f6fa66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row2_col1{ background-color: #eef666; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row2_col2{ background-color: #f8fc66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row3_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row3_col2{ background-color: #fafc66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row3_col1{ background-color: #fdfe66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row4_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row4_col1,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row5_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row5_col1,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row5_col2,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row6_col0,#T_83313358_1e5f_11eb_b25f_0242ac1c0002row6_col1{ background-color: #ffff66; color: #000000; }#T_83313358_1e5f_11eb_b25f_0242ac1c0002row6_col2{ background-color: #fefe66; color: #000000; } Pclass 1 2 3 SibSp 0 137 120 351 1 71 55 83 2 5 8 15 3 3 1 12 4 0 0 18 5 0 0 5 8 0 0 7 관측치:막대 그래프와 요인 그림은 승객이 형제 없이 혼자 탑승한 경우 생존율이 34.5%라는 것을 보여준다. 형제자매 수가 증가하면 그래프는 대략 감소한다. 이게 말이 되네. 즉, 만약 내가 승선하고 있는 가족이 있다면, 나는 먼저 나 자신을 구하지 않고 그들을 구하려고 노력할 것이다. 놀랍게도 5~8인 가족의 생존율은 0%이다. 이유는 Pclass일 수도 있다? 그 이유는 Pclass이다. 십자표는 SibSp&gt;3을 가진 사람이 모두 Pclass3에 있었다는 것을 보여준다. Pclass3(&gt;&gt;3)의 대가족이 모두 사망하는 일이 임박했다. Parch1pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r') #T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row0_col0,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row0_col1,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row0_col2{ background-color: #008066; color: #f1f1f1; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row1_col0{ background-color: #cfe766; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row1_col1{ background-color: #c2e066; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row1_col2{ background-color: #dbed66; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row2_col0{ background-color: #dfef66; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row2_col1{ background-color: #e1f066; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row2_col2{ background-color: #e3f166; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row3_col0,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row4_col1,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row5_col0,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row5_col1,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row6_col0,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row6_col1,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row6_col2{ background-color: #ffff66; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row3_col1{ background-color: #fcfe66; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row3_col2,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row4_col0,#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row4_col2{ background-color: #fefe66; color: #000000; }#T_ccb5b4d6_1e5f_11eb_b25f_0242ac1c0002row5_col2{ background-color: #fdfe66; color: #000000; } Pclass 1 2 3 Parch 0 163 134 381 1 31 32 55 2 21 16 43 3 0 2 3 4 1 0 3 5 0 0 5 6 0 0 1 크로스탭은 다시 더 큰 가족이 Pclass3에 있었다는 것을 보여준다. 1234567f,ax=plt.subplots(1,2,figsize=(20,8))sns.barplot('Parch','Survived',data=data,ax=ax[0])ax[0].set_title('Parch vs Survived')sns.factorplot('Parch','Survived',data=data,ax=ax[1])ax[1].set_title('Parch vs Survived')plt.close(2)plt.show() 관측치:여기에서도 결과는 꽤 비슷하다. 부모를 동반한 승객은 생존 가능성이 더 크다. 하지만 숫자가 늘어날수록 줄어든다. 생존 가능성은 배 안에 1-3명의 부모를 둔 사람에게 좋다. 혼자라는 것은 또한 치명적이고 누군가가 배에 4명 이상의 부모를 두고 있을 때 생존 가능성이 줄어든다는 것을 증명한다. 운임–&gt; 지속적 특징123print('Highest Fare was:',data['Fare'].max()) #최고 요금print('Lowest Fare was:',data['Fare'].min()) #최저 요금print('Average Fare was:',data['Fare'].mean()) #평균 요금 Highest Fare was: 512.3292 Lowest Fare was: 0.0 Average Fare was: 32.2042079685746 12345678f,ax=plt.subplots(1,3,figsize=(20,8))sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])ax[0].set_title('Fares in Pclass 1')sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])ax[1].set_title('Fares in Pclass 2')sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])ax[2].set_title('Fares in Pclass 3')plt.show() Pclass1의 승객 요금에는 큰 배분이 있을 것으로 보이며, 이 배분은 기준이 감소함에 따라 계속 감소하고 있다. 이것 또한 지속적이기 때문에 우리는 바이닝을 사용하여 이산값으로 변환할 수 있다. 모든 형상에 대한 간단한 관측치:성=남성에 비해 여성의 생존 가능성은 높다. Pclass:일등석 승객이 되면 생존 가능성이 더 높아진다는 눈에 띄는 추세가 있다. Pclass3의 생존율은 매우 낮다. 여성의 경우 Pclass1에서 생존할 확률은 거의 1이며 Pclass2에서 생존할 확률도 높다. 돈이 이긴다!!! 나이: 5~10세 미만의 어린이들은 생존 확률이 높다. 15세에서 35세 사이의 승객들이 많이 죽었다. 시작됨: 이것은 매우 흥미로운 특징이다. Pclass1 승객의 대다수가 S. Q. 승객들이 모두 Pclass3 출신이었음에도 불구하고 C에서 생존할 가능성은 더 좋아 보인다. Parch+SibSp: 1-2명의 형제자매가 있고, 기내에 스파우스를 두거나, 1-3명의 부모가 혼자 있거나, 대가족이 당신과 함께 여행하는 것보다 가능성이 더 높다. 특색 간의 상관 관계1234sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()--&gt;상관 행렬fig=plt.gcf()fig.set_size_inches(10,8)plt.show()","link":"/2020/11/04/titanic/"},{"title":"home_credit_default_risk","text":"Kaggle API 설치 Google Colab에서 Kaggle API를 불러오려면 다음 소스코드를 실행한다. 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) 1. Kaggle Token 다운로드 Kaggle에서 API Token을 다운로드 받는다. [Kaggle]-[My Account]-[API]-[Create New API Token]을 누르면 kaggle.json 파일이 다운로드 된다. 이 파일을 바탕화면에 옮긴 뒤, 아래 코드(토큰을 실행시키는 코드)를 실행 시킨다. 12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 66 bytes 아래 코드는 실행됬는지 확인하는 코드 1ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 2. Kaggle 데이터 불러오기 먼저 kaggle competition list를 불러온다. 1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 134 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 161 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 292 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2248 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 17260 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4325 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 366 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1130 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 226 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 1491 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False competitive-data-science-predict-future-sales 2020-12-31 23:59:00 Playground Kudos 9392 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 44 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 198 False hashcode-drone-delivery 2020-12-14 23:59:00 Playground Knowledge 80 False cdp-unlocking-climate-solutions 2020-12-02 23:59:00 Analytics $91,000 0 False lish-moa 2020-11-30 23:59:00 Research $30,000 3454 False google-football 2020-11-30 23:59:00 Featured $6,000 925 False conways-reverse-game-of-life-2020 2020-11-30 23:59:00 Playground Swag 132 False lyft-motion-prediction-autonomous-vehicles 2020-11-25 23:59:00 Featured $30,000 788 False 1!kaggle competitions download -c home-credit-default-risk Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) installments_payments.csv.zip: Skipping, found more recently modified local copy (use --force to force download) previous_application.csv.zip: Skipping, found more recently modified local copy (use --force to force download) application_test.csv.zip: Skipping, found more recently modified local copy (use --force to force download) bureau.csv.zip: Skipping, found more recently modified local copy (use --force to force download) sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download) POS_CASH_balance.csv.zip: Skipping, found more recently modified local copy (use --force to force download) credit_card_balance.csv.zip: Skipping, found more recently modified local copy (use --force to force download) HomeCredit_columns_description.csv: Skipping, found more recently modified local copy (use --force to force download) application_train.csv.zip: Skipping, found more recently modified local copy (use --force to force download) bureau_balance.csv.zip: Skipping, found more recently modified local copy (use --force to force download) ls는 디렉터리(파일,경로) 내의 데이터 파일을 보여주는 명령어 1!ls application_test.csv.zip installments_payments.csv.zip application_train.csv.zip POS_CASH_balance.csv.zip bureau_balance.csv.zip previous_application.csv.zip bureau.csv.zip sample_data credit_card_balance.csv.zip sample_submission.csv gender_submission.csv test.csv HomeCredit_columns_description.csv train.csv 12345678! unzip application_test.csv.zip ! unzipinstallments_payments.csv.zip! unzip application_train.csv.zip ! unzip POS_CASH_balance.csv.zip! unzip bureau_balance.csv.zip ! unzip previous_application.csv.zip! unzip bureau.csv.zip ! unzip credit_card_balance.csv.zip Archive: application_test.csv.zip inflating: application_test.csv /bin/bash: unzipinstallments_payments.csv.zip: command not found Archive: application_train.csv.zip replace application_train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: application_train.csv Archive: POS_CASH_balance.csv.zip replace POS_CASH_balance.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: POS_CASH_balance.csv Archive: bureau_balance.csv.zip replace bureau_balance.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: bureau_balance.csv Archive: previous_application.csv.zip replace previous_application.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: previous_application.csv Archive: bureau.csv.zip replace bureau.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: bureau.csv Archive: credit_card_balance.csv.zip replace credit_card_balance.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: credit_card_balance.csv 현재 총 14개의 데이터를 다운로드 받았다. application_test.csv.zip installments_payments.csv.zip application_train.csv.zip POS_CASH_balance.csv.zip bureau_balance.csv.zip previous_application.csv.zip bureau.csv.zip sample_data credit_card_balance.csv.zip sample_submission.csv gender_submission.csv test.csv HomeCredit_columns_description.csv train.csv 3. 캐글 데이터 수집 및 EDA 우선 데이터를 수집하기에 앞서서 EDA에 관한 필수 패키지를 설치하자. 12345678910111213# 데이터 조작을 위한 numpy와 팬더import numpy as npimport pandas as pd # 범주형 변수를 처리하기 위한 사전 처리 학습from sklearn.preprocessing import LabelEncoder# 파일 시스템 매니지먼트import os# 경고 억제import warningswarnings.filterwarnings('ignore')# 플롯을 위한 matplotlib 및 seaornimport matplotlib.pyplot as pltimport seaborn as sns 분류 감독됨: 라벨은 교육 데이터에 포함되며, 목적은 형상으로부터 라벨을 예측하는 방법을 학습하는 모델을 훈련시키는 것이다. 분류: 라벨은 0(대출금을 제때 상환할 수 있음), 1(대출금 상환에 어려움이 있음)의 이진 변수다. 데이터 : 은행을 이용하지 않은 사람들에게 신용대출(대출)을 제공하는 서비스인 홈 크레딧에 의해 제공된다 12# 사용 가능한 파일 나열print(os.listdir()) ['.config', 'previous_application.csv', 'application_train.csv', 'bureau_balance.csv', 'application_test.csv.zip', 'bureau.csv', 'POS_CASH_balance.csv', 'previous_application.csv.zip', 'bureau_balance.csv.zip', 'POS_CASH_balance.csv.zip', 'HomeCredit_columns_description.csv', 'bureau.csv.zip', 'installments_payments.csv.zip', 'test.csv', 'sample_submission.csv', 'application_train.csv.zip', 'gender_submission.csv', 'credit_card_balance.csv', 'credit_card_balance.csv.zip', 'train.csv', 'sample_data'] 1234# 교육자료app_train = pd.read_csv('application_train.csv')print('Training data shape: ', app_train.shape)app_train.head() Training data shape: (307511, 122) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE REGION_POPULATION_RELATIVE DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH OWN_CAR_AGE FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL OCCUPATION_TYPE CNT_FAM_MEMBERS REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION LIVE_REGION_NOT_WORK_REGION REG_CITY_NOT_LIVE_CITY REG_CITY_NOT_WORK_CITY LIVE_CITY_NOT_WORK_CITY ... LIVINGAPARTMENTS_MEDI LIVINGAREA_MEDI NONLIVINGAPARTMENTS_MEDI NONLIVINGAREA_MEDI FONDKAPREMONT_MODE HOUSETYPE_MODE TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE DAYS_LAST_PHONE_CHANGE FLAG_DOCUMENT_2 FLAG_DOCUMENT_3 FLAG_DOCUMENT_4 FLAG_DOCUMENT_5 FLAG_DOCUMENT_6 FLAG_DOCUMENT_7 FLAG_DOCUMENT_8 FLAG_DOCUMENT_9 FLAG_DOCUMENT_10 FLAG_DOCUMENT_11 FLAG_DOCUMENT_12 FLAG_DOCUMENT_13 FLAG_DOCUMENT_14 FLAG_DOCUMENT_15 FLAG_DOCUMENT_16 FLAG_DOCUMENT_17 FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR 0 100002 1 Cash loans M N Y 0 202500.0 406597.5 24700.5 351000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.018801 -9461 -637 -3648.0 -2120 NaN 1 1 0 1 1 0 Laborers 1.0 2 2 WEDNESDAY 10 0 0 0 0 0 0 ... 0.0205 0.0193 0.0000 0.00 reg oper account block of flats 0.0149 Stone, brick No 2.0 2.0 2.0 2.0 -1134.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 1.0 1 100003 0 Cash loans F N N 0 270000.0 1293502.5 35698.5 1129500.0 Family State servant Higher education Married House / apartment 0.003541 -16765 -1188 -1186.0 -291 NaN 1 1 0 1 1 0 Core staff 2.0 1 1 MONDAY 11 0 0 0 0 0 0 ... 0.0787 0.0558 0.0039 0.01 reg oper account block of flats 0.0714 Block No 1.0 0.0 1.0 0.0 -828.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 2 100004 0 Revolving loans M Y Y 0 67500.0 135000.0 6750.0 135000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.010032 -19046 -225 -4260.0 -2531 26.0 1 1 1 1 1 0 Laborers 1.0 2 2 MONDAY 9 0 0 0 0 0 0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -815.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 3 100006 0 Cash loans F N Y 0 135000.0 312682.5 29686.5 297000.0 Unaccompanied Working Secondary / secondary special Civil marriage House / apartment 0.008019 -19005 -3039 -9833.0 -2437 NaN 1 1 0 1 0 0 Laborers 2.0 2 2 WEDNESDAY 17 0 0 0 0 0 0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2.0 0.0 2.0 0.0 -617.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NaN NaN NaN NaN NaN NaN 4 100007 0 Cash loans M N Y 0 121500.0 513000.0 21865.5 513000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.028663 -19932 -3038 -4311.0 -3458 NaN 1 1 0 1 0 0 Core staff 1.0 2 2 THURSDAY 11 0 0 0 0 1 1 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -1106.0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows × 122 columns 교육 데이터에는 307511개의 관측치(각각 별도 대출)와 대상(우리가 예측하고자 하는 라벨)을 포함한 122개의 특징(변수)이 있다. 1234# 데이터 기능 테스트app_test = pd.read_csv('application_test.csv')print('Testing data shape: ', app_test.shape)app_test.head() Testing data shape: (48744, 121) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SK_ID_CURR NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE REGION_POPULATION_RELATIVE DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH OWN_CAR_AGE FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL OCCUPATION_TYPE CNT_FAM_MEMBERS REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION LIVE_REGION_NOT_WORK_REGION REG_CITY_NOT_LIVE_CITY REG_CITY_NOT_WORK_CITY LIVE_CITY_NOT_WORK_CITY ORGANIZATION_TYPE ... LIVINGAPARTMENTS_MEDI LIVINGAREA_MEDI NONLIVINGAPARTMENTS_MEDI NONLIVINGAREA_MEDI FONDKAPREMONT_MODE HOUSETYPE_MODE TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE DAYS_LAST_PHONE_CHANGE FLAG_DOCUMENT_2 FLAG_DOCUMENT_3 FLAG_DOCUMENT_4 FLAG_DOCUMENT_5 FLAG_DOCUMENT_6 FLAG_DOCUMENT_7 FLAG_DOCUMENT_8 FLAG_DOCUMENT_9 FLAG_DOCUMENT_10 FLAG_DOCUMENT_11 FLAG_DOCUMENT_12 FLAG_DOCUMENT_13 FLAG_DOCUMENT_14 FLAG_DOCUMENT_15 FLAG_DOCUMENT_16 FLAG_DOCUMENT_17 FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR 0 100001 Cash loans F N Y 0 135000.0 568800.0 20560.5 450000.0 Unaccompanied Working Higher education Married House / apartment 0.018850 -19241 -2329 -5170.0 -812 NaN 1 1 0 1 0 1 NaN 2.0 2 2 TUESDAY 18 0 0 0 0 0 0 Kindergarten ... NaN 0.0514 NaN NaN NaN block of flats 0.0392 Stone, brick No 0.0 0.0 0.0 0.0 -1740.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 1 100005 Cash loans M N Y 0 99000.0 222768.0 17370.0 180000.0 Unaccompanied Working Secondary / secondary special Married House / apartment 0.035792 -18064 -4469 -9118.0 -1623 NaN 1 1 0 1 0 0 Low-skill Laborers 2.0 2 2 FRIDAY 9 0 0 0 0 0 0 Self-employed ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 3.0 2 100013 Cash loans M Y Y 0 202500.0 663264.0 69777.0 630000.0 NaN Working Higher education Married House / apartment 0.019101 -20038 -4458 -2175.0 -3503 5.0 1 1 0 1 0 0 Drivers 2.0 2 2 MONDAY 14 0 0 0 0 0 0 Transport: type 3 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -856.0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 1.0 4.0 3 100028 Cash loans F N Y 2 315000.0 1575000.0 49018.5 1575000.0 Unaccompanied Working Secondary / secondary special Married House / apartment 0.026392 -13976 -1866 -2000.0 -4208 NaN 1 1 0 1 1 0 Sales staff 4.0 2 2 WEDNESDAY 11 0 0 0 0 0 0 Business Entity Type 3 ... 0.2446 0.3739 0.0388 0.0817 reg oper account block of flats 0.3700 Panel No 0.0 0.0 0.0 0.0 -1805.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 3.0 4 100038 Cash loans M Y N 1 180000.0 625500.0 32067.0 625500.0 Unaccompanied Working Secondary / secondary special Married House / apartment 0.010032 -13040 -2191 -4000.0 -4262 16.0 1 1 1 1 0 0 NaN 3.0 2 2 FRIDAY 5 0 0 0 0 1 1 Business Entity Type 3 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -821.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NaN NaN NaN NaN NaN NaN 5 rows × 121 columns 테스트 세트가 상당히 작고 대상 컬럼이 부족하다. 탐색적 데이터 분석.탐색적 데이터 분석(EDA)은 우리가 통계를 계산하고 수치를 만들어 데이터 내의 경향, 이상 징후, 패턴 또는 관계를 찾아내는 개방형 프로세스다. EDA의 목표는 우리의 데이터가 우리에게 말해줄 수 있는 것을 배우는 것이다. 일반적으로 높은 수준의 개요에서 시작되며, 데이터에서 흥미로운 영역을 발견하면 특정 영역으로 좁혀진다. 그 결과들은 그 자체로 흥미로울 수도 있고, 어떤 기능을 사용할지 결정하는 것을 도와줌으로써 우리의 모델 선택을 알리는 데 사용될 수도 있다. 대상 열의 분포 조사대상은 대출금 0을 제때 상환했거나 고객이 상환에 어려움을 겪었음을 나타내는 1을 예측하라는 것이다. 우리는 우선 각 범주에 속하는 대출의 수를 조사할 수 있다. 1app_train['TARGET'].value_counts() 0 282686 1 24825 Name: TARGET, dtype: int64 1app_train['TARGET'].astype(int).plot.hist(); 이 정보로부터, 우리는 이것이 불균형한 계급 문제임을 알 수 있다. 제때 갚지 못한 대출보다 제때 갚은 대출이 훨씬 많다. 일단 우리가 좀 더 정교한 기계 학습 모델에 들어가면, 우리는 이러한 불균형을 반영하기 위해 데이터에서의 그들의 표현에 따라 수업에 무게를 둘 수 있다. 결측값 검사다음으로 각 열에 있는 결측값의 수와 백분율을 살펴보기로 한다. 123456789101112131415161718192021222324252627# 결측값을 열로 계산하는 함수 #Funct def missing_values_table(df): # 결측값 합계 mis_val = df.isnull().sum() # 결측값 백분율 mis_val_percent = 100 * df.isnull().sum() / len(df) # 결과로 테이블 만들기 mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) # 열 이름 바꾸기 mis_val_table_ren_columns = mis_val_table.rename( columns = {0 : 'Missing Values', 1 : '% of Total Values'}) # 누락된 내림차순 백분율을 기준으로 테이블 정렬 mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0].sort_values( '% of Total Values', ascending=False).round(1) # 일부 요약 정보 인쇄 print (&quot;Your selected dataframe has &quot; + str(df.shape[1]) + &quot; columns.\\n&quot; &quot;There are &quot; + str(mis_val_table_ren_columns.shape[0]) + &quot; columns that have missing values.&quot;) # 누락된 정보가 있는 데이터 프레임 반환 return mis_val_table_ren_columns 123# 결측값 통계량missing_values = missing_values_table(app_train)missing_values.head(20) Your selected dataframe has 122 columns. There are 67 columns that have missing values. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Missing Values % of Total Values COMMONAREA_MEDI 214865 69.9 COMMONAREA_AVG 214865 69.9 COMMONAREA_MODE 214865 69.9 NONLIVINGAPARTMENTS_MEDI 213514 69.4 NONLIVINGAPARTMENTS_MODE 213514 69.4 NONLIVINGAPARTMENTS_AVG 213514 69.4 FONDKAPREMONT_MODE 210295 68.4 LIVINGAPARTMENTS_MODE 210199 68.4 LIVINGAPARTMENTS_MEDI 210199 68.4 LIVINGAPARTMENTS_AVG 210199 68.4 FLOORSMIN_MODE 208642 67.8 FLOORSMIN_MEDI 208642 67.8 FLOORSMIN_AVG 208642 67.8 YEARS_BUILD_MODE 204488 66.5 YEARS_BUILD_MEDI 204488 66.5 YEARS_BUILD_AVG 204488 66.5 OWN_CAR_AGE 202929 66.0 LANDAREA_AVG 182590 59.4 LANDAREA_MEDI 182590 59.4 LANDAREA_MODE 182590 59.4 열 유형각 데이터 유형의 열 수를 살펴보자. int64와 float64는 숫자 변수(이산형 또는 연속형일 수 있음)이다. 객체 열은 문자열을 포함하며 범주형 형상이다. 12# 각 열 유형별 수app_train.dtypes.value_counts() float64 65 int64 41 object 16 dtype: int64 이제 각 개체(범주형) 열에 있는 고유한 항목 수를 살펴봅시다. 12# 각 객체 열의 고유 클래스 수app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0) NAME_CONTRACT_TYPE 2 CODE_GENDER 3 FLAG_OWN_CAR 2 FLAG_OWN_REALTY 2 NAME_TYPE_SUITE 7 NAME_INCOME_TYPE 8 NAME_EDUCATION_TYPE 5 NAME_FAMILY_STATUS 6 NAME_HOUSING_TYPE 6 OCCUPATION_TYPE 18 WEEKDAY_APPR_PROCESS_START 7 ORGANIZATION_TYPE 58 FONDKAPREMONT_MODE 4 HOUSETYPE_MODE 3 WALLSMATERIAL_MODE 7 EMERGENCYSTATE_MODE 2 dtype: int64 대부분의 범주형 변수의 고유 항목 수는 상대적으로 적다. 우리는 이러한 범주형 변수에 대처할 방법을 찾아야 할 것이다! 범주형 변수를 인코딩하는 중:더 나아가기 전에 성가신 범주형 변수를 다뤄야 한다. 기계 학습 모델은 유감스럽게도 범주형 변수를 다룰 수 없다(LightGBM과 같은 일부 모델은 제외). 따라서 이 변수들을 모델로 넘기기 전에 숫자로 인코딩(표현)하는 방법을 찾아야 한다. 이 과정을 수행하는 방법에는 크게 두 가지가 있다. 라벨 인코딩: 정수를 사용하여 범주형 변수의 각 고유 범주를 할당하십시오. 새 열이 생성되지 않음 예는 아래와 같다. 단일 핫 인코딩: 범주형 변수의 각 고유 범주에 대해 새 열을 생성하십시오. 각 관측치는 해당 범주에 대해 열에 1을, 다른 모든 새 열에 0을 받는다. 라벨 인코딩의 문제는 카테고리에 임의의 순서를 부여한다는 것이다. 각 범주에 할당된 값은 랜덤이며 범주의 고유한 측면을 반영하지 않는다. 위의 예에서 프로그래머는 4를, 데이터 과학자는 1을 받지만, 만약 우리가 같은 과정을 다시 한다면, 라벨이 거꾸로 되거나 완전히 달라질 수 있다. 정수의 실제 할당은 임의적이다. 따라서 우리가 라벨 인코딩을 수행할 때 모델은 형상의 상대적 값(예: 프로그래머 = 4 및 데이터 과학자 = 1)을 사용하여 우리가 원하는 가중치를 할당할 수 있다. 범주형 변수(예: 남성/여성)에 대해 고유한 값이 두 개뿐이면 레이블 인코딩은 괜찮지만 두 개 이상의 고유한 범주에 대해서는 하나의 핫 인코딩이 안전한 옵션이다. 이러한 접근방식의 상대적 장점에 대해 일부 논쟁이 있으며, 일부 모델은 라벨로 인코딩된 범주형 변수를 문제 없이 다룰 수 있다. 여기 좋은 스택 오버플로 토론이 있다. 클래스가 많은 범주형 변수에 대해서는 (그리고 이것은 개인적인 의견일 뿐이다) 하나의 핫 인코딩이 범주에 임의의 값을 부과하지 않기 때문에 가장 안전한 접근법이라고 생각한다. 단일 핫 인코딩의 유일한 단점은 피쳐 수(데이터의 치수)가 범주가 많은 범주형 변수로 폭발할 수 있다는 점이다. 이를 처리하기 위해 PCA나 기타 차원성 감소 방법에 따라 1회 핫 인코딩을 수행하여 치수 수를 줄일 수 있다(여전히 정보 보존을 위해 노력). 이 노트북에서는 범주 2개만 있는 범주형 변수에 레이블 인코딩을 사용하고 범주 2개 이상의 범주형 변수에 대해 One-Hot 인코딩을 사용할 것이다. 이 과정은 프로젝트에 더 깊이 들어가면서 변화가 필요할 수 있지만, 현재로서는 이것이 우리에게 어떤 영향을 미치는지 알 수 있을 것이다. (우리는 또한 이 노트북에서 어떠한 차원성 감소도 사용하지 않고 향후 반복해서 검토할 것이다.) 레이블 인코딩 및 단일 핫 인코딩위에서 설명한 정책을 실행하자: 2개의 고유한 범주를 가진 범주형 변수(dtype == 객체)에 대해서는 레이블 인코딩을 사용하고, 2개 이상의 고유한 범주를 가진 범주형 변수에 대해서는 1-hot 인코딩을 사용한다. 라벨 인코딩의 경우 Scikit-Learn LabelEncoder를 사용하고, 핫 인코딩의 경우 팬더 get_dummies(df) 기능을 사용한다. 12345678910111213141516171819# 레이블 인코더 객체 생성le = LabelEncoder()le_count = 0# 열을 반복하십시오for col in app_train: if app_train[col].dtype == 'object': # 고유 범주가 2개 이하인 경우 if len(list(app_train[col].unique())) &lt;= 2: # 교육 데이터 교육 le.fit(app_train[col]) # 교육 및 테스트 데이터 모두 혁신 app_train[col] = le.transform(app_train[col]) app_test[col] = le.transform(app_test[col]) # 레이블이 인코딩된 열 수를 추적 le_count += 1 print('%d columns were label encoded.' % le_count) 0 columns were label encoded. 123456# 범주형 변수의 단일 핫 인코딩app_train = pd.get_dummies(app_train)app_test = pd.get_dummies(app_test)print('Training Features shape: ', app_train.shape)print('Testing Features shape: ', app_test.shape) Training Features shape: (307511, 243) Testing Features shape: (48744, 239) 교육 및 테스트 데이터 정렬훈련 데이터와 시험 데이터 모두에서 동일한 특징(색상)이 있어야 한다. 원핫 인코딩은 시험 데이터에 나타나지 않는 범주를 포함하는 일부 범주형 변수가 있었기 때문에 훈련 데이터에 더 많은 열을 생성했다. 테스트 데이터에 없는 교육 데이터 열을 제거하려면 데이터 프레임을 정렬해야 한다. 먼저 교육 데이터에서 대상 열을 추출한다(시험 데이터에는 없지만 이 정보를 보관해야 하기 때문이다). 정렬을 수행할 때 행이 아닌 열에 따라 데이터 프레임을 정렬하도록 축 = 1을 설정해야 함! 12345678910train_labels = app_train['TARGET']# 교육 및 테스트 데이터 정렬, 두 데이터 프레임에 열만 유지app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)# 목표값 다시 추가app_train['TARGET'] = train_labelsprint('Training Features shape: ', app_train.shape)print('Testing Features shape: ', app_test.shape) Training Features shape: (307511, 240) Testing Features shape: (48744, 239) 교육 및 테스트 데이터셋은 이제 머신러닝에 필요한 기능과 동일한 기능을 가지고 있다. 원핫 인코딩으로 피쳐 수가 크게 늘었다. 데이터셋 크기를 줄이기 위해 언젠가는 차원성 축소(관련되지 않은 기능 제거)를 시도해보려고 할 것이다. 탐색 데이터 분석으로 돌아가기이상 징후우리가 EDA를 할 때 항상 경계하고 싶은 한 가지 문제는 데이터 내의 이상 현상이다. 이는 잘못된 형식의 숫자, 측정 장비의 오류 또는 유효하지만 극단적인 측정 때문일 수 있다. 이상 징후를 정량적으로 지원하는 한 가지 방법은 설명 방법을 사용하여 열의 통계를 보는 것이다. DAYS_BOYDE 칼럼의 숫자는 현재 대출 신청과 관련하여 기록되기 때문에 음수가 된다. 이러한 통계치를 년 단위로 보려면 -1로 나누어 1년 내 일수로 나누면 된다. 1(app_train['DAYS_BIRTH'] / -365).describe() count 307511.000000 mean 43.936973 std 11.956133 min 20.517808 25% 34.008219 50% 43.150685 75% 53.923288 max 69.120548 Name: DAYS_BIRTH, dtype: float64 그 나이들은 합리적으로 보인다. 높은 쪽이든 낮은 쪽이든 나이에 대한 특이치는 없다. 1app_train['DAYS_EMPLOYED'].describe() count 307511.000000 mean 63815.045904 std 141275.766519 min -17912.000000 25% -2760.000000 50% -1213.000000 75% -289.000000 max 365243.000000 Name: DAYS_EMPLOYED, dtype: float64 그건 옳지 않아! 최대값(긍정적인 것 외에)은 약 1000년 12app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');plt.xlabel('Days Employment'); 그냥 궁금해서 변칙적인 고객들을 부분집합해서 나머지 고객들보다 채무불이행 비율이 더 높은지 낮은지 살펴보자. 12345anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))print('There are %d anomalous days of employment' % len(anom)) The non-anomalies default on 8.66% of loans The anomalies default on 5.40% of loans There are 55374 anomalous days of employment 이상 징후는 채무불이행 비율이 낮은 것으로 나타났다. 이상 징후를 다루는 것은 정해진 규칙 없이 정확한 상황에 따라 달라진다. 가장 안전한 방법 중 하나는 기계의 학습 전에 이상 징후를 결측값으로 설정한 다음 (귀책 사용) 입력하는 것이다. 이 경우 모든 이상 징후가 정확히 동일한 가치를 가지기 때문에 이러한 모든 대출이 공통적으로 공유될 경우에 대비하여 동일한 가치로 기입하고자 한다. 변칙적인 값들은 어느 정도 중요한 것 같아, 만약 우리가 실제로 이 값을 채웠다면 우리는 기계 학습 모델을 말해주고 싶다. 해결책으로 숫자(np.nan)가 아닌 변칙값으로 채운 다음 변칙값의 변칙 여부를 나타내는 부울란을 새로 만들겠다. 12345678# 변칙적인 플래그 열 생성app_train['DAYS_EMPLOYED_ANOM'] = app_train[&quot;DAYS_EMPLOYED&quot;] == 365243# 변칙적인 값은 nan으로 대체app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');plt.xlabel('Days Employment'); 분포는 우리가 예상할 수 있는 것과 훨씬 일치하는 것으로 보이며, 우리는 또한 모형에 이 값들이 원래 변칙적이라는 것을 알리기 위해 새로운 열을 만들었다. (왜냐하면 우리는 일부 값, 아마도 열의 중간값으로 난을 채워야 할 것이기 때문이다.) 데이터 프레임에 Days가 있는 다른 열은 명백한 특이치가 없는 것으로 우리가 기대하는 것에 대한 것으로 보인다. 매우 중요한 사항으로서, 교육 데이터에 대해 수행하는 모든 작업은 테스트 데이터에도 적용되어야 한다. 반드시 새 열을 만들고 기존 열을 테스트 데이터에 np.nan으로 채우자. 1234app_test['DAYS_EMPLOYED_ANOM'] = app_test[&quot;DAYS_EMPLOYED&quot;] == 365243app_test[&quot;DAYS_EMPLOYED&quot;].replace({365243: np.nan}, inplace = True)print('There are %d anomalies in the test data out of %d entries' % (app_test[&quot;DAYS_EMPLOYED_ANOM&quot;].sum(), len(app_test))) There are 9274 anomalies in the test data out of 48744 entries 상관 관계이제 범주형 변수와 특이치를 다루었으므로 EDA를 계속 진행합시다. 데이터를 이해하고 이해하는 한 가지 방법은 피쳐와 대상 간의 상관 관계를 찾는 것이다. 우리는 .corr 데이터프레임 방법을 사용하여 모든 변수와 목표값 사이의 Pearson 상관 계수를 계산할 수 있다. 상관 계수는 형상의 “관련성”을 나타내는 가장 큰 방법은 아니지만, 데이터 내에서 가능한 관계에 대한 아이디어를 제공한다. 상관관계의 절대값에 대한 일반적인 해석은 다음과 같다. .00-19 “매우 약함” .20-.39 “weak” .40-.59 “moderate” .60-.79 “강력” .80-1.0 “매우 강함” 123456# 대상과의 상관관계를 찾아 정렬correlations = app_train.corr()['TARGET'].sort_values()# 상관관계 표시print('Most Positive Correlations:\\n', correlations.tail(15))print('\\nMost Negative Correlations:\\n', correlations.head(15)) Most Positive Correlations: OCCUPATION_TYPE_Laborers 0.043019 FLAG_DOCUMENT_3 0.044346 REG_CITY_NOT_LIVE_CITY 0.044395 FLAG_EMP_PHONE 0.045982 NAME_EDUCATION_TYPE_Secondary / secondary special 0.049824 REG_CITY_NOT_WORK_CITY 0.050994 DAYS_ID_PUBLISH 0.051457 CODE_GENDER_M 0.054713 DAYS_LAST_PHONE_CHANGE 0.055218 NAME_INCOME_TYPE_Working 0.057481 REGION_RATING_CLIENT 0.058899 REGION_RATING_CLIENT_W_CITY 0.060893 DAYS_EMPLOYED 0.074958 DAYS_BIRTH 0.078239 TARGET 1.000000 Name: TARGET, dtype: float64 Most Negative Correlations: EXT_SOURCE_3 -0.178919 EXT_SOURCE_2 -0.160472 EXT_SOURCE_1 -0.155317 NAME_EDUCATION_TYPE_Higher education -0.056593 CODE_GENDER_F -0.054704 NAME_INCOME_TYPE_Pensioner -0.046209 DAYS_EMPLOYED_ANOM -0.045987 ORGANIZATION_TYPE_XNA -0.045987 FLOORSMAX_AVG -0.044003 FLOORSMAX_MEDI -0.043768 FLOORSMAX_MODE -0.043226 EMERGENCYSTATE_MODE_No -0.042201 HOUSETYPE_MODE_block of flats -0.040594 AMT_GOODS_PRICE -0.039645 REGION_POPULATION_RELATIVE -0.037227 Name: TARGET, dtype: float64 더 중요한 상관관계 몇 가지를 살펴봅시다: DAYS_BOYT는 가장 긍정적인 상관관계 입니다. (변수와 그 자체와의 상관관계가 항상 1이기 때문에 TARGET은 제외) 문서를 보면 Days_BOYT는 마이너스일(어떠한 이유로든!) 대출 당시 고객의 일(일) 연령이다. 상관관계는 양수지만, 이 특성의 값은 실제로 음수인데, 이는 클라이언트가 나이가 들수록 대출금 채무불이행(대상 == 0) 가능성이 적다는 것을 의미한다. 그건 좀 헷갈리니까 형상의 절대값을 취해서 그 다음에 상관관계는 음수가 될 겁니다. 상환연령이 상환에 미치는 영향123# 출생 후 양일간과 대상의 상관관계 찾기app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])app_train['DAYS_BIRTH'].corr(app_train['TARGET']) -0.07823930830982694 고객이 나이가 들수록 고객이 나이가 들수록 대출금을 제때 상환하는 경향이 있다는 목표와 부정적인 선형 관계가 형성된다. 이 변수부터 살펴보자. 첫째, 우리는 시대의 히스토그램을 만들 수 있다. 우리는 그 줄거리를 좀 더 이해할 수 있도록 몇 년 안에 x축을 넣을 것이다. 123456# 플롯 스타일 설정plt.style.use('fivethirtyeight')# 연령별 연령 분포 그림 그리기plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count'); 그 자체로 연령의 분포는 모든 연령대가 합리적이기 때문에 특이치가 없다는 것 이외에는 우리에게 별로 알려주지 않는다. 연령대가 대상에 미치는 영향을 시각화하기 위해 다음으로 대상의 값으로 색칠한 KDE(커널 밀도 추정도)를 만들겠다. 커널 밀도 추정 그림은 단일 변수의 분포를 보여주며 평활 히스토그램으로 생각할 수 있다(대개 가우스인 커널을 각 데이터 지점에서 계산한 다음 모든 개별 커널을 평균하여 단일 평활 곡선을 개발함으로써 생성된다). 우리는 이 그래프에 해저 Kdeplot을 사용할 것이다. 12345678910plt.figure(figsize = (10, 8))# 제때 상환된 대출의 KDE 플롯sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')# 제때 상환되지 않은 대출의 KDE 플롯sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')# 플롯의 라벨링plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages'); 목표 == 1 커브가 범위의 젊은 쪽 끝을 향해 기울어진다. 이는 유의미한 상관 계수(-0.07 상관 계수)는 아니지만, 이 변수는 대상에 영향을 미치기 때문에 머신러닝 모델에서 유용할 것으로 보인다. 이 관계를 다른 방식으로 보자: 평균적으로 연령대별 대출금 상환 실패. 이 그래프를 만들기 위해 먼저 나이 범주를 각각 5년씩의 빈으로 자른다. 그런 다음 각 빈에 대해 대상의 평균가치를 산출하는데, 이는 각 연령 범주별로 상환되지 않은 대출의 비율을 알려준다.","link":"/2020/11/05/home_credit_default_risk/"},{"title":"Seaborn with Matplotlib (1)","text":"1. seaborn + matplotlib seaborn을 matplotlib과 섞어쓰는 방법입니다. 4부 중 첫 번째 시간입니다. seaborn 함수 중 matplotlib axes를 반환하는 함수들에 관한 내용입니다. seaborn API seaborn은 matplotlib을 쉽고 아름답게 쓰고자 만들어졌습니다. 따라서 seaborn의 결과물은 당연히 matplotlib의 결과물입니다. 그러나 간혹 seaborn이 그린 그림의 폰트, 색상에 접근이 되지 않아 난처합니다. seaborn의 구조를 잘 이해하지 못하면 해결도 어렵습니다. v0.11 기준으로 seaborn에는 다음 함수들이 있습니다. matplotlib의 출력물은 figure와 axes만을 반환합니다. seaborn의 명령어 중 axes를 반환하는 것들은 matplotlib과 섞어 쓰기 좋습니다. 먼저 matplotlib의 객체 지향object oriented interface를 사용해서 그림의 틀을 만든 뒤, 특정 axes에 seaborn을 삽입하면 됩니다. 결론적으로, 하고 싶은 거 다 됩니다. 1.1. Load data 예제로 사용할 펭귄 데이터를 불러옵니다. seaborn에 내장되어 있습니다.123456import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snspenguins = sns.load_dataset(&quot;penguins&quot;)penguins.head() 1.2. figure and axes matplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다. 1 x 2 축공간을 구성합니다. 123fig, axes = plt.subplots(ncols=2, figsize=(8,4))fig.tight_layout() 1.3. plot with matplotlib matplotlib 기능을 이용해서 산점도를 그립니다. x축은 부리 길이 bill length y축은 부리 위 아래 두께 bill depth 색상은 종species로 합니다. Adelie, Chinstrap, Gentoo이 있습니다. 두 축공간 중 왼쪽에만 그립니다. 123456789101112131415fig, axes = plt.subplots(ncols=2, figsize=(8, 4))species_u = penguins[&quot;species&quot;].unique()# plot 0 : matplotlibfor i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 )axes[0].legend(species_u, title=&quot;species&quot;)axes[0].set_xlabel(&quot;Bill Length (mm)&quot;)axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 1.4. plot with seabornseaborn scatterplot 이번엔 같은 plot을 seaborn으로 그려봅니다. 위 코드에 아래 세 줄만 추가합니다. 1234# plot 1 : seabornsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1])axes[1].set_xlabel(&quot;Bill Length (mm)&quot;)axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) 세 줄로 거의 동일한 그림이 나왔습니다. scatter plot의 점 크기만 살짝 작습니다. label의 투명도만 살짝 다릅니다. seaborn 명령 scatterplot()을 그대로 사용했습니다. x축과 y축 label도 바꾸었습니다. ax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다. matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다. 1.5. matplotlib + seaborn &amp; seaborn + matplotlib matplotlib과 seaborn이 자유롭게 섞일 수 있습니다. matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고, seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다. 파이썬 코드는 다음과 같습니다. 12345678910111213141516171819202122232425262728293031323334fig, axes = plt.subplots(ncols=2, figsize=(8, 4))species_u = penguins[&quot;species&quot;].unique()# plot 0 : matplotlib + seabornfor i, s in enumerate(species_u): # matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 ) # seaborn 추세선 sns.regplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins.loc[penguins[&quot;species&quot;]==s], scatter=False, ax=axes[0]) axes[0].legend(species_u, title=&quot;species&quot;)axes[0].set_xlabel(&quot;Bill Length (mm)&quot;)axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;)# plot 1 : seaborn + matplotlib# seaborn 산점도sns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1])axes[1].set_xlabel(&quot;Bill Length (mm)&quot;)axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;)for i, s in enumerate(species_u): # matplotlib 중심점 axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), c=f&quot;C{i}&quot;, alpha=1, marker=&quot;x&quot;, s=100 )fig.tight_layout() 1.6. seaborn + seaborn + matplotlib 안 될 이유가 없습니다. seaborn scatterplot + seaborn kdeplot + matplotlib text입니다. 1234567891011121314151617181920fig, ax = plt.subplots(figsize=(6,5))# plot 0: scatter plotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, color=&quot;k&quot;, data=penguins, alpha=0.3, ax=ax, legend=False)# plot 1: kde plotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.5, ax=ax, legend=False)# text:species_u = penguins[&quot;species&quot;].unique()for i, s in enumerate(species_u): ax.text(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), s = s, fontdict={&quot;fontsize&quot;:14, &quot;fontweight&quot;:&quot;bold&quot;,&quot;color&quot;:&quot;k&quot;} )ax.set_xlabel(&quot;Bill Length (mm)&quot;)ax.set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 1.7. 결론 seaborn을 matplotlib과 마음껏 섞어쓰세요 단, axes를 반환하는 명령어에 한해서 말입니다. 이런 명령어를 axes-level function 이라고 합니다. 출저: https://jehyunlee.github.io/2020/09/30/Python-DS-34-seaborn_matplotlib/","link":"/2020/11/06/Seaborn_with_Matplotlib_1/"},{"title":"Seaborn with Matplotlib (3)","text":"3. seaborn figure-level function seaborn의 핵심기능, 강력한 명령입니다. 4부 중 세 번째 시간입니다. matplotlib으로는 매우 수고스러울 일을 줄여줍니다. 그러나 손대기 어렵기도 합니다. 이유와 해결방법을 알아봅시다. seaborn API seaborn tutorial seaborn with matplotlib (1) 지난 글에서 matplotlib과 친한 함수를 알아봤습니다. 이런 axex 반환 함수를 axes-level 함수, 반대로 그림 전체를 반환하는 함수를 figure-level 함수라고 합니다. 아래 노란 부분이 figure-level 함수입니다. 반환 형식return type이 왜 이렇게 깔끔하지 않은지는 뒤에 보겠습니다. figure-level 함수는 복잡한 그림을 한 번에 편하게 그려줍니다. 3.1. figure-level 편리함 맛보기: FacetGrid() seaborn에 내장된 penguins dataset에는 이런 데이터가 있습니다. bill_length_mm : 부리 길이 bill_depth_mm : 부리 위아래 두께 species : 펭귄 종 sex : 성별 island : 서식지 이 데이터를 산점도로 한번에 나타내고자 합니다. X축 : bill_length_mm Y축 : bill_depth_mm 색상 : species X방향 axes : island Y방향 axes : sex matplotlib 코드는 이렇습니다. 12345678910111213141516171819202122232425262728293031fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), sharex=True, sharey=True)# 인자별 데이터 종 수 세기sex = penguins[&quot;sex&quot;].dropna().unique()island = penguins[&quot;island&quot;].dropna().unique()species = penguins[&quot;species&quot;].dropna().unique()# X방향 axes: sexfor i in range(len(sex)): # Y방향 axes: island for j in range(len(island)): # 색상 : species for k in range(len(species)): try: axes[i][j].scatter(penguins.loc[penguins[&quot;sex&quot;] == sex[i]].loc[penguins[&quot;island&quot;] == island[j]].loc[penguins[&quot;species&quot;] == species[k]][&quot;bill_length_mm&quot;], penguins.loc[penguins[&quot;sex&quot;] == sex[i]].loc[penguins[&quot;island&quot;] == island[j]].loc[penguins[&quot;species&quot;] == species[k]][&quot;bill_depth_mm&quot;]) axes[i][j].set_title(f&quot;sex = {sex[i]} | island = {island[j]}&quot;) except: # 결측치 예외처리 pass # 맨 아래줄에만 xlabel 추가 axes[len(sex)-1, j].set_xlabel(&quot;bill_length_mm&quot;) # 맨 왼쪽에만 ylabel 추가 axes[i, 0].set_ylabel(&quot;bill_depth_mm&quot;)# 그래프 우측에 범례 표시fig.legend(species, title=&quot;species&quot;, bbox_to_anchor=(0.95, 0.5))# 그래프 간격 조정fig.tight_layout(rect=[0,0,0.85,1]) 따로 꾸민 것도 없는데 매우 번잡합니다. 인자별로 몇가지인지를 알아내야 합니다. 결측치 처리가 필요합니다. 안하면 에러납니다. 색상별로 따로 그려야 합니다. 그런데 seaborn을 사용하면 세 줄 만에 끝납니다. 123g = sns.FacetGrid(penguins, row=&quot;sex&quot;, col=&quot;island&quot;, hue=&quot;species&quot;)g.map(sns.scatterplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)g.add_legend() FacetGrid()로 만든 공간을 .map()으로 채웁니다. 데이터셋 이름, 변수 이름, 그리는 방식 외에 다른 내용이 없습니다. 인자가 여럿인 관계를 보기에 아주 좋습니다. 밀도 함수density plot로 바꾸는 것도 간단합니다. 위 코드에서 sns.scatterplot만 sns.kdeplot으로 넣으면 됩니다. 123g = sns.FacetGrid(penguins, row=&quot;sex&quot;, col=&quot;island&quot;, hue=&quot;species&quot;)g.map(sns.kdeplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)g.add_legend() figure-level 함수는 복잡한 그림을 간단히 그릴 때 매우 강력합니다. 이 그림들을 matplotlib으로 그리려면 얼마나 막막할까요. 3.2. figure-level 꾸미기: FacetGrid()seaborn.FacetGrid seaborn.kdeplot seaborn.regplot 이번엔 figure-level 그림을 꾸며 보겠습니다. kdeplot위에 추세선을 겹쳐 그립니다. 추세선이 중간에 안끊기면 좋겠습니다. 신뢰구간은 80% 수준으로 그리고 싶습니다. xlabel을 “Bill Length (mm)”로 바꾸고 ylabel을 “Bill Depth (mm)”로 바꿉니다. 공식 홈페이지의 설명에 힘입어 해냈습니다. 123456g = sns.FacetGrid(penguins, row=&quot;sex&quot;, col=&quot;island&quot;, hue=&quot;species&quot;)g.map(sns.kdeplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, alpha=0.3)g.map(sns.regplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, truncate=False, ci=80, scatter=False)g.set_axis_labels(&quot;Bill Length (mm)&quot;, &quot;Bill Depth (mm)&quot;)g.add_legend() 한 단계 더 꾸며보도록 합시다. xlabel, ylabel 글자를 키우고 싶습니다. 추세선의 신뢰구간 80%를 적어두고 싶습니다. axes마다 붙어 있는 title을 정리하고 싶습니다. 첫 단계에서 막혔습니다 xlabel, ylabel을 수정할 때 set_axis_labels()를 사용했습니다. 공식 홈페이지의 FacetGrid()부분 가이드를 따른 것입니다. 그런데 set_axis_labels()에 대한 설명이 더 이상 없습니다. 혹시나, matplotlib 명령어를 넣어봅니다. fontdict=를 적용합니다 1234567g = sns.FacetGrid(penguins, row=&quot;sex&quot;, col=&quot;island&quot;, hue=&quot;species&quot;)g.map(sns.kdeplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, alpha=0.3)g.map(sns.regplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, truncate=False, ci=80, scatter=False)g.set_axis_labels(&quot;Bill Length (mm)&quot;, &quot;Bill Depth (mm)&quot;, fontdict={&quot;fontsize&quot;:&quot;large&quot;, &quot;color&quot;:&quot;gray&quot;, &quot;fontweight&quot;:&quot;bold&quot;})g.add_legend() figure-level 그림의 세부 설정은 도움말 찾기도 어렵습니다. 매뉴얼에 없는 내용을 상상해서 넣어야 합니다. 아직은 공식 홈페이지가 충만하지 않습니다. 소스코드를 뜯어봐서 기능을 확인하거나 matplotlib 명령어를 숙지하고 대응시켜 시도해야 합니다. 3.3. figure-level의 장단점 공식 튜토리얼에 정리된 장단점은 이렇습니다 하나씩 짚어보겠습니다. (1) 데이터 변수에 따른 축공간 생성 matplotlib에서 변수 갯수를 세어야 했던 것에 비해서 편리합니다. seaborn에 구현된 그림을 제어하는 인자 수가 확실히 적습니다. 번거롭더라도 axes에 직접 접근해서 matplotlib 명령을 쓰는 게 낫습니다. (2) 그림 밖 범례 생성 그림 밖에 붙는 것은 다행입니다. 그러나 통제가 안되어 심각한 갈증을 유발합니다. (3) figure-level 수정 &amp; (4) figure size를 조정하는 인자가 다름 장점보다 단점이 크다고 생각됩니다. 더 쉬운 명령어를 제공한다고 해도 결국은 또 다른 문법입니다. matplotlib 명령어만 해도 정신이 없는데 말이죠. 지금까지의 경험으로 이런 결론이 나옵니다. “figure-level의 가성비는 그리자마자, 손을 더 대기 전이 가장 높다.” 새로운 명령어는 새로운 혼돈입니다. 시각화만 붙잡고 있을 게 아니라 통계분석, 머신러닝도 해야 하거든요. 3.4. figure-level그림이 손대기 어려운 이유github: seaborn source code seaborn 코드를 뜯어보면 클래스 구조는 이렇습니다. multi-plot grids는 Grid 클래스를 상속받는 가족입니다. FacetGrid(), PairGrid(), ClusterGrid 입니다. JointGrid()는 Grid를 상속받지 않습니다. Grid로 만든 공간에 목적에 맞는 그림을 채웁니다. relplot(), displot(), catplot(), lmplot()은 1x1 FacetGrid()로 출력됩니다. 그리고 Grid 클래스는 **figure**의 wrapper입니다. 정확히는 matplotlib.pyplot.subplots()의 wrapper입니다. 따라서 figure, axes에 적용되는 matplotlib 명령이 안통합니다. 그래서 set_axis_labels같은 자체 명령어를 탑재하고 있습니다. 하지만 보셨다시피 도움말이 충분치 않습니다. 3.5. figure-level 그림을 꾸미는 방법Doctor! The heart’s stopped! 한마디로, seaborn 제공 함수는 한계가 큽니다. 기능 자체도 많이 빠져있고 있는 기능도 문서화가 덜 됐습니다. 이걸 믿고 쓰긴 어렵습니다. 하지만 대안이 있습니다. seaborn 그림을 matplotlib 그림으로 간주합니다. figure-level 객체 안으로 한 걸음 들어갑니다. 그리고 figure와 axes를 직접 건드립니다. 개흉 심장마사지를 상상하시면 됩니다. 가슴을 열고 직접 심장을 마사지하는 겁니다. 그러면, 풀지 못했던 난제도 이렇게 풀립니다. 1234567891011121314151617181920212223242526272829g = sns.FacetGrid(penguins, row=&quot;sex&quot;, col=&quot;island&quot;, hue=&quot;species&quot;, margin_titles=True, despine=False)g.map(sns.kdeplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, alpha=0.3)g.map(sns.regplot, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, truncate=False, ci=80, scatter=False)g.add_legend()# xlabel, ylabel 수정g.set_axis_labels(&quot;Bill Length (mm)&quot;, &quot;Bill Depth (mm)&quot;, fontdict={&quot;fontsize&quot;:&quot;large&quot;, &quot;color&quot;:&quot;gray&quot;, &quot;fontweight&quot;:&quot;bold&quot;})# axes 직접 접근, 1차원 행렬로 표현axs = g.axes.ravel()for i, ax in enumerate(axs): # 열 title 수정 if i &lt; 3: ax.set_title(ax.get_title(), fontdict={&quot;fontsize&quot;:&quot;x-large&quot;, &quot;color&quot;:&quot;k&quot;}, pad=12) # 행 title 수정 if i%3 == 2: sex = &quot;Male&quot; if i == 2 else &quot;Female&quot; ax.texts.clear() text = ax.annotate(f&quot;sex = {sex}&quot;, xy=(1.02, .5), xycoords=&quot;axes fraction&quot;, rotation=270, ha=&quot;left&quot;, va=&quot;center&quot;, fontsize=&quot;x-large&quot;, color=&quot;k&quot;)# suptitle 추가g.fig.suptitle(&quot;Penguins dataset summary (ci = 80%) &quot;, fontsize=&quot;xx-large&quot;, fontweight=&quot;bold&quot;, color=&quot;indigo&quot;)# 전반적 크기 조정 g.fig.tight_layout(rect=[0,0,0.88,0.97]) 외웁시다. figure-level 함수는 matplotlib과 못 섞는다. figure-level 함수는 figure와 axes를 한번 더 감싸고 있다. figure-level 함수를 수정하려면 후벼 파는 과정이 필요하다. 출저: https://jehyunlee.github.io/2020/10/06/Python-DS-36-seaborn_matplotlib3/","link":"/2020/11/06/Seaborn_with_Matplotlib_3/"},{"title":"Seaborn with Matplotlib (4)","text":"4. seaborn figure-level 다듬기 seaborn + matplotlib 마지막 시간입니다. figure-level 그림을 그리고 다듬어 봅니다. 자체적으로 지원하는 명령어를 사용해보고, 개흉 심장마사지 방법을 알아보겠습니다. seaborn API seaborn tutorial 4.1. figure-level으로 편리하게 그리기: pairplot()wikipedia: Exploratory data analysis wikipedia: Anscombe’s quartet autodesk: datasaurus seaborn.pairplot EDA에서 피할 수 없는 과정이 상관도입니다. 두 변수 사이의 관계를 그려 관계를 파악합니다. 대개 산점도scatter plot나 밀도 함수density plot를 쓰고, 추세선regression을 덧붙이는 경우도 있습니다. 데이터 분포는 통계값으로만 확인하면 안됩니다. 평균과 표준편차는 같은데 분포는 다른 경우가 많습니다. 반드시 분포를 그려 봐야 합니다. Anscombe’s quartet이 아주 좋은 예시입니다. N개의 인자 사이에는 \\(N(N−1)/2\\)가지 관계가 있습니다. seaborn의 pairplot()은 여러 인자 사이의 수많은 상관도를 쉽게 그려줍니다. 붓꽃iris 데이터셋으로 상관도를 그려봅니다. 꽃받침sepal과 꽃잎petal, 길이length와 폭width 네 가지 인자로부터 여섯 가지의 상관도, 네 가지의 히스토그램이 나옵니다. 코드 한 줄로 그려봅시다. 123import seaborn as snsiris = sns.load_dataset(&quot;iris&quot;) 종species별 분포를 그려봅니다. 1g = sns.pairplot(iris, hue=&quot;species&quot;) hue=라는 인자 하나만 추가했을 뿐입니다. 그림이 갑자기 예뻐졌습니다. 히스토그램도 밀도함수로 바뀌어서 훨씬 보기 좋습니다. 여기에 다른 그림을 겹쳐보겠습니다. 대각선 위 : 2차원 밀도함수 + 추세선 대각선 아래 : 2차원 밀도함수 + 산포도 seaborn figure-level 기능만으로 가능합니다. .map_lower()와 .map_upper를 사용합니다. 123g = sns.pairplot(iris, hue=&quot;species&quot;, kind=&quot;kde&quot;, plot_kws={&quot;alpha&quot;:0.3})g.map_lower(sns.scatterplot)g.map_upper(sns.regplot, scatter=False, truncate=False, ci=False) map_lower()와 map_upper()는 대각선의 위와 아래에 그림을 덧씌우는map 함수입니다. 앞서 완성한 pairplot에 새로운 그림을 덧붙입니다. 인자로는 함수 이름과 이 함수의 옵션들을 연달아 넣어 줍니다. 함수에 맞는 인자를 넣어줘야 합니다. 4.2. figure-level 그림 뼈대부터 세우기: PairGrid()seaborn.PairGrid pairplot을 단계별로 그리는 방법입니다. 코드 양은 pairplot()한 줄보다 늘어납니다. 그러나 자유도가 늘어납니다. .pairplot() 으로는 위 아래를 완전히 다르게 할 수 없습니다. 대각선의 아래엔 산점도와 추세선을 그리고 대각선 위에는 밀도함수만 그려보겠습니다. 123456789101112131415# 틀 만들기g = sns.PairGrid(iris, hue=&quot;species&quot;, diag_sharey=False)# diagonalg.map_diag(sns.kdeplot, fill=True)# lowerg.map_lower(plt.scatter, s=30, edgecolor=&quot;w&quot;)g.map_lower(sns.regplot, scatter=False, truncate=False, ci=False)# upperg.map_upper(sns.kdeplot, alpha=0.3)# legendg.add_legend() .pairplot()과 같은 형식의 그림이 나왔습니다. 내 의도를 더 잘 반영할 수 있게 되었습니다. 8번째 줄에는 plt.scatter가 들어가 있습니다. .pairplot()도 마찬가지지만 .map()에는 seaborn대신 matplotlib - 함수를 사용할 수 있습니다. 중복되는 오른쪽 윗부분을 생략할 수 있습니다. corner=True를 넣어줍니다. 123456789101112g = sns.PairGrid(iris, hue=&quot;species&quot;, diag_sharey=False, corner=True)# diagonalg.map_diag(sns.kdeplot, fill=True)# lowerg.map_lower(plt.scatter, s=30, edgecolor=&quot;w&quot;)g.map_lower(sns.regplot, scatter=False, truncate=False, ci=False)g.map_lower(sns.kdeplot, alpha=0.3)# legendg.add_legend() 이제 작은 수정을 해봅니다. 대각선에 사각 테두리 치기 범례를 그림 안으로 가져오기 가슴을 열고 심장마사지를 할 시간입니다. 4.3. figure-level 그림 객체 접근seaborn.FacetGrid matplotlib.figure.Figure matplotlib.axes.Axes matplolib 그림은 figure와 axes로 나뉩니다. seaborn 그림도 결국 matplotlib 그림입니다. seaborn의 figure와 axes에 접근하면 됩니다. 4.3.1. axes: .axes 이전 글에서 axes를 이용해 jointplot을 재현했습니다. axes 객체는 .axes를 이용해 접근 가능합니다. 1g.axes 확인 결과 AxesSubplot의 array입니다. 4.3.1.1. 특정 axes 윤곽선 그리기Seaborn with Matplotlib (2) Spines &amp; Grids 정체를 알았으니 axes[i][j]로 접근할 수 있습니다. g.axes[i][j]의 spine을 보이게 합니다. 12345678910111213141516g = sns.PairGrid(iris, hue=&quot;species&quot;, diag_sharey=False, corner=True)# diagonalg.map_diag(sns.kdeplot, fill=True)for i in range(4): g.axes[i][i].spines[&quot;left&quot;].set_visible(True) g.axes[i][i].spines[&quot;top&quot;].set_visible(True) g.axes[i][i].spines[&quot;right&quot;].set_visible(True) # lowerg.map_lower(plt.scatter, s=30, edgecolor=&quot;w&quot;)g.map_lower(sns.regplot, scatter=False, truncate=False, ci=False)g.map_lower(sns.kdeplot, alpha=0.3)# legendg.add_legend() 4.3.1.2. 범례 조정seaborn axisgrid.py 소스코드 matplotlib.axes.Axes.get_legend_handles_labels matplotlib.axes.Axes.legend legend 정보를 가져와서 legend로 삽입합니다. 대개 legend에 필요한 handles와 labels는 .get_legend_handles_labels()로 가져옵니다. 하지만 seaborn figure-level 그림엔 이 명령이 통하지 않습니다. private처럼 보이는 ._legend_data를 사용합니다. 공식 문서에 없습니다. 소스 코드를 뒤적여야 합니다. ._legend_data 정체를 확인합니다. 범례가 dict 형식으로 들어 있습니다. 1g._legend_data 아래 코드를 추가하여 범례를 이동합니다. 12345678handles = g._legend_data.values()labels = g._legend_data.keys()# axes[1][0] 기준으로 오른쪽 멀리 범례 삽입g.axes[1][0].legend(handles=handles, labels=labels, bbox_to_anchor=(3.45, 1), fontsize=&quot;large&quot;, frameon=False ) 이런 방식으로 특정 axes만 제어할 수 있습니다. 원하는 그림, 글자을 넣을 수 있고 특정 데이터의 색상을 변경할 수 있습니다. 4.3.2. figure: .fig 개별 공간은 axes로 제어하지만 여러 axes가 연관된 공간은 figure로 제어합니다. figure 객체는 .fig를 이용해 접근 가능합니다 1type(g.fig) type()으로 감싸주지 않으면 그림이 통으로 출력됩니다. 4.3.2.1. ylabel alignmentmatplolib.figure.Figure #align_ylabels matplotlib Align y-labels 위 그림에서 ylabel 줄이 잘 맞지 않습니다. 맨 왼쪽 아래 axes[3][0]의 label이 비죽 나왔네요. 다른 ylabel을 움직여서 열을 맞춰 봅시다. 위 그림 코드의 맨 마지막 줄에 한 줄만 추가합니다 1g.fig.align_ylabels(g.axes[:,0]) 4.3.2.2. 범례 조정matplolib.figure.Figure #legend 범례는 figure 기준으로도 삽입 가능합니다. 아래 코드는 위의 axes[1][0].legend()와 동일합니다. 전체적인 그림의 위치를 잡을 수 있어 더 좋습니다. 123g.fig.legend(handles=handles, labels=labels, bbox_to_anchor=(0.75, 0.75), fontsize=&quot;large&quot;) 4.3.2.3. suptitlematplolib.figure.Figure #suptitle 그림 전체에 제목을 붙일 수 있습니다. 1234g.fig.suptitle(&quot;iris dataset&quot;, y=1.01, weight=&quot;bold&quot;, fontsize=&quot;x-large&quot; )g.fig.tight_layout() 4.3.2.4. facecolor, edgecolormatplolib.figure.Figure #set_facecolor 그림 바탕색을 칠합니다. 1g.fig.set_facecolor(&quot;whitesmoke&quot;) 4.3.3. 최종 x, ylabel까지 정리하면 이렇습니다. 123456789101112131415161718192021222324252627282930313233343536373839g = sns.PairGrid(iris, hue=&quot;species&quot;, diag_sharey=False, corner=True)# diagonalg.map_diag(sns.kdeplot, fill=True)for i in range(4): g.axes[i][i].spines[&quot;left&quot;].set_visible(True) g.axes[i][i].spines[&quot;top&quot;].set_visible(True) g.axes[i][i].spines[&quot;right&quot;].set_visible(True) # lowerg.map_lower(plt.scatter, s=30, edgecolor=&quot;w&quot;)g.map_lower(sns.regplot, scatter=False, truncate=False, ci=False)g.map_lower(sns.kdeplot, alpha=0.3)# legendhandles = g._legend_data.values()labels = g._legend_data.keys()g.fig.legend(handles=handles, labels=labels, bbox_to_anchor=(0.75, 0.75), fontsize=&quot;large&quot;)# x, y labelslabels = [&quot;Sepal Length&quot;, &quot;Sepal Width&quot;, &quot;Petal Length&quot;, &quot;Petal Width&quot;]font_labels = {&quot;fontsize&quot;:&quot;large&quot;, &quot;color&quot;:&quot;gray&quot;, &quot;fontweight&quot;:&quot;bold&quot;}for i in range(4): g.axes[3, i].set_xlabel(labels[i], fontdict=font_labels) g.axes[i, 0].set_ylabel(labels[i], fontdict=font_labels)# ylabel alignmentg.fig.align_ylabels(g.axes[:,0])# suptitleg.fig.suptitle(&quot;iris dataset&quot;, y=1.01, weight=&quot;bold&quot;, fontsize=&quot;x-large&quot; )g.fig.tight_layout()# facecolorg.fig.set_facecolor(&quot;whitesmoke&quot;) 5. 결론 네 편의 글에 걸쳐 matplotlib + seaborn 연계기를 정리했습니다. 개별적인 코드와 기술보다는 본질에 집중합시다. figure와 axes를 통하면 웬만한건 다 된다는 겁니다. 0.11에서 seaborn의 공식문서가 대폭 정리됐습니다. 하지만 아직 부족한 점이 많습니다. 특히 많은 경우 matplotlib에 숙달되었다고 전제합니다. 일단 matplotlib부터 제대로 익혀봅시다. 출저: https://jehyunlee.github.io/2020/10/10/Python-DS-37-seaborn_matplotlib4/","link":"/2020/11/06/Seaborn_with_Matplotlib_4/"},{"title":"Seaborn with Matplotlib (2)","text":"2. seaborn + matplotlib을 이용한 jointplot 보완 seaborn을 matplotlib과 섞어쓰는 방법입니다. 4부 중 두 번째 시간입니다. seaborn jointplot의 단점을 보완합니다. 2.1. seaborn jointplotseaborn API seaborn의 jointplot은 매력적인 기능입니다. 두 변수 각각의 분포와 2차원 분포를 함께 보여줍니다. 각각의 분포는 histogram과 kdeplot으로 표현할 수 있고 2차원 분포는 scatterplot, regression, kdeplot, hexbin등으로 표현할 수 있습니다. 만약 matplotlib에서 일일이 만들어야 한다면 눈물이 앞을 가릴 일입니다. seaborn에서는 단 한 줄로 이런 그림들이 탄생합니다. 2.2. multiple jointplot 그러나 seaborn 만으로는 여러 jointplot이 합쳐진 그림을 표현할 수 없습니다. y축을 공유시키는 것 만으로도 훨씬 기능이 강력해질텐데요. 현실적으로 가장 빠른 방법은 파워포인트입니다. 같은 y축 범위로 그림을 여러 개 그린 후 붙이면 됩니다. 그러나 수작업이 동반되고, 오류 가능성이 커집니다. seaborn과 matplotlib의 힘을 합해서 그려봅시다. 2.3. matplotlib + seaborn 지난 글에서 seaborn과 matplotlib이 섞일 수 있음을 보였습니다. matplotlib으로 틀을 만들고 여기에 seaborn을 삽입합니다. 12345import matplotlib.pyplot as pltimport seaborn as snspenguins = sns.load_dataset(&quot;penguins&quot;) # 펭귄 데이터셋으로 시작합니다.penguins.head() 가로축에 두 개의 데이터 : bill_length_mm, flipper_length_mm, 세로축에 한 개의 데이터 : bill_depth_mm를 놓아보겠습니다. 2.3.1. 축공간 배열 : gridspecmatplotlib.pyplot.subplots matplotlib.gridspec.GridSpec 저는 matplotlib을 객체지향 방식으로 사용할 때 이렇게 합니다. fig, ax = plt.subplots() 명령으로 figure와 axes를 만드는데 가로세로 여러 axes를 만들 때는 ncols, nrows 인자를 사용합니다. 그러나 이렇게 하면 모든 축공간의 크기가 같아집니다. jointplot은 축공간의 크기가 일정하지 않습니다. gridspec을 이용해서 비대칭 축공간을 만듭니다. 12345678910111213fig = plt.figure(figsize=(12,7))widths = [4, 4, 1]heights = [1, 4]### 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights)### 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)]) axs[i].text(0.5, 0.5, f&quot;axs[{i}]&quot;, fontdict={&quot;horizontalalignment&quot;:&quot;center&quot;, &quot;color&quot;:&quot;gray&quot;}) 축공간은 원래 2차원 배열입니다. 하지만 이를 별로 좋아하지 않아서, dictionary를 사용해 1차원으로 취급합니다. 개인적인 취향입니다. 꼭 따르지 않아도 좋습니다. 2.3.2. 첫 번째 jointplot : scatterplot + kdeplotseaborn.scatterplot seaborn.kdeplot 좌측 하단에 첫 번째 2차원 분포도를 그립니다. x축 변수는 bill_length_nm, y축 변수는 bill_depth_mm로 지정합니다. seaborn 명령의 ax= 인자를 사용해 축공간을 지정합니다. 1234567891011121314151617181920fig = plt.figure(figsize=(12,7))widths = [4, 4, 1]heights = [1, 4]# 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights)# 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)])# 3. bill_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[3])# 3.2. scatterplotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[3])fig.tight_layout() 2.3.3. 첫 번째 jointplot : 1차원 kdeplot 첫 번째 jointplot의 상단과 우측에 1차원 분포도를 그립니다. 두 가지를 주의해야 합니다. 중심이 되는 jointplot과 가로세로 범위를 일치시켜야 합니다. 1차원 분포도의 눈금과 수치는 필요없으니 제거합니다. 123456789101112131415161718192021222324252627282930313233343536373839fig = plt.figure(figsize=(12,7))widths = [4, 4, 1]heights = [1, 4]### 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights)### 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)])### 3. bill_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[3])# 3.2. scatterplotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[3])# 3.3. histogram (bill_length_mm)sns.kdeplot(&quot;bill_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[0], legend=False)axs[0].set_xlim(axs[3].get_xlim())axs[0].set_xlabel('')axs[0].set_xticklabels([])axs[0].spines[&quot;left&quot;].set_visible(False)axs[0].spines[&quot;top&quot;].set_visible(False)axs[0].spines[&quot;right&quot;].set_visible(False)# 3.3. histogram (bill_depth_mm)sns.kdeplot(y=&quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[5], legend=False)axs[5].set_ylim(axs[3].get_ylim())axs[5].set_ylabel('')axs[5].set_yticklabels([])axs[5].spines[&quot;bottom&quot;].set_visible(False)axs[5].spines[&quot;top&quot;].set_visible(False)axs[5].spines[&quot;right&quot;].set_visible(False)fig.tight_layout() 2.3.4. 두 번째 jointplot 같은 요령으로 두 번째 분포도 도시합니다. 첫 번째 jointplot에서 x 변수만 바꿔주면 됩니다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455fig = plt.figure(figsize=(12,7))widths = [4, 4, 1]heights = [1, 4]### 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights)### 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)])### 3. bill_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[3])# 3.2. scatterplotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[3])# 3.3. histogram (bill_length_mm)sns.kdeplot(&quot;bill_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[0], legend=False)axs[0].set_xlim(axs[3].get_xlim())axs[0].set_xlabel('')axs[0].set_xticklabels([])axs[0].spines[&quot;left&quot;].set_visible(False)axs[0].spines[&quot;top&quot;].set_visible(False)axs[0].spines[&quot;right&quot;].set_visible(False)# 3.3. histogram (bill_depth_mm)sns.kdeplot(y=&quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[5], legend=False)axs[5].set_ylim(axs[3].get_ylim())axs[5].set_ylabel('')axs[5].set_yticklabels([])axs[5].spines[&quot;bottom&quot;].set_visible(False)axs[5].spines[&quot;top&quot;].set_visible(False)axs[5].spines[&quot;right&quot;].set_visible(False)### 4. flipper_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[4])# 3.2. scatterplotsns.scatterplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[4])# 3.3. histogram (bill_length_mm)sns.kdeplot(&quot;flipper_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[1], legend=False)axs[1].set_xlim(axs[4].get_xlim())axs[1].set_xlabel('')axs[1].set_xticklabels([])axs[1].spines[&quot;left&quot;].set_visible(False)axs[1].spines[&quot;top&quot;].set_visible(False)axs[1].spines[&quot;right&quot;].set_visible(False)fig.tight_layout() 2.3.5. 부대효과 조정 : spines, grids, 간격 데이터는 모두 올라갔으니 부대 효과를 조정합니다. 불필요한 요소(ex. spines)는 제거하고, 애매한 요소(ex. 위치)는 grid로 명확히 합니다 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576fig = plt.figure(figsize=(12,7))widths = [4, 4, 1]heights = [1, 4]### 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights, wspace=0.03, hspace=0.03) # setting spaces### 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)])### 3. bill_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[3], zorder=1)# 3.2. scatterplotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[3], zorder=2)# 3.3. histogram (bill_length_mm)sns.kdeplot(&quot;bill_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[0], legend=False, zorder=1)axs[0].set_xlim(axs[3].get_xlim())axs[0].set_xlabel('')axs[0].set_xticklabels([])axs[0].spines[&quot;left&quot;].set_visible(False)axs[0].spines[&quot;top&quot;].set_visible(False)axs[0].spines[&quot;right&quot;].set_visible(False)# 3.3. histogram (bill_depth_mm)sns.kdeplot(y=&quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[5], legend=False, zorder=1)axs[5].set_ylim(axs[3].get_ylim())axs[5].set_ylabel('')axs[5].set_yticklabels([])axs[5].spines[&quot;bottom&quot;].set_visible(False)axs[5].spines[&quot;top&quot;].set_visible(False)axs[5].spines[&quot;right&quot;].set_visible(False)### 4. flipper_length_mm vs bill_depth_mm# 4.1. kdeplotsns.kdeplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[4], zorder=1)# 4.2. scatterplotsns.scatterplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[4], zorder=2)# 4.3. histogram (flipper_length_mm)sns.kdeplot(&quot;flipper_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[1], legend=False, zorder=1)axs[1].set_xlim(axs[4].get_xlim())axs[1].set_xlabel('')axs[1].set_xticklabels([])axs[1].spines[&quot;left&quot;].set_visible(False)axs[1].spines[&quot;top&quot;].set_visible(False)axs[1].spines[&quot;right&quot;].set_visible(False)### 5. unnecessary elements removal# 5.1. upper-right axesaxs[2].axis(&quot;off&quot;)# 5.2. margin kdeplot scale unificationhist_range_max = max(axs[0].get_ylim()[-1], axs[1].get_ylim()[-1], axs[5].get_xlim()[-1])for i in range(len(widths)-1): axs[i].set_ylim(0, hist_range_max)axs[5].set_xlim(0, hist_range_max)# 5.3. redundent labels and titles removalaxs[1].set_yticklabels([])axs[1].set_ylabel('')axs[4].set_yticklabels([])axs[4].set_ylabel('')# 5.4. gridsfor i in range(len(heights)*len(widths)): axs[i].grid(&quot;on&quot;, color=&quot;lightgray&quot;, zorder=0)fig.tight_layout() 1차원 분포의 스케일도 맞춰주었습니다. 그러나 꼭 필요한 작업인지는 의구심이 듭니다. ‘밀도’라는 정의에 맞게 넓이를 1로 만드는 과정이 포함되어 있는데, 이로 인해 x축 스케일이 크면 높이가 낮아지는 경향이 있기 때문입니다. 데이터의 범위에 집중하고 싶다면 스케일을 맞추지 않는 것이 나을지도 모릅니다. 2.3.6. 마무리 작업seaborn tutorial: controlling figure aesthetics 데이터를 표현하는 작업은 사실상 완료되었습니다. 폰트의 크기, 색상 등 가독성을 높입니다. 이 때도 seaborn의 set_style()과 set_context()를 사용하면 편리합니다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788sns.set_style(&quot;white&quot;)sns.set_context(&quot;talk&quot;)fig = plt.figure(figsize=(14,8))widths = [4, 4, 1]heights = [1, 4]### 1. gridspec preparationspec = fig.add_gridspec(ncols=3, nrows=2, width_ratios=widths, height_ratios=heights, wspace=0.03, hspace=0.03) # setting spaces### 2. setting axesaxs = {}for i in range(len(heights)*len(widths)): axs[i] = fig.add_subplot(spec[i//len(widths), i%len(widths)])### 3. bill_length_mm vs bill_depth_mm# 3.1. kdeplotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[3], zorder=1, legend=False)# 3.2. scatterplotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[3], zorder=2, legend=False)# 3.3. histogram (bill_length_mm)sns.kdeplot(&quot;bill_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[0], legend=False, zorder=1, fill=True)axs[0].set_xlim(axs[3].get_xlim())axs[0].set_xlabel('')axs[0].set_xticklabels([])axs[0].spines[&quot;left&quot;].set_visible(False)axs[0].spines[&quot;top&quot;].set_visible(False)axs[0].spines[&quot;right&quot;].set_visible(False)# 3.3. histogram (bill_depth_mm)sns.kdeplot(y=&quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[5], legend=False, zorder=1, fill=True)axs[5].set_ylim(axs[3].get_ylim())axs[5].set_ylabel('')axs[5].set_yticklabels([])axs[5].spines[&quot;bottom&quot;].set_visible(False)axs[5].spines[&quot;top&quot;].set_visible(False)axs[5].spines[&quot;right&quot;].set_visible(False)### 4. flipper_length_mm vs bill_depth_mm# 4.1. kdeplotsns.kdeplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, alpha=0.3, ax=axs[4], zorder=1)# 4.2. scatterplotsns.scatterplot(&quot;flipper_length_mm&quot;, &quot;bill_depth_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[4], zorder=2)# 4.3. histogram (flipper_length_mm)sns.kdeplot(&quot;flipper_length_mm&quot;, data=penguins, hue=&quot;species&quot;, ax=axs[1], legend=False, zorder=1, fill=True)axs[1].set_xlim(axs[4].get_xlim())axs[1].set_xlabel('')axs[1].set_xticklabels([])axs[1].spines[&quot;left&quot;].set_visible(False)axs[1].spines[&quot;top&quot;].set_visible(False)axs[1].spines[&quot;right&quot;].set_visible(False)### 5. unnecessary elements removal# 5.1. upper-right axesaxs[2].axis(&quot;off&quot;)# 5.2. margin kdeplot scale unificationhist_range_max = max(axs[0].get_ylim()[-1], axs[1].get_ylim()[-1], axs[5].get_xlim()[-1])for i in range(len(widths)-1): axs[i].set_ylim(0, hist_range_max)axs[5].set_xlim(0, hist_range_max)# 5.3. redundent labels and titles removalaxs[1].set_yticklabels([])axs[1].set_ylabel('')axs[4].set_yticklabels([])axs[4].set_ylabel('')# 5.4. gridsfor i in range(len(heights)*len(widths)): axs[i].grid(&quot;on&quot;, color=&quot;lightgray&quot;, zorder=0) # 5.5. labelsfont_label = {&quot;color&quot;:&quot;gray&quot;}axs[3].set_xlabel(&quot;Bill Legnth (mm)&quot;, fontdict=font_label, labelpad=12)axs[3].set_ylabel(&quot;Bill Depth (mm)&quot;, fontdict=font_label, labelpad=12)axs[4].set_xlabel(&quot;Flipper Legnth (mm)&quot;, fontdict=font_label, labelpad=12)axs[0].set_ylabel(&quot;Density&quot;, fontdict=font_label, labelpad=12)axs[5].set_xlabel(&quot;Density&quot;, fontdict=font_label, labelpad=12)fig.tight_layout() jointplot의 한계인 2중 jointplot이 구현되었습니다. 2.3.7. generalize : 함수로 만들기 하는 김에, 기능을 일반화합시다. 다중 jointplot 제작 기능을 함수로 만들고, x 변수의 수와 그림 크기, 1차원 분포 스케일 통일 등을 인자로 만듭니다 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163def jointplots(xs, y, data, hue=None, height=6, ratio=5, space=0.2, xlabels=None, ylabel=None, margin_norm=False): &quot;&quot;&quot; ------------------- Input Parameters ------------------- xs : (list or str) feature name(s) of data y : (str) feature name of data data : (pandas.DataFrame) hue : (str) semantic variable that is mapped to determine the color of plot elements. Semantic variable that is mapped to determine the color of plot elements. height : (float) size of the figure ratio : (float) ratio of the joint axes height to marginal axes height. space : (float) space between the joint and marginal axes xlabels : (list or str) xlabels ylabel : (str) ylabel margin_norm : (boolean) if True, kdeplots at marginal axes have same scale. &quot;&quot;&quot; ### 1. input check # input type assert isinstance(xs, list) or isinstance(xs, str) if isinstance(xs, list): assert all([isinstance(x, str) for x in xs]) else: xs = [xs] if xlabels != None: assert isinstance(xlabels, list) or isinstance(xlabels, str) if isinstance(xlabels, list): assert all([isinstance(xlabel, str) for xlabel in xlabels]) else: xlabels = [xlabels] if ylabel != None: assert isinstance(ylabel, str) if hue != None: assert isinstance(hue, str) # input data assert all([x in data.columns for x in xs]) assert y in data.columns if hue != None: assert hue in data.columns ### 2. figure h_margin = height / (ratio + 1) h_joint = height - h_margin if isinstance(xs, list): n_x = len(xs) else: n_x = 1 widths = [h_joint] * n_x + [h_margin] heights = [h_margin, h_joint] ncols = len(widths) nrows = len(heights) fig = plt.figure(figsize=(sum(widths), sum(heights))) ### 3. gridspec preparation spec = fig.add_gridspec(ncols=ncols, nrows=nrows, width_ratios = widths, height_ratios = heights, wspace=space, hspace=space ) ### 4. setting axes axs = {} for i in range(ncols * nrows): axs[i] = fig.add_subplot(spec[i//ncols, i%ncols]) ### 5. jointplots (scatterplot + kdeplot) for i, x in enumerate(xs, ncols): if i == ncols: legend=True else: legend=False sns.kdeplot(x=x, y=y, data=data, hue=hue, alpha=0.3, ax=axs[i], zorder=2, legend=False) sns.scatterplot(x=x, y=y, data=data, hue=hue, alpha=0.8, ax=axs[i], zorder=3, legend=legend) ### 6. kdeplots at marginal axes axs[ncols-1].axis(&quot;off&quot;) axes_mx = list(range(ncols-1)) axes_my = 2*ncols - 1 for i, x in zip(axes_mx, xs): sns.kdeplot(x=x, data=data, hue=hue, fill=True, ax=axs[i], zorder=2, legend=False) axs[i].set_xlim(axs[i+ncols].get_xlim()) axs[i].set_xlabel(&quot;&quot;) axs[i].set_xticklabels([]) axs[i].spines[&quot;left&quot;].set_visible(False) axs[i].spines[&quot;top&quot;].set_visible(False) axs[i].spines[&quot;right&quot;].set_visible(False) sns.kdeplot(y=y, data=data, hue=hue, fill=True, ax=axs[axes_my], zorder=2, legend=False) axs[axes_my].set_ylim(axs[ncols].get_ylim()) axs[axes_my].set_ylabel(&quot;&quot;) axs[axes_my].set_yticklabels([]) axs[axes_my].spines[&quot;bottom&quot;].set_visible(False) axs[axes_my].spines[&quot;top&quot;].set_visible(False) axs[axes_my].spines[&quot;right&quot;].set_visible(False) if margin_norm == True: hist_range_max = max([axs[m].get_ylim()[-1] for m in axes_mx] + [axs[axes_my].get_xlim()[-1]]) for i in axes_mx: axs[i].set_ylim(0, hist_range_max) axs[axes_my].set_xlim(0, hist_range_max) ### 7. unnecessary elements removal # 7.1. labels and ticklabels axes_j = list(range(ncols, 2*ncols-1)) for i in axes_j: if i != ncols: axs[i].set_ylabel(&quot;&quot;) axs[i].set_yticklabels([]) # 7.2. marginal axes for i in axes_mx: if i != 0: axs[i].set_ylabel(&quot;&quot;) axs[i].grid(&quot;on&quot;, color=&quot;lightgray&quot;, zorder=0) axs[i].set_yticklabels([]) yticks = axs[i].get_yticks() ylim = axs[i].get_ylim() for ytick in yticks: if 0 &lt; ytick &lt; ylim[-1]: axs[i].text(axs[i].get_xlim()[0], ytick, str(ytick), fontdict={&quot;verticalalignment&quot;:&quot;center&quot;}) axs[axes_my].grid(&quot;on&quot;, color=&quot;lightgray&quot;, zorder=0) axs[axes_my].set_xticklabels([]) axes_my_xticks = axs[axes_my].get_xticks() axes_my_xlim = axs[axes_my].get_xlim() for xtick in axes_my_xticks: if 0 &lt; xtick &lt; axes_my_xlim[-1]: axs[axes_my].text(xtick, axs[axes_my].get_ylim()[0], str(xtick), rotation=270, fontdict={&quot;horizontalalignment&quot;:&quot;center&quot;}) # 7.3. labels font_label = {&quot;color&quot;: &quot;gray&quot;, &quot;fontsize&quot;:&quot;large&quot;} labelpad = 12 for i, x in zip(axes_j, xlabels): axs[i].set_xlabel(x, fontdict=font_label, labelpad=labelpad) if i == ncols: axs[i].set_ylabel(ylabel, fontdict=font_label, labelpad=labelpad) axs[0].set_ylabel(&quot;Density&quot;, fontdict=font_label, labelpad=labelpad) axs[2*ncols-1].set_xlabel(&quot;Density&quot;, fontdict=font_label, labelpad=labelpad) fig.align_ylabels([axs[0], axs[ncols]]) fig.align_xlabels([axs[x] for x in range(ncols, 2*ncols)]) plt.tight_layout() return fig, axs X인자의 수가 바뀌어도 jointplot이 안정적으로 그려집니다. 1차원 분포도 표현 방식을 바꾸어 전보다 깔끔해졌습니다. 123jointplots([&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;], &quot;body_mass_g&quot;, penguins, hue=&quot;species&quot;, height=8, ratio=5, space=0.03, xlabels=[&quot;Bill Length (mm)&quot;, &quot;Bill Depth (mm)&quot;, &quot;Flipper Length (mm)&quot;], ylabel=&quot;Body Mass (g)&quot;) 2.4. 결론 seaborn의 jointplot을 가져다 쓰는 데 그치지 않았습니다. matplotlib의 객체지향 방식을 이용해 seaborn의 한계를 벗어날 수 있었습니다. 본 예제에서는 scatterplot과 2차원 kdeplot만 결합했습니다. 그러나 이 외에도 seaborn과 matplotlib이 제공하는 거의 모든 기능을 결합할 수 있습니다. 매뉴얼의 한계에 얽매이지 말고 상상력을 동원해 보시면 어떨까요. 출저: https://jehyunlee.github.io/2020/10/03/Python-DS-35-seaborn_matplotlib2/","link":"/2020/11/06/Seaborn_with_Matplotlib_2/"},{"title":"Part01. 데이터 이해","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터의 이해1절. 데이터와 정보1. 데이터의 정의와 특성 데이터의 정의 데이터는 1646년 영군 문헌에 처음 등장, 라틴어인 dare(주다)의 과거분사형으로 주어진 것이라는 의미 데이터는 추론과 추정의 근거를 이루는 사실 단순한 객체로서의 가치뿐 아니라 다른 객체와의 상호관계 속에서 가치를 가짐 데이터의 특성 구분 특성 존재적 특성 객관적 사실(fact, raw material) 당위적 특성 추론·예측·전망·추정을 위한 근거(basis) 2. 데이터의 유형 구분 형태 예 특징 정성적 데이터(qualitative data) 언어, 문자 등 회사 매출 증가 저장·검색·분석에 많은 비용 소모 정량적 데이터(quantitative data) 수치, 도형, 기호 등 나이, 몸무게, 주가 등 정형화된 데이터로 비용 소모 적음 3. 지식경영의 핵심 이슈 데이터는 지식경영의 핵심 이슈인 암묵지(tacit knowledge)와 형식지(explicit knowledge)의 상호 작용에 있어 중요한 역할을 함 구분 의미 예 특징 상호작용 암묵지 학습/경험으로 개인에 체화되어 있지만 드러나지 않는 지식 김장, 자전거 타기 사회적으로 중요하지만 공유 어려움 공통화, 내면화 형식지 문서/매뉴얼처럼 형상화된 지식 교과서, 비디오, DB 전달과 공유 용이 표출화, 연결화 암묵지: 개인에게 축적된 내면화(internalization)된 지식 → 조직의 지식으로 공통화(socialization) 형식지: 언어, 기호, 숫자로 표출화(externalization)된 지식 → 개인의 지식으로 연결화(combination) 4. 데이터와 정보의 관계 DIKW의 정의 구분 내용 데이터(data) 개별 데이터 자체로는 의미가 중요하지 않은 객관적 사실 정보(information) 데이터 가공, 처리와 데이터 간 연관관계 속에서 의미가 도출된 것 지식(knowledge) 도출된 정보를 구조화해 유의미한 정보 분류, 개인 경험과 결합시켜 고유의 지식으로 내재화된 것 지혜(wisdon) 지식의 축적과 아이디어가 결합된 창의적 산물 DIKW 피라미드 지혜: 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어 i.e. A마트의 다른 상품도 B마트보다 저렴할 것이다. 지식: 상호 연결된 정보 패턴을 이해하여 이를 토대로 예측한 결과물 i.e. 상대적으로 저렴한 A마트에서 연필을 사야겠다. 정보: 데이터 가공 및 상관관계를 이해하여 패턴을 인식하고 의미를 부여한 데이터 i.e. A마트에서 파는 연필이 더 저렴하다. 데이터: 존재형식을 불문하고, 타 데이터와 상관관계 있는 가공 전의 순수한 수치나 기호 : 연필은 A마트에서는 100원에, B마트에서는 200원에 판매된다. 2절. 데이터베이스 정의와 특징1. 용어의 연혁 1950년대: 미국, 군비상황 집중 관리를 위해 컴퓨터 도서관 설립 → 데이터의 기지(base)라는 뜻의 데이터베이스 탄생 1975년: 미국의 CAC가 KORSTIC을 통해 서비스되며 우리나라에서 데이터베이스 이용 1980년대 중반: 국내 데이터베이스 관련 기술의 연구 및 개발 2. 데이터베이스 정의 1차 개념 확대: 정형데이터 관리 2차 개념 확대: 빅데이터의 출현으로 비정형데이터 포함 3. 데이터베이스의 특징 데이터베이스의 일반적 특징 데이터베이스 특징 설명 통합된(integrated) 데이터 동일한 내용의 데이터가 중복되지 않음, 데이터 중복은 관리상 부작용 초래 저장된(stored) 데이터 컴퓨터가 접근할 수 있는 저장 매체에 저장, 데이터베이스는 컴퓨터 기술 바탕 공용(shared) 데이터 여러 사용자가 서로 다른 목적으로 데이터 이용, 대용량화 &amp; 복잡한 구조 변화되는(changable) 데이터 데이터베이스에 저장된 내용은 데이터베이스의 현 시점의 상태를 나타냄, 항상 변화하면서도 현재의 정확한 데이터 유지 필요 데이터베이스의 다양한 측면에서의 특징 정보 축적 및 전달 기계가독성: 일정 형식에 따라 컴퓨터 등 정보처리기기가 읽고 쓸 수 있음 검색가독성: 다양한 방법으로 필요한 정보 검색 원격조작성: 정보통신망을 통해 원거리에서도 즉시 온라인 이용 정보 이용 이용자의 정보 요구에 맞게 다양한 정보를 신속하게 획득 원하는 정보를 정확하고 경제적으로 찾아낼 수 있음 정보 관리 정보를 일정한 질서와 구조에 따라 정리, 저장, 검색, 관리할 수 있도록 하여 방대한 양의 정보를 체계적으로 축적 새로운 내용의 추가 또는 갱신이 용이 정보 기술 발전 정보처리, 검색·관리 소프트웨어, 관련된 하드웨어, 네트워크기술 발전 견인 경제·산업 정보 인프라적 특성 3절. 데이터베이스의 활용 기업내부 데이터베이스 1980년대 기업내부 데이터베이스 OLTP(On-Line Transaction Processing) 호스트 컴퓨터와 온라인으로 접속된 여러 단말간 처리 형태의 하나 호스트 컴퓨터가 데이터베이스를 엑세스하고, 처리 결과를 바로 돌려보내는 형태 주문입력시스템 재고관리시스템 등 현업의 대부분 업무가 이 성격을 띔 OLAP(On-Line Analytical Processing) 다양한 비즈니스 관점에서 쉽고 빠르게 다차원적 데이터 접근하여 의사 결정에 활용할 수 있는 정보를 얻게 하는 기술 OLTP에서 처리된 트랜잭션 데이터로 다양한 분석 실행 OLTP가 데이터 갱신 위주라면, OLAP는 데이터 조회 위주 (비교 보기 p.69) 2000년대 기업내부 데이터베이스 CRM(Customer Relationship Management) SCM(Supply Chain Management) 분야별 내부 데이터베이스 제조: ERP(Enterprise Resource Planning), BI(Business Intelligence), CRM, RTE(Real-Time Enterprise) 금융: EAI(Enterprise Application Integration), EDW(Enterprise Data Warehouse) 유통: KMS(Knowledge Management System), RFID(RF, Radio Frequency) 사회기반구조로서의 데이터베이스 EDI(Electronic Data Interchange), VAN(Value Added Network), CALS(Commerce At Light Speed) 2장. 데이터의 가치와 미래1절. 빅데이터의 이해1. 빅데이터의 이해 관점에 따른 빅데이터의 정의 좁은 범위의 정의: 3V로 요약되는 데이터 자체의 특성 변화에 초점 중간 범위의 정의: 데이터 자체뿐 아니라 처리, 분석의 기술적 변화까지 포함 넓은 관점에서의 정의: 인재, 조직 변화까지 포함 가트너 그룹 더그 래니의 3V 양(Volume): 데이터 규모 측면 → 센싱 데이터, 비정형데이터 다양성(Variety): 데이터 유형과 소스 측면 → 정형, 비정형데이터(영상, 사진) 속도(Velocity): 대이터 수집과 처리 측면 → 원하는 데이터 추출 및 분석 속도 PLUS, 가치(Value), 시각화(Visualization), 정확성(Veracity) 빅데이터 정의의 범주 및 효과 데이터 변화: 규모, 형태, 속도 기술 변화: 데이터 처리, 저장, 분석기술 및 아키텍처, 클라우드 컴퓨팅 활용 인재, 조직 변화: Data Scientist 같은 새로운 인재 필요, 데이터 중심 조직 2. 출현 배경과 변화없던 것이 새로 등장한 것이 아니라, 기존의 것에서 변화된 것 - 3가지 출현 배경 출현 배경 내용 산업계 고객 데이터 축적 데이터에 숨은 가치를 발굴해 새로운 성장동력원 확보 학계 거대 데이터 활용, 과학 확산 거대 데이터를 다루는 학문이 많아지면서 필요한 기술 아키텍처 및 통계 도구 발전 기술 발전 관련 기술의 발달 디지털화, 저장 기술 발달, 인터넷 보급, 모바일 혁명, 클라우드 컴퓨팅 3. 빅데이터의 기능 비유 비유 대상 내용 산업혁명의 석탄, 철 제조업 ~ 서비스 분야 생산성을 끌어올려 사회 전반에 혁명적 변화를 가져올 것 21세기의 원유 경제 성장에 필요한 정보를 제공하여 산업 생산성을 한 단계 향상 시키고 기존에 없던 새 범주 산업을 만들어 낼 것 렌즈 현미경이 생물학 발전에 미친 영향만큼 데이터가 산업 발전에 영향을 미칠 것 플랫폼 공동 활용 목적으로 구축된 유무형의 구조물로써 다양한 서드파티 비즈니스에 활용되며 플랫폼 역할을 할 것 4. 빅데이터가 만드는 본질적 변화 과거에서 현재로 사전 처리 → 사후처리 표본조사 → 전수조사 질 → 양 인과관계 → 상관관계 2절. 빅데이터의 가치와 영향1. 빅데이터의 가치 빅데이터 가치 산정이 어려운 이유 데이터 활용방식: 재사용, 재조합, 다목적용 데이터 개발 새로운 가치 창출: 기존에 없던 가치를 창출하여 가치 측정이 어려움 분석 기술 발전: 현재는 가치 없는 데이터가 추후 분석 기법 등장으로 큰 가치를 지닐 수 있음 2. 빅데이터의 영향 빅데이터가 미치는 영향 기업: 혁신, 경쟁력 제고, 생산성 향상 → 소비자 행동 분석, 시장 변동 예측 정부: 환경 탐색, 상황 분석, 미래 대응 → 기상, 인구 이동, 법제 데이터 등 수집 개인: 목적에 따른 활용 → 개인 인지도 향상에도 빅데이터가 활용 3절. 비즈니스 모델1. 빅데이터 활용 사례 기업 구글: 사용자 로그 데이터를 활용한 검색엔진 개발, 기존 페이지랭크 알고리즘 혁신으로 검색 서비스 개선 월마트: 고객 구매패턴을 분석해 상품 진열에 활용 정부 실시간 교통정보 수집, 기후 정보, 지질 활동, 소방 서비스 등 국가 안전 확보를 위해 실시간 모니터링 개인 정치인: 선거 승리를 위해 사회관계망 분석 → 유세 지역 선정, 해당 지역 유권자에게 영향을 줄 수 있는 내용을 선정해 효과적인 선거 활동 가수: 팬들의 음악 청취 기록 분석을 통해 실제 공연에서 부를 노래 순서 선정 2. 빅데이터 활용 기본 테크닉 테크닉 종류와 예시 연관규칙학습: 변인들 간 주목할 상관관계 확인 : 커피를 구매하는 사람이 탄산음료를 더 많이 사는가? 유형분석: 문서 분류 또는 조직과 팀을 특성에 따라 분류할 때 : 이 사용자는 어떤 특성을 가진 집단에 속하는가? 유전자 알고리즘: 최적화 필요한 문제의 해결책을 선택, 돌연변이 같은 매커니즘으로 점진적으로 진화(evolve)시키는 법 : 최대의 시청률을 얻으려면 어떤 프로그램을 어떤 시간대에 방송해야 하는가? 기계학습: 훈련 데이터로부터 학습한 알려진 특성을 활용해 예측하는 방법 : 기존 시청 기록을 바탕으로, 시청자가 현재 보유한 영화 중 어떤 것을 가장 보고 싶어 할까? 회귀분석: 독립변수를 조작함에 따라, 종속변수가 어떻게 변하는지를 보며 두 변인 관계를 파악 : 구매자의 나이가 구매 차량의 타입에 어떤 영향을 미치는가? 감정분석: 특정 주제에 관해 말하거나 글 쓴 사람의 감정 분석 : 새로운 환불 정책에 대한 고객 평가는? 소셜네트워크(사회관계망)분석: 특정인과 다른 사람이 몇 촌 정도 관계인가를 파악, 영향력 있는 사람을 찾을 때 사용 : 고객들 간 관계망은 어떻게 구성되어 있나? 4절. 위기 요인과 통제 방안1. 빅데이터 시대의 위기 요인 사생활 침해 내용: 개인정보 포함된 데이터가 목적 외에 사용될 경우 사생활 침해 + 사회적 위협으로 변형될 수 있음 예시: 여행사실을 트위트한 사람 집을 강도가 노리는 사례 → 익명화의 기술 발전 필요 책임 원칙 훼손 내용: 분석대상이 되는 사람들이 예측 알고리즘의 희생양이 될 가능성 증가 예시: 범죄 예측 프로그램에 의해 범행을 저지르기 전 체포, 신용도와 무관하게 대출 거절 데이터 오용 내용: 빅데이터으로 한 예측은 항상 맞을 수 없음 예시: 적군 사망자 수를 전쟁 진척 상황 지표로 사용했으나, 적군 사망자 수가 과장되어 보고 2. 위기 요인에 따른 통제 방안 동의에서 책임으로 개인정보 제공자의 동의 → 개인정보 사용자의 책임 결과 기반 책임 원칙 고수 책임원칙 훼손 위기 요인에 대한 통제 방안 예측 자료에 의한 불이익을 당할 가능성을 최소화하는 장치 마련 필요 알고리즘 접근 허용 데이터 오용 위기요소에 대한 대응책 → 예측 알고리즘의 부당함을 반증할 수 있는 방법 5절. 미래의 빅데이터1. 빅데이터 활용 3요소 기본 3요소 데이터: 모든 것을 데이터화(Datafication)하는 추세로, 목적없이 축적된 데이터를 통한 창의적인 분석이 가능 기술: 대용량 데이터를 빠르게 처리하기 위한 알고리즘의 진화, 스스로 학습하고 데이터를 처리할 수 있는 인공지능 기술 출현 인력: 빅데이터 처리 위한 데이터 사이언티스트와 알고리즈미스트의 역할 → 빅데이터의 다각적 분석을 통한 인사이트 도출이 중요해짐 3장. 가치 창조를 위한 데이터 사이언스와 전략 인사이트1절. 빅데이터 분석과 전략 인사이트1. 빅데이터 열풍과 회의론빅데이터 회의론은 실제 빅데이터 분석에서 찾을 수 있는 가치를 발굴하기도 전에 사전에 활용 자체를 차단해 버릴 수 있음 2. 빅데이터 회의론의 원인 및 진단 투자효과를 못 거둔 부정적 학습효과 → 과거 CRM 빅데이터 성공사례 중, 기존 분석 프로젝트를 포함한 것이 많음 3. ’Big’이 핵심이 아님 빅데이터 분석 가치 크기 이슈가 아니라, 어떤 시각과 통찰을 얻을 수 있는지가 중요 4. 전략적 통찰이 없는 분석의 함정5. 일차원적 분석 vs 가치기반 분석 산업별 분석 애플리케이션 산업 일차원적 분석 애플리케이션 금융 서비스 신용점수 산정, 사기 탐지, 가격 책정, 프로그램트레이딩, 클레임분석, 고객수익성분석 소매업 판촉, 매대 관리, 수요 예측, 재고 보충, 가격 및 제조 최적화 제조업 공급사슬 최적화, 수요 예측, 재고 보충, 보증서 분석, 맞춤형 상품 개발, 신상품 개발 운송업 일정 관리, 노선 배정, 수익 관리 헬스케어 약품 거래, 예비 진단, 질병 관리 병원 가격 책정, 고객 로열티, 수익 관리 에너지 트레이딩, 공급/수요 예측 커뮤니케이션 가격 계획 최적화, 고객 보유, 수요 예측, 생산능력 계획, 네트워크 최적화, 고객 수익성 관리 서비스 콜센터 직원 관리, 서비스-수익 사슬 관리 정부 사기 탐지, 사례 관리, 범죄 방지, 수익 최적화 온라인 웹 매트릭스, 사이트 설계 고객 추천 모든사업 성과관리 일차원적 분석의 문제점은 환경변화와 같은 큰 변화에 대응하기 어렵고, 새로운 기회를 포착하기 어렵다는 것 전략도출 가치기반 분석 해당 사업에 중요한 기회 발굴, 주요 경영진의 지원 얻기 가능 분석의 활용 범위를 더 넓고 전략적으로 변화시키는 것 필요 차별화를 위한 전략적 인사이트를 주는 가치기반 분석단계로 나아가야 함 2절. 전략 인사이트 도출에 필요한 역량1. 데이터 사이언스 데이터사이언스는 데이터로부터 의미 있는 정보를 추출해내는 학문 비즈니스 성과를 좌우하는 핵심 이슈에 답하고, 사업의 성과를 견인할 수 있어야 함 2. 데이터 사이언스의 구성요소 데이터 사이언스의 영역 Analytics: 수학, 확률모델, 머신러닝, 분석학, 패턴 인식과 학습, 불확실성 모델링 IT: 시그널 프로세싱, 프로그래밍, 데이터 엔지니어링, 데이터 웨어하우스, 고성능 컴퓨팅 비즈니스 분석: 커뮤니케이션, 프레젠테이션, 스토리텔링, 시각화 3. 데이터 사이언티스트 요구 역량 Hard Skill 빅데이터에 관한 이론적 지식 분석 기술의 숙련 Soft skill 통찰력 있는 분석 설득력 있는 전달 다분야간 협력 4. 데이터 사이언스: 과학과 인문의 교차로스토리텔링, 커뮤니케이션, 창의력, 열정, 직관력, 비판적 시각, 대화능력 등의 인문학적 요소가 필요 5. 전략적 통찰력과 인문학의 외부 환경 측면에서 본 인문학 열풍 이유 외부환경의 변화 내용 예시 컨버전스 → 디버전스 단순세계화에서 복잡한 세계화로 변화 규모의 경제, 세계화, 표준화, 이성화 → 복잡한 세계, 다양성, 관계, 연결성, 창조성 생산 → 서비스 비즈니스 중심이 제품 생산에서 서비스로 이동 고장나지 않는 제품 → 뛰어난 서비스 생산 → 시장 창조 공급자 중심 기술 경쟁에서 무형자산 경쟁으로 변화 생산 기술 중심, 기술 중심 투자 → 패러다임에 근거한 시장 창조, 현지 사회와 문화에 관한 지식 3절. 빅데이터, 데이터 사이언스의 미래1. 가치 패러다임 변화 과거: Digitalization, 아날로그 세상을 디지털화하는지가 가치 창출 원천 현재: Connection, 디지털화된 정보가 연결되기 시작하면서 효과적인 연결을 찾는 것이 성공 요인 미래: Agency, 복잡한 연결을 효과적이고 믿을 수 있게 관리하는 것이 중요 2. 데이터 사이언스의 한계 한계 분석과정에서는 가정 등 인간의 해석이 개입되는 단계가 반드시 존재 분석결과를 해석하는 사람에 따라 다른 결과가 도출 정량적인 분석이라고 할지라도 결국 가정에 근거 추가. 최신 빅데이터 상식1. DMBS와 SQL DMBS(Data Base Management System) 데이터베이스를 관리하여 응용프로그램들이 데이터베이스를 공유하며 사용하는 환경을 제공하는 소프트웨어 데이터베이스를 구축하는 틀, 데이터 검색, 저장 기능 등 제공 대표 시스템: 오라클, 인포믹스, 액세스 데이터베이스 관리 시스템의 종류 관계형 DBMS 데이터를 column과 row를 이루는 하나 이상의 테이블/관계로 정리 Primary key가 각 row를 식별 row는 레코드나 튜플로 불림 일반적으로 각 테이블/관계는 하나의 엔티티 타입(고객이나 제품과 같은)을 대표 객체지향 DMBS 일반적으로 사용되는 테이블 기반의 관계형DB와 다르게 정보를 ‘객체’ 형태로 표현하는 데이터베이스 모델 네트워크 DMBS 레코드들이 노드로, 레코드들 사이 관계가 간선으로 표현되는 그래프를 기반으로 하는 데이터베이스 모델 계층형 DMBS 트리 구조를 기반으로 하는 계층 데이터베이스 모델 SQL(Structured Query Language) 데이터베이스에 접근할 수 있는 데이터베이스의 하부 언어 단순한 질의 기능뿐 아니라 데이터의 완전한 정의와 조작 기능을 갖춤 테이블 단위로 연산 수행, 영어 문장과 비슷한 구문으로 사용하기 쉬움 2. Data 관련 기술 개인정보 비식별 기술 데이터 셋에서 개인을 식별할 수 있는 요소를 전부/일부 삭제하거나 다른 값으로 대체하는 기술 비식별 기술 내용 예시 데이터 마스킹 데이터 길이, 유형, 형식 유지한채, 새로운 데이터를 익명으로 생상 홍길동, 35세, 서울 거주, 한국대 재학 → 홍–, 35세, 서울 거주, –대학 재학 가명 처리 개인정보 주체 이름을 변경하는 기술, 변경 규칙이 노출되지 않아야 함 홍길동, 35세, 서울 거주, 한국대 재학 → 임꺽정, 30대, 서울 거주, 국내대 재학 총계처리 데이터의 총합값을 보임 임꺽정 180cm, 홍길동 170cm → 물리학과 학생 키 합: 350cm, 평균키 175cm 데이터값 삭제 필요 없는 값 또는 개인식별에 중요한 값을 삭제, 날짜 정보는 연단위 처리 홍길동, 35세, 서울 거주, 한국대 졸업 → 35세, 서울 거주, 주민번호 901206-1234567 → 90년대 생, 남자 데이터 범주화 데이터 값을 범주 값으로 변환하여 값 숨김 홍길동, 35세 → 홍씨, 30~40세 무결성과 레이크 데이터 무결성(Data integrity) 데이터 변경/수정 시 제한을 두어 데이터의 정확성을 보증 유형: 개체 무결성(Entity integrity), 참조 무결성(Referential integrity), 범위 무결성(Domain integrity) 데이터 레이크(Data Lake) 수많은 정보 속에서 의미 있는 내용을 찾기 위해 방식 상관 없이 데이터를 저장하는 시스템 대용량의 정형 및 비정형 데이터 저장, 접근이 쉬운 대규모의 저장소 주요 플랫폼: Apache Hadoop, Teredata Integrated Big Data Platform 1700 3. 빅데이터 분석 기술 Hadoop 여러 컴퓨터를 하나인 것처럼 묶어 대용량 데이터를 처리하는 기술 분산파일 시스템(HDFS)을 통해 대용량 파일을 저장할 수 있는 기능 제공 하둡 에코시스템으로 하둡의 부족한 기능 보완 Apache Spark 실시간 분산형 컴퓨팅 플랫폼 스칼라로 작성되었으나, 스칼라, 자바, R, 파이썬, API 지원 In-Memory 방식으로 하둡에 비해 처리속도가 빠름 Smart Factory 공장 내 설비와 기계에 사물인터넷이 설치 → 공정 데이터가 실시간으로 수집, 데이터에 기반한 의사결정 Machine Learning &amp; Deep Learning 머신러닝: 인공지능 연구 분야 중 하나, 인간의 학습 능력과 같은 기능을 컴퓨터에서 실현 딥러닝: 컴퓨터가 데이터를 이용해 스스로 합습하도록 인공신경망(Artificial Neural Natwork) 등 기술로 구축한 기계 학습 기술 4. 기타 데이터의 유형 &lt;table&gt; &lt;colgroup&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;/colgroup&gt; &lt;thead&gt; &lt;tr class=&quot;header&quot;&gt; &lt;th&gt;유형&lt;/th&gt; &lt;th&gt;내용&lt;/th&gt; &lt;th&gt;예시&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr class=&quot;odd&quot;&gt; &lt;td&gt;정형데이터&lt;/td&gt; &lt;td&gt;형태(고정된 필드) 존재, 연산 가능, 주로 관계형 데이터베이스에 저장, 데이터 수집 난이도 낮고 형식이 정해져 처리 쉬움&lt;/td&gt; &lt;td&gt;관계형 데이터베이스, 스프레드시트, CSV&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;even&quot;&gt; &lt;td&gt;반정형데이터&lt;/td&gt; &lt;td&gt;형태(스키마, 메타데이터) 존재, 연산 불가능, 주로 파일로 저장, 보통 API 형태로 제공되어 데이터 처리 기술(파싱) 필요&lt;/td&gt; &lt;td&gt;XML. HTML, JSON, 로그형태(웹로그, 센서데이터)&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;odd&quot;&gt; &lt;td&gt;비정형데이터&lt;/td&gt; &lt;td&gt;형태 없음, 연산 불가능, 주로 NoSQL에 저장, 데이터 수집 난이도 높음, 텍스트 마이닝 혹은 파일일 경우 데이터 형태로 파싱이 필요해 수집 데이터 처리가 어려움&lt;/td&gt; &lt;td&gt;소셜데이터(트위터, 페이스북), (영상, 이미지, 음성, 텍스트)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; - 참고: XML은 Extensible Markup Language의 약자로 다목적 마크업 언어를 이용, 인터넷에 연결된 시스템끼리 데이터를 쉽게 주고 받을 수 있게 함(HTML 한계를 극복할 목적으로 만들어짐)","link":"/2020/11/19/Part01_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%9D%B4%ED%95%B4/"},{"title":"Part02. 데이터 분석 기획","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터 분석 기획의 이해1절. 분석기획 방향성 도출1. 분석기획의 특징 분석기획이란? 분석을 수행할 과제를 정의하고, 의도했던 결과를 도출하도록 관리할 방안을 사전에 계획하는 작업 데이터 사이언티스트의 역량 Math &amp; Statistics Information Technology Domain Knowledge 2. 분석 대상과 방법 분석 대상(What), 분석 방법(How) 따라 4가지로 나뉘어짐 Optimization(최적화): 분석 대상 &amp; 분석 방법 모두 앎 Solution(솔루션): 분석 대상 앎 &amp; 분석 방법 모름 Insight(통찰): 분석 대상 모름 &amp; 분석 방법 앎 Discovery(발견): 분석 대상 모름 &amp; 분석 방법 모름 3. 목표 시점별 분석 기획 방안 과제 중심적 접근 방식: 당면 과제를 빠르게 해결하는 방식 장기적인 마스터 플랜 방식: 지속적인 분석 내재화를 위한 방식 분석 기획에서는 문제해결을 위한 단기적 접근방식과 분석과제 정의를 위한 중장기적인 마스터 플랜 접근방식을 융합하는 것이 중요 목표 시점별 분석 기획 방안 당면한 분석 주제 해결(과제 단위) 지속적 분석 문화 내재화(마스터 플랜 단위) Speed &amp; Test 1차 목표 Accuracy &amp; Deploy Quick &amp; Win 과제 유형 Long Term View Problem Solving 접근 방식 Problem Definition 의미 있는 분석을 위해서는 분석 기술, IT 및 프로그래밍, 분석 주제에 관한 도메인 전문성, 의사 소통이 중요 분석대상 및 방식에 따른 다양한 분석 주제를 과제 단위 또는 마스터 플랜 단위로 도출할 수 있어야 함 4. 분석 기획 시 고려사항 가용 데이터(Available Data) 분석을 위한 데이터 확보가 우선적이며, 유형 분석이 선행되어야 함 데이터 유형에 따라 적용 가능한 솔루션 및 분석 방법이 다르기 때문 적절한 활용 방안과 유즈 케이스(Proper Business Use Case) 분석을 통해 가치가 창출될 수 있음 ‘바퀴를 재발명하지 마라’ 기존에 구현된 유사 분석 시나리오와 솔루션을 최대한 활용 장애요소에 대한 사전계획 수립(Low Barrier Of Execution 일회성 분석에 그치지 않고 조직의 역량으로 내재화 필요 충분하고 계속적인 교육 및 활용방안 등의 변화 관리가 고려되어야함 2절. 분석 방법론1. 분석 방법론 개요 데이터 분석이 기업 내 효과적으로 정착하기 위해서는 절차와 방법이 정리된 데이터 분석 방법론 수립 필요 프로젝트는 일정한 수준의 품질을 갖춘 산출물과 성공 가능성을 확보할 수 있어야 함 (개인의 역량이나 조직의 우연한 성공에 기인하면 안 됨) 상체한 절차(Procedures), 방법(Methods), 도구와 기법(Tools&amp;Techniques), 템플릿과 산출물(Templates&amp;Options)로 구성되어 어느 정도 지식으로 활용할 수 있어야 함 데이터 기반 의사결정의 필요성 경험과 감에 따른 의사결정 → 데이터 기반 의사결정 기업의 합리적 의사결정을 막는 장애요소 고정관념(Stereotype), 편향된 생각(Bias), 프레이밍 효과(Framing Effect, 문제 표현 방식에 따라 동일한 사건임에도 판단이나 선택이 달라지는 현상) 방법론의 생성 과정 암묵지 → (형식화) → 형식지 → (체계화) → 방법론 → (내재화) → 암묵지 방법론 적용 업무 특성에 따른 모델 폭포수 모델(Waterfall Model) 단계를 순차적으로 진행하는 방법, 이전 단계가 완료되어야 다음 단계 진행 가능 문제 발결 시 피드백 과정이 수행됨 기존 IT의 SW 개발 방식 프로토타임 모델(Prototype Model) 폭포수 모델 단점을 보환하기 위해 점진적으로 시스템을 개발해가는 접근 방식 고객 요구를 완전히 이해하지 못하거나 완벽한 요구 분석의 어려움을 해결하기 위한 방법 일부분을 우선 개발하여 사용자에게 제공 시험 사용 후, 사용자 요구를 분석하여 요구 정당성을 점검하고 개선 작업을 진행 나선형 모델(Spiral Model) 반복을 통해 점증적으로 개발하는 방법 처음 시도하는 프로젝트에는 용이하나, 관리 체계를 갖추지 못하면 복잡도 상승 방법론의 구성 단계 최상위 계층으로서 프로세스 그룹을 통해 완성된 단계별 산출물 생성, 각 단계는 기준선으로 설정되어 관리되어야 함, 버전관리 등으로 통제 → 단계별 완료 보고서 태스크 단계를 구성하는 단위 활동으로써 물리적 또는 논리적 단위로 품질검토의 항목이 됨 → 보고서 스탭 WBS(Work Breakdown Structure)의 워크 패키지에 해당, 입력자료/처리 및 도구/출력자료로 구성된 단위 프로세스 → 보고서 구성요소 2. KDD 분석 방법론 KDD(Knowledge Discovery in Databases) 프로파일링 기술 기반으로 데이터에서 통계적 패턴이나 지식을 찾는 데 활용할 수 있도록 정리한 데이터 마이닝 프로세스 데이터 마이닝, 기계학습, 인공지능, 패턴인식, 데이터 시각화 등에서 응용될 수 있는 구조를 가짐 KDD 분석 절차 [Data] → 1. Selection → [Target Data] → 2. Preprocessing → [Preprocessed Data] → 3. Transformation → [Transformed Data] → 4.Data Mining → [Patterns] → 5. Interpretation / Eveluation → [Knowledge] 데이터셋 선택(Selection) 데이터셋 선택에 앞서 분석 대상 비즈니스 도메인에 대한 이해와 프로젝트목표 설정이 필수 - 데이터베이스 또는 원시 데이터에서 분석에 필요한데이터를 선택하는 단계 - 데이터마이닝에 필요한 목표데이터(target data)를구성하여 분석에 활용 데이터 전처리(Preprocessing) 데이터셋을 정제하는 단계 잡음(Noise), 이상치(Outlier), 결측치(Missing Value)를 식별하고 제거하거나 의미 있는 데이터로 재처리 전처리 단계에서 추가로 요구되는 데이터셋이 필요한 경우, 데이터 선택 프로세스를 재실행 데이터 변환(Transformation) 데이터 전처리 과정을 통해 정제된 데이터에 분석 목적에 맞는 변수 생성, 선택 데이터 차원을 축소하여 효율적으로 데이터 마이닝 하도록 변경하는 단계 학습용 데이터(training data)와 검증용 데이터(test data)로 데이터를 분리하는 단계 데이터 마이닝(Data Mining) 학습용 데이터를 이용하여 분석 목적에 맞는 데이터 마이닝 기법을 선택하고 적절한 알고리즘으로 데이터 마이닝 작업을 실행 필요에 따라 데이터 전처리와 데이터 변환 프로세스를 추가로 실행 데이터 마이닝 결과 평가(Interpretation / Evaluation) 데이터 마이닝 결과에 대한 해석과 평가, 분석 목적과의 일치성 확인 데이터 마이닝을 통해 발견한 지식을 업무에 활용하기 위한 방안 마련 단계 필요에 따라 데이터 선택 프로세스에서 데이터 마이닝 프로세스를 반복 수행 3. CRISP-DM 분석 방법론 CRISP-DM(Cross Industy Standard Process for Data Mining) 5개 업체가 주도: Daimler-Chrysler, SPSS, NCR, Teradata, OHRA 계층적 프로세스 모델로써 4개 레벨로 구성됨 CRISP-DM의 4레벨 구조 Phases(단계) → Generic Tasks(일반화 태스크) → Specialized Task(세분화 태스크) → Process Instances(프로세스 실행) CRISP-DM의 6단계 프로세스 각 단계는 단방향으로 구성되지 않고, 단계 간 피드백을 통해 단계별 완성도를 높이게 되어 있음 단계 내용 수행업무 1. 업무 이해 프로젝트 목적과 요구사항을 이해, 도메인 지식을 데이터 분석을 위한 문제 정의로 변경하고 초기 프로젝트 계획을 수립 업무 목적 파악, 상황 파악, 데이터 마이닝 목표 설정, 프로젝트 계획 수립 2. 데이터 이해 데이터 수집하고 데이터 속성 이해, 데이터 품질 문제점 식별, 숨겨진 인사이트 발견 초기 데이터 수집, 데이터 기술 분석, 데이터 탐색, 데이터 품질 확인 3. 데이터 준비 분석을 위해 수진된 데이터에서 분석기법에 적합한 데이터 편성 초기 데이터 수집, 데이터 기술 분석, 데이터 탐색, 데이터 품질 확인 4. 모델링 다양한 모델링 기법과 알고리즘 선택, 파라미터 최적화, 데이터셋이 추가로 필요한 경우 준비 단계 반복 수행, 테스트용 데이터셋을 평가해 모델의 과적합 문제 확인 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가 5. 평가 모델링 결과가 프로젝트 목적에 부합하는지 평가, 데이터 마이닝 결과를 최종적으로 수용할 것인지 판단 분석 결과 평가, 모델링 과정 평가, 모델 적용성 평가 6. 전개 모델을 실 업무에 적용하기 위한 계획 수립, 유지보수 계획 마련(생명주기 고려 필요) 전개 계획 수립, 모니터링/유지보수 계획 수립, 프로젝트 종료 보고서 작성, 프로젝트 리뷰 4. KDD vs CRISP-DM KDD CRISP-DM 분석대상 비즈니스 이해 업무 이해 데이터셋 선택 / 데이터 전처리 데이터의 이해 데이터 변환 데이터 준비 데이터 마이닝 모델링 데이터 마이닝 결과 평가 평가 데이터 마이닝 활용 전개 5. 빅데이터 분석 방법론 빅데이터 분석의 계층적 프로세스 단계(Phase): 프로세스 그룹을 통해 완성된 단계별 산출물 생성 태스크(Task): 각 단계는 여러 개의 태스크로 구성, 단계를 구성하는 단위 활동이며 물리/논리적 단위로 품질 검토 항목이 될 수 있음 스텝(Step): WBS의 워크 패키지에 해당되고 입력자료, 처리 및 도구, 출력자료로 구성된 단위 프로세스 빅데이터 분석 방법론 5단계 분석 기획(Planning): 비즈니스 도메인과 문제점 인식, 분석 계획 및 프로젝트 수행계획을 수립하는 단계 데이터 준비(Preparing): 비즈니스 요구사항과 데이터 분석데 필요한 원천 데이터를 정의하고 준비하는 단계 데이터 분석(Analyzing): 원천 데이터를 분석용 데이터셋으로 편성하고 분석 기법과 알고리즘으로 데이터를 분석하는 단계, 추가 데이터가 필요할 경우 준비 단계로 피드백하여 두 단계 반복 진행 시스템 구현(Developing): 분석 기획에 맞는 모델 도출, 운영 중인 가동 시스템에 적용하거나 시스템 개발을 위한 사전 검증 평가 및 전개(Lesson Learned): 프로젝트 성과를 평가하고 정리, 모델 발전 계획을 수립하여 차기 분석 기획으로 전달 3절. 분석 과제 발굴1. 분석 과제 발굴 방법론 개요 과제 정의서 형태로 도출 하향식 접근 방법과 상향식 접근 방법이 있음 최적의 의사 결정은 두 접근 방식이 상호 보완일 때 가능 디자인 사고: 상향식 접근의 발산 단계, 하향식 접근의 수렴 단계를 반복적으로 수행하여 분석 가치를 높임 하향식 접근법(Top Down Approach) 현황 분석을 통해 기회나 문제 탐색 → 문제 정의 → 해결방안 탐색 데이터 분석의 타당성 평가를 거쳐 분석 과제를 도출하는 과정으로 구성 1단계. 문제 탐색 문제를 해결함으로 나타나는 가치에 중점 비즈니스 모델 기반 문제 탐색: 비즈니스 모델 캔버스 업무(Operation), 제품(Product), 고객(Customer), 규제와 감사(Regulation &amp; Audit), 지원 인프라(IT &amp; Human Resource) 분석 기회 발굴 범위 확장 거시적 관점: 사회, 기술, 경제, 환경, 정치 경쟁자 확대: 대체제, 경쟁자, 신규 진입자 시장 니즈 탐색: 고객, 채널, 영향자들 역량의 재해석: 내부역량, 파트너 네트워크 외부 참조 모델 기반 문제탐색 Quick &amp; Easy 방식으로 빠르게 도출 데이터 분석을 통한 인사이트 도출 지속적 조사와 데이터 분석을 통한 가치 발굴 사례를 정리하여 풀(Pool)로 만들면 좋음 분석 유즈 케이스(Analytics Use Case) 빠짐 없이 도출한 분석 기회를 구체적인 과제로 만들기 전에 분석 유즈 케이스로 표기하는 것이 필요 2단계. 문제 정의(Problem Definition) 식별된 비즈니스 문제를 데이터 문제로 변환하여 정의하는 단계 (How ?) 데이터 분석 문제 정의 및 요구사항: 분석 수행 당사자뿐 아니라 최종 사용자(End User) 관점에서 이루어져야 함 데이터 정의 및 기법 발굴을 용이하게 하기 위해 정확히 분석의 관점에서 문제를 재정의할 필요가 있음 3단계. 해결 방안 탐색(Solution Search) 정의된 데이터 분석 문제 해결을 위해 다양한 방안 모색 기존 정보시스템의 단순한 보완으로 분석 가능한지 고려 엑셀 등 간단한 도구로 분석 가능한지 고려 하둡 등 분산병렬처리를 활용한 빅데이터 분석 도구로 보다 체계적이고 심도 있는 방안 고려 분석 역량이 없을 경우, 교육이나 전문인력 채용으로 역량을 확보하거나 전문 업체를 활용 4단계. 타당성 검토(Feasibility Study) 경제적 타당성: 비용 대비 편익 분석 관점으로 접근 데이터 및 기술적 타당성 상향식 접근법(Bottom up Approach) 다양한 원천 데이터를 대상으로 분석하여 가치 있는 모든 문제를 도출하는 과정 하향식 접근법의 한계를 극복하기 위한 분석 방법론 단계별 접근법은 문제 구조가 분명하고, 문제 해결이 데이터 분석가 및 의사결정자에게 주어져 있음을 가정 → 솔루션 도출에는 유리하나 새로운 문제 탐색에는 한계 디자인 사고 접근법을 통해 전통적인 분석적 사고 한계를 극복해야 함 Why가 아닌 사물을 그대로 인식하는 What 관점으로 보아야 함 데이터 그 자체를 관찰하고 행동하여 대상을 잘 이해하는 방식의 접근법 Empathize → Define → Ideate → Prototype → Test 비지도 학습과 지도 학습 비지도 학습(Unsupervised Learning) 일반적으로 상향식 접근방식의 데이터 분석은 비지도 학습 방법으로 수행 데이터 자체의 결합, 연관성, 유사성을 중심으로 데이터 상태를 표현하는 것 예) 장바구니 분석, 군집 분석, 기술 통계 및 프로파일링 지도 학습(Supervised Learning) 명확한 목적 하에 데이터 분석을 실시하는 것 분류, 추측, 예측, 최적화를 통해 사용자 주도 하 분석을 실시하고 지식을 도출하려는 목적 상관관계 분석, 연관 분석을 통해 다양한 문제를 해결 시행 착오를 통한 문제 해결 프로토타이핑 접근법 요구사항이나 데이터 규정이 어렵고, 데이터 소스를 명확히 파악하기 어려운 상황에서 일단 분석하고 결과를 보면서 반복적으로 개선해 나가는 방식 완벽하지는 못하지만, 신속하게 해결책이나 모형을 제시함으로써 이를 바탕으로 문제를 명확히 인식하고 필요한 데이터를 식별하여 구체화하게 하는 상향식 접근 방식 가설 생성 → 디자인 실험 → 실제 환경 테스트 → 인사이트 도출 및 가설 확인 빅데이터 분석 환경에서 프로토타이핑의 필요성 문제 인식 수준: 문제 정의가 불명확하거나 새로운 문제일 경우 문제 이해와 구체화에 도움 필요 데이터 존재 여부의 불확실성: 데이터를 어떻게 찾을 것인지 사용자와 분석가 간 반복적인 협의 과정 필요 데이터 사용 목적 가변성: 기존 데이터 정의를 재검토하여 데이터 사용 목적과 범위 확대 가능 분석과제 정의: 분석별 필요한 소스 데이터, 분석 방법, 데이터 입수, 분석 난이도, 분석 수행주기, 분석 결과 검증 오너십, 상세 분석 과정 정의 분석 데이터 소스: 내/외부 비구조적인 데이터와 소셜 미디어 및 오픈 데이터까지 범위 확장하여 고려하고 분석 방법 또한 상세하게 정의 4절. 분석 프로젝트 관리 방안1. 분석과제 관리를 위한 5가지 주요 영역 Data Size 분석하고자 하는 데이터 양 고려 Data Complexity 초기 데이터 확보와 통합뿐 아니라 해당 데이터에 적용될 수 있는 분석 모델 선정 등의 사전 고려 필요 Speed 시나리오 측면에서의 속도 고려 필요 프로젝트 수행 시 분석 모델의 성능 및 속도를 고려한 개발/테스트 Analytic Complexity 분선 모델의 정확도와 복잡도는 트레이드 오프 관계 분석 모델이 복잡할수록 정확도는 올라가지만 해석이 어려워짐 해석이 가능하면서도 정확도를 올릴 수 있는 최적모델을 찾아야 함 Accuracy &amp; Precision 정확도: 모델과 실제 값 사이 차이가 적음을 의미 일관성: 모델을 반복했을 때의 편차의 수준 활용 측면에서는 정확도가, 안정성 측면에서는 일관성이 중요 2장. 분석 마스터 플랜1절. 마스터 플랜 수립 프레임 워크1. 분석 마스터 플랜 수립 프레임 워크 마스터 플랜 수립 개요 우선 순위 고려 요소: 전략적 중요도, 비즈니스 성과/ROI, 실행 용이성 적용 범위/방식 고려 요소: 업무 내재화 적용 수준, 분석 데이터 적용 수준, 기술 적용 수준 수행 과제 도출 및 우선순위 평가 일반적인 IT 프로젝트의 우선순위 평가 예시 전략적 중요도: 전략적 필요성, 시급성 실행 용이성: 투자 용이성, 기술 용이성 ROI 관점의 빅데이터 핵심 특징 3V(난이도): 크기(Volume), 다양성(variety), 속도(Velocity) → 투자비용 요소 4V(시급성): 3V + 가치(Value) → 비즈니스 효과 2절. 분석 거버넌스 체계 수립1. 거버넌스 체계 구성 요소 분석 기획 및 관리를 수행하는 조직(Organization) 과제 기획 및 운영 프로세스(Process) 분석 관련 시스템(System) 데이터(Data) 분석 관련 교육 및 마인드 육성 체계(Human Resource) 2. 데이터 분석 수준 진단 분석 준비도(Readiness): 분석 업무, 분석 인력 및 조직, 분석 기법, 분석 데이터, 분석 문화, 분석 인프라 분석 성숙도(Maturity): 도입 &gt; 활용 &gt; 확산 &gt; 최적화(비즈니스, 조직 및 역량, IT) 분석 준비도 진단 과정 내용 분석업무 파악 발생 사실 분석, 예측 분석, 시뮬레이션 분석, 최적화 분석, 분석 업무 정기적 개선 인력 및 조직 분석 전문가 직무, 분석 전문가 교육 훈련, 관리자의 기본적 분석 능력, 전사 분석업무 총괄 조직, 경영진의 분석 업무 이해 분석 기법 업무별 적합한 분석 기법, 분석 업무 도입 방법론, 분석 기법 라이브러리, 분석 기법 효과성 평가, 분석 기법 정기적 개선 분석 데이터 데이터 충분성 / 신뢰성 / 적시성, 비구조적 데이터 관리, 외부 데이터 활용 체계, 기준 데이터 관리(MDM) IT 인프라 운영 시스템 데이터 통합, EAI/ETL 등 데이터 유통 체계, 분석 전용 서버 및 스토리지, 빅데이터 분석 환경, 통계 분석 환경, 비쥬얼 분석 환경 분석 성숙도 모델 조직의 성숙도 평가 도구: CMMI(Capability Maturity Model Integration) 성숙도 수준 분류: 도입 단계 → 활용 단계 → 확산 단계 → 최적화 단계 분석 성숙도 진단 분류: 비즈니스 부문, 조직/역량 부문, IT 부문 분석 관점에서의 사분면 분석 정착형(준비도 낮음, 성숙도 높음) 확산형(준비도 높음, 성숙도 높음) 준비형(준비도 낮음, 성숙도 낮음) 도입형(준비도 높음, 성숙도 낮음) 3. 분석 지원 인프라 방안 수립 분석 과제 단위별로 별도 분석 시스템을 구축하면, 관리 복잡 &amp; 비용 증대 문제가 발생 분석 마스터 플랜 기획 단계에서부터 확장성을 고려한 플랫폼 구조 도입이 필요 플랫폼 구조: 공동 기능, 중앙 집중적 데이터 관리, 시스템 간 인터페이스 최소화 플랫폼 단순한 분석 응용프로그램뿐 아니라, 분석 서비스를 위한 응용프로그램이 실행될 수 있는 기초를 이루는 컴퓨터 시스템 일반적으로 하드웨어에 탑재되어 데이터 분석에 필요한 프로그래밍 환경과 실행 및 서비스 환경을 제공 분석 플랫폼이 구성된 경우, 개별적인 분석 시스템 추가 대신 서비스를 추가하는 방식으로 확장성을 높일 수 있음 4. 데이터 거버넌스 체계 수립 데이터 거버넌스 전사 차원 데이터에 대해 관리 체계 수립, 프레임워크 및 저장소 구축 마스터 데이터(Master Data), 메타 데이터(Meta Data), 데이터 사전(Data Dictionary) 데이터 거버넌스 체계를 구축함으로써 데이터의 가용성, 유용성, 통합성, 보안성, 안정성 확보 가능 독자적 수행도 가능하나, 전사 차원의 IT 거버넌스나 EA(Enterprise Architecture)의 구성요소로 구축되는 경우도 있음 빅데이터의 효율적인 관리, 다양한 데이터 관리 체계, 데이터 최적화, 정보 보호, 데이터 생명주기 관리, 데이터 카테고리별 관리 책임자 지정 등 포함 데이터 거버넌스 구성 3요소 원칙(Principle) 데이터를 유지/관리하기 위한 지침과 가이드 보안, 품질 기준, 변경 관리 조직(Organization) 데이터 관리 조직의 역할과 책임 데이터 관리자, 데이터베이스 관리자, 데이터 아키텍트 프로세스(Process) 데이터 관리 위한 활동과 체계 작업 절차, 모니터링 활동, 측정 활동 데이터 거버넌스 체계 데이터 표준화 업무: 데이터 표준 용어 설정, 명명 규칙 수립, 메타 데이터 구축, 데이터 사전 구축 데이터 표준 용어는 표준 단어사전, 표준 도메인사전, 표준 코드 등으로 구성 (점검 프로세스 포함 필요) 명명 규칙은 필요 시 언어별로 작성되어 매핑 상태를 유지해야 함 데이터 관리 체계 표준 데이터를 포함한 메타 데이터와 데이터 사전 관리 원칙 수립 → 데이터 정합성 및 활용 효율성을 위해 수립된 원칙에 근거해 상세 프로세스를 만들고 담당자와 조직을 상세히 준비 데이터 생명주기 관리 방안(Data Life Cycle Management) 수립 필요 데이터 저장소 관리(Repository) 메타 데이터 및 표준 데이터 관리를 위한 전사 차원 저장소 워크플로우 및 관리용 응용 소프트웨어를 지원하고 관리 대상 시스템과의 인터페이스를 통제가 이뤄져야 함 데이터 구조 변경에 따른 사전 영향 평가 수행 필요 → 효율적 활용을 위해 표준화 활동 데이터 거버넌스 체계 구축 후, 표준 준수 여부를 주기적으로 점검하고 모니터링 실시 5. 데이터 조직 및 인력방안 수립 분석을 위한 3가지 조직 구조 집중 구조 전사 분석업무를 별도 분석 전담 조직에서 담당 전략적 중요도에 따라 분석조직이 우선순위 정하여 진행 현업 업무부서의 분석업무와 이중화/이원화 가능성이 높음 기능 구조 일반적인 분석 수행 구조 별도 분석조직이 없고 해당 업무 부서에서 분석 수행 전사적 핵심 분석이 어려우며, 과거 실적에 국한된 분석이 수행될 가능성이 높음 분산 구조 분석조직 인력을 현업부서로 직접 배치하여 분석 업무 수행 전사 차원 우선순위 수행 분석결과에 따른 신속한 Action 가능 베스트 프랙티스 공유 가능 부서 분석업무와 역할 분담을 명확히 해야함 → 업무 과다 이원화 가능성 분석 조직 인력 구성 분석 조직(DSCoE: Data Science Center of Excellence) 비즈니스 인력, IT 기술 인력, 분석 전문 인력, 변화 관리 인력, 교육 담당 인력 6. 분석 과제 관리 프로세스 수립 과제 관리 프로세스 과제 발굴: 분석 idea 발굴 → 분석 과제 후보 제안 → 분석 과제 확정 과제 수행: → 탐구성 → 분석 과제 실행 → 분석 과제 진행 관리 → 결과 공유 및 개선","link":"/2020/11/20/Part02_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_%EA%B8%B0%ED%9A%8D/"},{"title":"Part03. 데이터 분석_1","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터 분석 개요1절. 데이터 분석 기법의 이해1. 데이터 처리 데이터 분석 통계 기반이나, 통계지식과 복잡한 가정이 상대적으로 적은 실용적인 분야 활용 대기업은 데이터 웨어하우스(DW)와 데이터마트(DM)를 통해 분석 데이터를 가져와 사용 신규 시스템이나 DW에 포함되지 못한 자료는, 기존 운영 시스템(Legacy)나 스테이징 영역(staging area)과 ODS(Operational Data Store)에서 데이터를 가져와 DW에서 가져온 내용과 결합하여 활용 가능 단, 운영시스템에 직접 접근해 데이터를 활용하는 것은 매우 위험 → 스테이징 영역 데이터는 운영시스템에서 임시로 저장된 데이터기에 가급적 클린징 영역인 ODS에서 데이터 전처리를 하여 DW나 DM과 결합해 사용 최종 데이터 구조로 가공 데이터 마이닝 분류 분류값과 입력변수를 연관시켜 인구통계, 요약변수, 파생벽수 등 산출 정형화된 패턴 처리 비정형 데이터나 소셜 데이터는 정형화된 패턴으로 처리해야 함 비정형 데이터 DMBS에 저장됐다가 텍스트 마이닝을 거쳐 데이터 마트와 통합 관계형 데이터 DBMS에 저장되어 사회 신경망 분석을 거쳐 분석 결과 통계값이 마트와 통합되어 활용 2. 시각화(시각화 그래프) 가장 낮은 수준의 분석이지만, 제대로 사용하면 복잡한 분석보다도 효율적 대용량 데이터를 다루는 빅데이터 분석에서 시각화는 필수 탐색적 분석을 할 때, 시각화는 필수 SNA 분석(사회연결망 분석)을 할 때, 자주 활용 3. 공간분석(GIS) 공간분석(Spatial Analysis): 공간적 차원과 관련된 속성을 시각화하는 분석 지도 위에 관련 속성을 생성하고 크기, 모양, 선, 굵기 등으로 구분하여 인사이트를 얻음 4. 탐색적 자료 분석(EDA) 탐색적 분석 다양한 차원과 값을 조합하며 특이점이나 의미 있는 사실을 도출하여 분석의 최종 목적을 달성하는 과정 데이터의 특징과 내재하는 구조적 관계를 알아내기 위한 기법의 통칭 EDA의 4가지 주제 저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성 탐색적 분석 효율 예시-데이터 이해 단계, 변수 생성 단계, 변수 선택 단계에서 활용 5. 통계분석 통계 어떤 현상을 종합적으로 알아보기 쉽게 일정한 체계에 따라 숫자, 표, 그림 형태로 나타낸 것 기술통계(descriptive statistics) 모집단으로부터 표본을 추출하고 표본이 가진 정보를 쉽게 파악하도록 데이터를 정리하거나 요약하기 위해 하나의 숫자 또는 그래프 형태로 표현하는 절차 추측(추론)통계(inferential statistics) 모집단으로부터 추출된 표본의 표본통계량으로부터 모집단 특성인 모수에 관해 통계적으로 추론하는 절차 활용 분야 정부 경제정책 수립 / 평가 근거자료(통계청 실업률, 고용률, 물가지수) 농업(가뭄, 수해, 병충해에 강한 품종 개발 및 개량) 의학(치료 방법의 효과나 신약 개발을 위한 임상실험 결과 분석) 경영(제품 개발, 품질관리, 시장조사, 영업관리) 스포츠(선수 체질향상, 경기 분석, 전략 분석, 선수 평가, 기용) 6. 데이터 마이닝 데이터 마이닝 대표적인 고급 데이터 분석법 대용량 자료를 요약하고 미래 예측을 목표로 자료의 관계, 패턴, 규칙을 탐색하고 모형화 이전에 알려지지 않은 유용한 지식을 추출하는 분석 방법론 데이터베이스에서의 지식 탐색 데이터 웨어하우스에서 데이터 마트를 생성하면서 각 데이터 속성을 사전분석하여 지식을 얻는 방법 기계학습(machine learning) 인공지능의 한 분야 컴퓨터가 학습할 수 있도록 알고리즘과 기술을 개발하는 분야 인공신경망, 의사결정나무, 클러스터링, 베이지안 분류, SVM 등 패턴인식(pattern recognition) 원자료를 이용하여 사전지식, 패턴에서 추출된 통계 정보를 기반으로 자료 또는 패턴을 분류 장바구니 분석, 연관 규칙 활용 분야 데이터베이스 마케팅(고객 행동정보를 활용한 목표 마케팅, 고객 세분화, 장바구니 분석, 추천 시스템) 신용평가 및 조기경보시스템(금융기관에서 신용카드 발급, 보험, 대출 발생 시) 생물정보학(세포 유전자 분석으로 질병 진단과 치료법, 신약 개발) 텍스트마이닝(전자우편, SNS 등 디지털 텍스트 정보로 고객성향, 감정, 사회관계망 분석) 2장. R 프로그래밍 기초1절. R 소개1. 데이터 분석 도구의 현황 R의 탄생 오픈소스 프로그램으로 통계, 데이터 마이닝과 그래프를 위한 언어 최신 통계 분석과 마이닝 기능 제공 세계적인 사용자와 다양한 예제 공유 가능 패키지가 수시로 업데이트 됨 분석 도구 비교 SAS SPSS 오픈소스 R 프로그램 비용 유료, 고가 유료, 고가 오픈소스 설치 용량 대용량 대용량 모듈화로 간단함 다양한 모듈 지원 및 비용 별도 구매 별도 구매 오픈 소스 최근 알고리즘 및 기술 반양 느림 다소 느림 매우 빠름 학습자료 입수 편의성 유료 도서 위주 유료 도서 위주 공개 논문 및 자료 많음 질의용 공개 커뮤니티 NA NA 매우 활발 R의 특징 오픈소스 프로그램 사용자 커뮤니티에 도움 요청이 많음 많은 패키지가 수시 업데이트 그래픽 및 성능 프로그래밍, 그래픽 측명 등 사용 프로그램과 대등하거나 월등함 시스템 데이터 저장 방식 각 세션 사이마다 시스템에 데이터셋을 저장 → 매번 데이터 로딩 필요가 없음 명령어 스토리 저장 가능 모든 운영체제 윈도우, 맥, 리눅스 운영체제에서 사용 가능 표준 플랫폼 S 통계 언어 기반으로 구현 R/S 플랫폼은 통계전문가의 사실상 표준 플랫폼 객체지향 언어이며 함수형 언어 통계 기능뿐 아니라 일반 프로그래밍 언어처럼 자동화하거나 새로운 함수 생성 가능 객체지향 언어의 특징 SAS, SPSS 회귀 분석 시, 화면에 결과가 나와 추가 작업이 필요 R은 추정계수, 표준오차, 잔차 등 결괏값을 객체에 저장할 수 있어서 활용이 쉬움 함수형 언어의 특징 깔끔하고 단축된 코드 코드 실행이 빠름 단순한 코드로 디버깅 노력 감소 병렬 프로그래밍으로의 전환 용이 R 스튜디오 오픈소스이며 다양한 운영체계 지원 메모리에 변수가 어떻게 되어 있는지, 타입이 무엇인지를 볼 수 있음 스크립트 관리와 도큐먼테이션이 편리 코딩은 스크립트용 프로그래밍으로 어렵지 않게 자동화 가능 래틀(Rattle)은 GUI가 패키지와 긴밀하게 결합외어 있어 정해진 기능만 사용 가능 → 업그레이드가 제대로 되지 않으면 통합성에 문제 발생 R 기반 작업 환경 R 메모리: 64bit 유닉스- 무제한, x86 64bit- 128TB, 64bit 윈도우- 8TB 2절. R 기초 교재 참고 3절. 입력과 출력1. 데이터 분석 과정 분석자가 분석 목적에 맞는 방법론을 선택하여 얻은 결과를 해석하는 과정 INPUT → ANALYSIS → OUTPUT 2. R에서의 데이터 입력과 출력 R에서 다룰 수 있는 파일 타입 Tab-delimited text, Comma-separated text, Excel file, JSON file, HTML/XML file, Database, (other) Statistical SW’s file 4절. 데이터 구조와 데이터 프레임 11. 백터(Vector) 백터들은 동질적 한 백터의 모든 원소는 같은 자료형 또는 같은 모드(mode)를 가짐 백터는 위치로 인덱스 됨 V[2]는 V 백터의 2번째 원소 백터는 인덱스를 통해 여러 개 원소로 구성된 하위 백터를 반환할 수 있음 V[c(2,3)]은 V 백터의 2번째, 3번째 원소로 구성된 하위 백터 백터 원소들은 이름을 가질 수 있음 1234V &lt;- c(10,20,30); names(v) &lt;- c(&quot;Moe&quot;, &quot;Larry&quot;, &quot;Curly&quot;)v[&quot;Larry&quot;] Larry 20 2. 리스트(Lists) 리스트는 이질적 여러 자료형 원소가 포함될 수 있음 리스트는 위치로 인덱스 됨 L[[2]]는 L 리스트의 2번째 원소 리스트에서 하위 리스트 추출 가능 L[c(2,3)]은 L 리스트의 2번째, 3번째 원소로 이루어진 하위 리스트 리스트의 원소들은 이름을 가질 수 있음 L[[“Moe”]]와 L$Moe는 둘 다 “Moe”라는 이름의 원소를 지칭 3. R에서의 자료 형태(mode) 객체 예시 모드 숫자 3.1415 수치형(numeric) 숫자 백터 c(2,3,4,5,5) 수치형(numeric) 문자열 “Tom” 문자형(character) 문자열 백터 c(“Tom”,”Yoon”,”Kim”) 문자형(character) 요인 factor(c(“A”,”B”,”C”)) 수치형(numeric) 리스트 list(“Tom”,”Yoon”,”Kim”) 리스트(list) 데이터 프레임 data.frame(x=1:3, y=c(“Tom”,”Yoon”,”Kim”)) 리스트(list) 함수 print 함수(function) 4. 데이터 프레임(data frames) 강력하고 유연한 구조, SAS 데이터셋을 모방해서 만들어짐 데이터 프레임의 리스트 원소는 백터 또는 요인 백터와 요인은 데이터 프레임의 열 백터와 요인은 동일한 길이 데이터 프레임은 표 형태의 데이터 구조, 열별로 다른 데이터 형식을 가질 수 있음 열에는 이름이 있어야 함 데이터 프레임 원소 접근 방법 123b[1]; b[&quot;empno&quot;] b[[i]]; b[[&quot;empno&quot;]] b$empno 5. 그밖의 데이터 구조 단일값(Scalars) R에서는 원소가 하나인 백터로 인식/처리12pilength(pi) 행렬(Matrix) R에서는 차원을 가진 백터로 인식123a &lt;- 1:9dim(a) &lt;- c(3,3)a 배열(Arrays) 행렬에 3차원 또는 n차원까지 확장된 형태 주어진 백터에 더 많은 차원을 부여해 배열 생성12b &lt;- 1:12dim(b) &lt;- c(2,3,2) 요인(Factors) 백터처럼 생겼지만, R에서는 백터에 있는 고유값(unique key) 정보를 얻는데, 고유값들을 요인의 수준(level)이라고 함 요인의 주된 2가지 사용처: 범주형 변수, 집단 분류 6. 백터, 리스트, 행렬 다루기 행렬(Matrix)은 R에서 차원을 가진 백터이며, 텍스트마이닝과 소셜 네트워크 분석 등에 활용 재활용 규칙(Recycling Rule) 길이가 다른 두 백터 연산을 할 때, R은 짧은 백터의 처음으로 돌아가 연산이 끝날 때까지 원소를 재활용 3장. 데이터 마트1절. 데이터 변경 및 요약1. R reshape를 이용한 데이터 마트 개발 데이터 마트 데이터 웨어하우스와 사용자 사이의 중간층에 위치한 것 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스 데이터 마트 내 대부분의 데이터는 데이터 웨어하우스로부터 복제 또는 자체적으로 수집되거나 관계형/다차원 데이터 베이스를 이용해 구축 CRM 관련 업무 중 핵심: 고객 데이터 마트 구축 동일한 데이터셋 활용 시, 데이터 마트를 어떻게 구축하느냐에 분석 효과 차이를 만듦 요약변수 수집된 정보를 분석에 맞게 종합한 변수 가장 기본적인 변수로 총 구매 금액, 금액, 횟수, 구매여부 등 데이터 분석을 위해 만들어지는 변수 많은 모델을 공통으로 사용할 수 있어 재활용성이 높음 합계, 횟수와 같이 간단한 구조이므로 자동화하여 구측 가능 단점: 얼마 이상이면 구매하더라도 기준값 의미 해석이 애매할 수 있음 → 연속형 변수를 그룹핑해 사용하는 것이 좋음 파생변수 사용자(분석자)가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수 매우 주관적일 수 있으므로 논리적 타당성을 갖추어 개발해야 함 세분화, 고객행동 예측, 캠페인 반응 예측에 활용 파생변수는 상황에 따라 특정 상황에만 유의미하지 않고 대표성을 띄게 해야 함 reshape의 활용 reshape 패키지에는 melt()와 cast()라는 2개 핵심 함수가 있음 melt(): 쉬운 casting을 위해 적당한 형태로 만들어주는 함수 melt(data, id=…) cast(): 데이터를 원하는 형태로 계산, 변형하는 함수 cast(data, formula=… ~ variable, fun) 변수를 조합해 변수명을 만들고 변수를 시간, 상품 등 차원과 결합해 다양한 요약변수와 파생변수를 쉽게 생성하여 데이터 마트를 구성할 수 있게 함 2. sqldf를 이용한 데이터 분석 sqldf는 R에서 sql 명령어를 사용 가능하게 하는 패키지 SAS에서의 proc sql과 같은 역할을 하는 패키지 명령어 차이(sql, R) sql: select * from [data frame], R: sqldf(“select * from [data frame]”) sql: select * from [data frame] numrows 10, R: sqldf(“select * from [data frame] limit 10”) sql: select * from [data frame] where [col] = ‘char%’, R: sqldf(“select * from [data frame] where [col] like ‘char%’ “) 3. plyr을 이용한 데이터 분석 apply 함수에 기반해 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지 split - apply - combine: 데이터 분리, 처리, 결합 등 필수적인 처리 기능 제공 array data frame list nothing array aaply adply alply a_ply data frame daply ddply dlply d_ply list laply ldply llply l_ply n replicates raply rdply rlply r_ply function arguments maply mdply mlply m_ply 4. 데이터 테이블 data.table 패키지는 R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나 data.table은 큰 데이터를 탐색, 연산, 병합하는 데 유용 기존 data.frame 방식보다 월등히 빠른 속도 특정 column을 key 값으로 색인 지정 후, 데이터 처리 빠른 그루핑과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용(속도차 큼) 2절. 데이터 가공1. Data Exploration Data Exploration 데이터 분석을 위해 구성된 데이터 변수들의 상태를 파악 종류 head(데이터셋), tail(데이터셋) summary(데이터셋) 수치형변수: 최대값, 최소값, 평균, 1사분위수, 2사분위수(중앙값), 3사분위수 명목형변수: 명목값, 데이터 개수 2. 변수 중요도 변수 중요도 변수 선택법과 유사한 개념으로 모형을 생성하여 사용된 변수의 중요도를 살피는 과정 종류 klaR 패키지 특정 변수가 주어졌을 때, 클래스가 어떻게 분류되는지에 관한 에러율을 계산하고 그래픽으로 결과를 보여주는 기능 greedy.wilks(): 세분화를 위한 stepwise forward 변수 선택을 위한 패키지, 종속변수에 가장 영향력을 미치는 변수를 wilks lambda를 활용하여 변수 중요도 정리 (Wilk’s Lambda = 집단내분산/총분산) 3. 변수의 구간화 변수의 구간화 연속형 변수를 분석 목적에 맞게 활용하기 위해 구간화하여 모델링에 적용 일반적으로 10진수 단위로 구간화, 구간을 5개로 나누는 것이 보통이며 7개 이상의 구간을 만들지 않음 신용 평가 모형, 고객 세분화 같은 시스템에서 모형에 활용하는 각 변수를 구간화해서 구간별로 점수를 적용하는 스코어링 방식으로 활용 구간화 방법 binning 신용평가모형 개발에서 연속형 변수(부채비율 등)를 범주형 변수로 구간화하는데 자주 활용 의사결정나무 세분화 또는 예측에 활용되는 의사결정나무 모형을 사용해 입력변수 구간화 가능 동일한 변수를 여러 번의 분리 기준으로 사용 가능하기 때문에, 연속변수가 반복적으로 선택될 경우 각각 분리 기준값으로 연속형 변수를 구간화할 수 있음 3절. 기초 분석 및 데이터 관리1. 데이터 EDA(탐색적 자료 분석) 데이터 분석에 앞서 전체적으로 데이터 특징을 파악하고 다양한 각도로 데이터에 접근 summary()를 이용해 데이터의 기초통계량 확인 2. 결측값 인식 결측값은 NA, 99999999, ‘ ‘(공백), Not Answer 등으로 표현 결측값 자체에 의미가 있는 경우도 있음: 쇼핑몰 중 특정 거래 자체가 존재하지 않는 경우, 아주 부자이거나 아주 가난한 경우 정보를 잘 채우지 않음 결측값 처리는 전체 작업속도에 많은 영향을 줌 3. 결측값 처리 방법 단순 대치법(Single Imputation) completes analysis 결측값이 존재하는 레코드 삭제 평균 대치법(Mean Imputation) 관측 또는 실험을 통해 얻어진 데이터의 평균으로 대치 비조건부 평균 대치법: 관측 데이터 평균으로 대치 조건부 평균 대치법(regression imputation): 회귀분석을 활용한 대치법 단순확률 대치법(Single Stochastic Imputation) 평균 대치법에서 추정량 표준 오차의 과소 추정문제를 보완하고자 고안된 방법 Hot-deck 방법, nearest neighbor 방법 등 다중 대치법(Multiple Imputation) m번의 대치를 통해 m개의 가상적 완전 자료를 만드는 방법 1단계: 대치(imputation step), 2단계: 분석(Analysis step), 3단계: 결합(combination step) Amelia-time series cross sectional data set(여러 국가에서 매년 측정된 자료)에서 boostrapping based algorithm을 활용한 다중 대치법 4. R에서 결측값 처리 관련 함수 함수 내용 complete.cases() 데이터 내 레코드에 결측값 있으면 FALSE, 있으면 TRUE로 반환 is.na() 결측값을 NA로 인식하여 결측값 있으면 TRUE, 없으면 FALSE로 반환 DMwR 패키지의 centrallmputation() NA 값에 가운데 값(central value)으로 대치, 숫자는 중위수, 요인(factor)은 최빈값으로 대치 DMwR 패키지의 knnlmputation NA 값을 k 최근 이웃 분류 알고리즘을 사용하여 대치, k개 주변 이웃까지의 거리를 고려하여 가중 평균한 값 사용 Amelia 패키지의 amelia() time-series-cross-sectional data set에서 활용(랜덤포레스트 모델은 결측값 돈재할 경우 바로 에러 발생), ramdomForest 패키지의 rflmpute() 함수를 활용해 NA 결측값을 대치한 후 알고리즘에 적용 5. 이상값(Outlier) 인식과 처리 이상값이란? 의도하지 않게 잘못 입력한 경우 (Bad data) 의도하지 않게 입력되었으나 분석 목접에 부합되지 않아 제거해야 하는 경우 (Bad data) 의도하지 않은 현상이지만 분석에 포함해야 하는 경우 의도된 이상값(fraud, 불량)인 경우 이상값을 꼭 제거해야 하는 것은 아님 이상값 인식 방법 ESD(Extreme Studentized Deviation) 평균으로부터 3 표준편차 떨어진 값(각 0.15%) 기하평균 -2.5 x 표준편차 &lt; data &lt; 기하평균 +2.5 x 표준편차 사분위수 이용하여 제거하기(상자 그림의 outer fence 밖에 있는 값 제거) 이상값 정의: Q1 - 1.5(Q3 - Q1) &lt; data &lt; Q3 + 1.5(Q3 - Q1)을 벗어나는 데이터 극단값 절단(trimming) 방법 기하평균을 이용한 제거 geo_mean 하단, 상단 % 이용한 제거 10% 절단(상하위 5%에 해당되는 데이터 제거) 극단값 조정(winsorizing) 방법 상한값과 하한값을 벗어나는 값들을 상한, 하한값으로 바꾸어 활용","link":"/2020/11/21/Part03_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_1/"},{"title":"Part03. 데이터 분석_2","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 4장. 통계 분석1절. 통계분석의 이해1. 통계 특정집단을 대상으로 수행한 조사 / 실험 결과의 요약된 형태 조사 대상에 따라 총조사(census)와 표본조사로 구분 2. 통계자료의 획득 방법 총 조사(전수 조사, census) 대상 집단 모두를 조사하면 많은 비용과 시간이 소요되므로 특별한 경우를 제외하고는 사용되지 않음 표본조사 모집단에서 샘플을 추출하여 진행하는 조사 모집단(population): 조사하고자 하는 대상 집단 전체 원소(element): 모집단을 구성하는 개체 표본(sample): 조사하기 위해 추출한 모집단의 일부 원소 모수(parameter): 표본 관측에 의해 구하고자 하는 모집단에 대한 정보 모집단의 정의, 표본 크기, 조사 방법, 조사 기간, 표본 추출 방법을 정확히 명시해야 함 표본 추출 방법 단순 랜덤 추출법(simple random sampling) 각 샘플에 번호를 부여해 임의의 n개를 추출하는 방법 각 샘플이 선택될 확률은 동일(비복원, 복원(추출 element를 다시 집어넣음) 추출) 계통추출법(systematic sampling) 단순랜덤추출법의 변형된 방식 임의 위치에서 매 k번째 행목을 추출하는 방법 번호를 부여한 샘플을 나열하여 K개씩 (K = N/n) n개의 구간으로 나누고 첫 구간(1, 2, …, K)에서 하나를 임의로 선택한 후 K개씩 띄어서 n개의 표본을 선택 집락추출법(cluster random sampling) 군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링 하는 방법 지역 표본 추출, 다단계 표본 추출 층화추출법(stratified random sampling) 이질적인 원소들로 구성된 모집단에서 각 계층을 대표할 수 있도록 표본을 추출하는 방법 유사한 원소끼리 몇 개의 층(stratum)으로 나누어 각 층에서 랜덤 추출하는 방법 비례층화추출법, 불비례층화추출법 측정(measurement) 측정 표본조사나 실험 과정에서 추출된 원소들이나 실험 단위로부터 주어진 목적에 적합하도록 관측하여 자료를 얻는 것 측정 방법 내용 명목척도 측정 대상이 어느 집단에 속하는지 분류할 때 사용 (성별, 출생지 구분) 순서척도 측정 대상의 서열관계를 관측하는 척도 (만족도, 선호도, 학년, 신용등급) 구간척도(등간척도) 측정 대상이 갖는 속성의 양을 측정, 구간이나 구간 사이 간격에 의미 있는 자료 (온도, 지수) +,- 가능 *,/ 불가능 비율척도 간격(차이) 비율이 의미를 가지는 자료, 절대적 기준인 0이 존재하고 사칙연산 가능, 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리) 질적 척도: 명목척도, 순서척도 → 범주형 자료, 숫자 크기 차이가 계산되지 않는 척도 양적 척도: 구간척도, 비율척도 → 수치형 자료, 숫자 크기 차이를 계산할 수 있는 척도 3. 통계분석 통계분석 특정한 집단이나 불확실한 현상을 대상으로, 자료를 수집해 대상 집단 정보를 구하고 통계분석 방법을 이용하여 의사결정 하는 과정 기술통계(descriptive statistic) 주어진 자료로부터 어떠한 판단이나 예측과 같은 주관이 섞일 수 있는 과정을 배제하여 통계집단의 특성을 수량화하여 객관적인 데이터로 나타내는 통계분석 방법론 sample에 대한 특성인 평균, 표준편차, 중위수, 최빈값, 그래프, 왜도, 첨도 등을 구하는 것 통계적 추론(추측통계, inference statistics) 수집된 자료를 이용해 대상 집단(모집단)에 관한 의사결정을 하는 것 sample을 통해 모집단을 추정 모수추정 표본집단으로부터 모집단의 특성인 모수(평균, 분산 등)를 분석하여 모집단 추론 가설검정 대상집단에 관해 특정한 가설을 설정한 후에 가설 채택 여부를 결정하는 방법론 예측 미래의 불확실성을 해결해 효율적인 의사결정을 하기 위해 활용 예. 회귀분석, 시계열분석 등 4. 확률 및 확률분포 확률 표본공간 S에 부분집합인 각 사상에 대해 실수값을 가지는 함수의 확률값이 0과 1 사이에 있고 전체 확률의 합이 1인 것을 의미 표본공간 Ω의 부분집합인 사건 E의 확률은 표본공간의 원소 개수에 대한 사건 E 개수의 비율로 확률을 P(E)라고 할 때, 다음과 같의 정의 P(E) = $\\frac[n(E)][N(Ω)]$ 표본공간(sample space, Ω) 어떤 실험을 실시할 때 나타날 수 있는 모든 결과 집합 사건(event) 관찰자가 관심 있는 사건, 표본공간의 부분집합 원소(element) 나타날 수 있는 개별 결과 확률변수(random variable) 특정값이 나타날 가능성이 확률적으로 주어지는 변수 정의역(domain)이 표본공간, 치역(range)이 실수값 (0 &lt; y &lt; 1)인 함수 0이 아닌 확률을 갖는 실수값이 형태에 따라, 이산형 확률변수(discrete random variable)와 연속형 확률변수(continuous random variable)로 구분 덧셈정리(배반 X) 사건 A와 사건 B가 동시에 일어날 수 있을 때(교집합 성립) 일어날 확률 P(A 또는 B): P(A∪B) = P(A) + P(B) - P(A∩B) 사건 B가 주어졌을 때, 사건 A의 조건부확률: P(A|B) = P(A∩B)/P(B) 덧셈정리(배반 O) 사건 A와 사건 B가 동시에 일어나지 않을 때 사건 A or 사건 B 중, 한 쪽만 일어날 확률: P(A∪B) = P(A) + P(B) 곱셈정리 사건 A와 B가 서로 무관계하게 나타날 때(독립사건) 사건 B가 주어졌을 때, 사건 A의 조건부확률: P(A|B) = P(A) 확률분포 이산형 확률 변수 베르누이 확률분포(Bernoulli distribution) 결과가 2개만 나오는 경우 동전 던지기, 시험의 합격/불합격, 안타를 칠 확률 이항분포(Binomial distribution) 베르누이 시행을 n번 반복했을 때, k번 성공할 확률 5번 타석에 들어와서 3번 안타를 칠 확률 → n=5, k=3, 안타를 칠 확률 P(x)=타율 성공할 확률 P가 0이나 1에 가깝지 않고 n이 충분히 크면 정규분포에 가까워짐, 1/2에 가까우면 종 모양 기하분포(Geometric distribution) 성공확률이 p인 베르누이 시행에서 첫 번째 성공이 있기까지 x번 실패할 확률 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률 다항분포(Multinomial distribution) 세 가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포 (이항분포 확장한 것) 포아송분포(Poisson distribution) 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 책에 오타가 5p당 10개 나온다고 할 때, 한 페이지에 오타가 3개 나올 확률 최근 5경기에서 10개의 홈런을 쳤다고 할 때, 오늘 경기에서 홈런을 치지 못할 확률 연속형 확률 변수 균일분포(일양분포, Uniform distribution) 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포) 정규분포(Normal distribution) 평균이 μ이고 표준편차가 σ인 X의 확률밀도함수 표준편차가 클 경우 그래프가 퍼져보임 표준정규분포: 평균 0, 표준편차 1 → 정규분포를 표준정규분포로 만드는 공식: z = $\\frac{X-μ}{σ}$ 지수분포(Exponential distribution) 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포 전자레인지 수명 시간, 콜센터에 전화가 걸려올 때까지의 시간, 은행 고객 내방에 걸리는 시간, 버스가 올 때까지 시간 t-분포(t-distribution) 데이터가 연속형일 때, 두 집단 평균이 동일한지 알고 싶을 때 사용 평균이 0을 중심으로 좌우가 동일한 분포 정규분포보다 퍼져 있고 자유도가 커질수록 정규분포에 가까워짐 X^2^-분포(chi-square distribution) 두 집단 간의 동질성 검정에 활용 범주형 자료에 얻어진 관측값과 기대값 차이를 보는 적합성 검정에 활용 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포 6 F-분포(F-distribution) 두 집단 간 분산의 동일성 검정에 사용 확률변수는 항상 양의 값만 갖고 x^2^-분포와 달리 자유도를 2개 가지며 자유도가 커질수록 정규분포에 가까워짐 5. 추정과 가설 검정 추정의 개요 확률표본(random sample) 확률분포는 분포를 결정하는 평균, 분산 등 모수(parameter)를 가지고 있음 특정한 확률분포로부터 독립적으로 반복해 표본을 추출하는 것 각 관찰값들은 서로 독립적이며 동일한 분포를 가짐 추정 표본으로부터 미지의 모수를 추측하는 것 점추정(point estimation) ‘모수가 특정한 값일 것‘이라고 추정하는 것 표본의 평균, 중위수, 최빈값 등을 사용 점추정량의 조건, 표본평균, 분산 불편성: 가능한 표본에서 얻은 추정량의 기대값은 모집단의 모수와 편의(차이)가 없음 효율성: 추정량의 분산이 작을수록 좋음 일치성: 표본 크기가 아주 커지면, 추정량이 모수와 거의 같아짐 충족성: 추정량은 모수에 대해 모든 정보를 제공 표본평균: 모집단 평균(모평균)을 추정하기 위한 추정량, 확률표본의 평균값 표본분산: 모집단의 분산(모분산)을 추정하기 위한 추정량 구간추정(interval estimation) 점추정의 정확성을 보완하기 위해, 확률로 표현된 믿음의 정도 하에서 모수가 특정한 구간에 있을 것이라고 선언하는 것 항상 추정량 분포에 대한 전제와, 구해진 구간 안에 모수가 있을 가능성의 크기(신뢰수준(confidence interval))가 주어져야 함 참고: 모분산을 알 때는 분자에 σ, 모를 때는 S를 넣음 가설검정 모집단에 대한 가설을 설정하고, 표본관찰을 통해 가설의 채택여부를 결정하는 분석 방법 표본 관찰 또는 실험을 통해 귀무가설과 대립가설 중 하나를 선택 귀무가설이 옳다는 전제 하에 검정통계량 값을 구하고, 이 값이 나타날 가능성의 크기에 의해 귀무가설 채택 여부를 결정 귀무가설(null hypothesis, H0) ‘비교하는 값과 차이가 없다, 동일하다’를 기본개념으로 하는 가설 대립가설(alternative hypothesis, H1) 뚜렷한 증거가 있을 때 주장하는 가설 검정통계량(test statistic) 관찰된 표본으로부터 구하는 통계량, 검정 시 가설 진위를 판단하는 기준 유의수준(significance level, α) 귀무가설이 옳은데도 기각하는 확률 크기 기각역(critical regoin, C) 귀무가설이 옳다는 전제 하에서 구한 검정통계량 분포에서, 확률이 유의수준 α인 부분 반대는 채택역(acceptance region) 제1종 오류와 제2종 오류 사실 \\ 가설검정 결과 H0가 사실이라고 판정 H0가 사실 아니라고 판정 H0가 사실 옳은 결정 제1종 오류(α) H0가 사실 아님 제2종 오류(β) 옳은 결정 두 가지 오류는 상충관계라, 가설검정에서는 제1종 오류 크기를 0.1, 0.05, 0.01 등으로 고정한 뒤, 제2종 오류가 최소가 되도록 기각역을 설정함 6. 비모수 검정 모수적 방법 검정하고자 하는 모집단의 분포에 대해 가정하고, 가정 하에서 검정통계량과 검정통계량 분포를 유도해 검정 실시 비모수적 방법 자료가 추출된 모집단의 분포에 대한 아무 제약을 가하지 않고 검정 실시 관측된 자료가 특정분포를 따른다고 가정할 수 없는 경우에 이용 관측된 자료 수가 많지 않거나(30개 미만) 자료가 개체 간의 서열관계를 나타내는 경우에 이용 모수적 검정 vs 비모수적 검정 가설의 설정 모수적 검정: 가정된 분포의 모수에 대해 가설 설정 비모수적 검정: 가정된 분포 x → 가설은 단지 분포의 형태가 동일하다/동일하지 않다’처럼 분포 형태를 설명 검정 방법 모수적 검정: 관측된 자료로 구한 표본평균, 표본분산 등 이용해 검정 비모수적 검정: 관측값의 절대적 크기에 의존하지 않는 관측값의 순위나 두 관측값 차이의 부호 등 이용해 검정 비모수적 검정의 예 부호 검정, 윌콕슨의 순위합검정, 윌콕슨의 부호순위합검정, 만-위트니의 U검정, 런검정, 스피어만의 순위상관계수 2절. 기초 통계분석1. 기술통계 기술통계(Descriptive Statistics) 자료 특성을 그림, 통계량을 사용해 쉽게 파악할 수 있도록 정리하는 것 자료를 요약하는 기초적 통계를 의미 데이터 분석에 앞서 대략적 통계적 수치를 계산 → 통찰력 얻기에 유리 통계량에 이한 자료 정리 중심위치의 측도 산포의 측도: 분산, 표준편차, 범위, 사분위수 범위 등 분포 형태에 관한 측도 왜도: 분포의 비대칭 정도를 나타내는 측도 m3 &gt; 0: 오른쪽으로 긴 꼬리를 갖는 분포 (최빈값 &lt; 중앙값 &lt; 평균) m3 = 0: 좌우가 대칭인 분포 m3 &lt; 0: 왼쪽으로 긴 꼬리를 갖는 분포 (평균 &lt; 중앙값 &lt; 최빈값) 그래프를 이용한 자료 정리 히스토그램 표로 된 도수분포를 그림으로 나타낸 것 막대그래프 vs 히스토그램 막대그래프 범주(category)형으로 구분된 데이터를 표현 → 의도에 따라 범주의 순서를 바꿀 수 있음 직업, 종교, 음식 히스토그램 연속(continuous)형으로 표시된 데이터 → 임의로 순서를 바꿀 수 없고 막대의 간격이 없음 몸무게, 성적, 연봉 히스토그램의 생성 계급의 수는 2^k^ ≥ n을 만족하는 최소의 정수 log2n = k에서 최소의 정수 계급 간격은 $\\frac{(최대값 - 최소값)}{계급수}$로 파악 가능 계급 수와 간격이 변하면 히스토그램 모양도 변함 줄기-잎 그림(stem-and leaf plot) 상자그림(Box plot) 다섯 숫자 요약을 통해 그림으로 표현(최소값, Q1, Q2, Q3, 최대값) 사분위수 범위(IQR): Q3 - Q1 안울타리(inner fence): Q1 - 1.5 X IQR 또는 Q3 + 1.5 X IQR 바깥울타리(outer fence): Q1 - 3 X IQR 또는 Q3 + 3 X IQR 보통이상점(mild outlier): 안쪽울타리와 바깥울타리 사이 자료 극단이상점(extreme outlier): 바깥울타리 밖 자료 2. 인과관계의 이해 용어 종속변수(반응변수, y) 다른 변수의 영향을 받는 변수 독립변수(설명변수, x) 영향을 주는 변수 산점도(sxatter plot) 좌표평면 위에 점들로 표현한 그래프 공분산(covariance) 두 확률변수 X, Y 방향의 조합(선형성) 공분산 부호로 두 변수의 방향성 확인 가능 공분산 부호가 +: 두 변수는 양의 방향성, 공분산 부호가 -: 두 변수는 음의 방향성을 가짐 X, Y가 서로 독립이면, Cov(X,Y) = 0 3. 상관분석 상관분석(Correlation Analysis) 두 변수 간 관계의 정도를 알아보기 위한 분석 방법 상관계수(Correlation coefficient)이용 상관관계 특성 상관계수 범위 해석 0.7 &lt; r ≤ 1 강한 양(+)의 상관이 있다 0.3 &lt; r ≤ 0.7 약한 양(+)의 상관이 있다 0 &lt; r ≤ 0.3 거의 상관이 없다 r = 0 상관관계(선형, 직선)가 존재하지 않는다 -0.3 ≤ r &lt; 0 거의 상관이 없다 -0.7 ≤ r &lt; -0.3 약한 음(-)의 상관이 있다 -1 ≤ r &lt; -0.7 강한 음(-)의 상관이 있다 상관분석 유형 구분 피어슨 스피어만 개념 등간척도 이상으로 측정된 두 변수의 상관관계 측정 방식 서열척도인 두 변수 상관관계 측정 방식 특징 연속형 변수, 정규성 가정, 대부분 많이 사용 순서형 변수, 비모수적 방법, 순위 기준 상관관계 측정 상관계수 피어슨 r(적률상관계수) 순위상관계수(p, 로우) 상관분석을 위한 R 분산: var(x,y=NULL, na.rm=FALSE) 공분산: cov(x,y=NULL, use=”everything”, method=c(“pearson”, “kendall”, “spearman”)) 상관관계: cor(x,y=NULL, use=”everything”, method=c(“pearson”, “kendall”, “spearman”)) 상관관계(Hmisc 패키지): rcorr(matrix(data명), type=c(“pearson”, “kendall”, “spearman”)) 상관분석의 가설 검정 상관계수 r이 0이면 입력변수 x와 출력변수 y사이에는 아무런 관계가 없음 (귀무가설: r=0, 대립가설: r≠0) t 검정통계량을 통해 얻은 p-value값이 0.05 이하인 경우, 대립가설을 채택하게 되어 데이터에서 구한 상관계수를 활용할 수 있게 됨 상관분석 예제 cov: 공분산 cor: 상관계수 p-value: 유의수준 0.05보다 작게 나타나면 상관계수가 있음 3절. 회귀분석1. 회귀분석 회귀분석 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법 변수 사이의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위한 분석 방법 독립변수의 개수가 하나면 단순선형회귀분석, 독립변수 개수가 두 개 이상이면 다중선형회귀분석 회귀분석의 변수 영향 받는 변수(y): 반응변수(response variable), 종속변수(dependent variable), 결과변수(outcome variable) 영향 주는 변수(x): 설명변수(explanatory variable), 독립변수(independent variable), 예측변수(predictor variable) 선형회귀분석의 가정 선형성 입력변수와 출력변수의 관계가 선형 (가장 중요한 가정) 등분산성 오차 분산이 입력변수와 무관하게 일정 잔차플롯(산점도)를 활용해 잔차와 입력변수 간 아무런 관련성이 없게 무작위적으로 고루 분포돼야 등분산성 가정 만족 독립성 입력변수와 오차는 관련 없음 자기상관(독립성)을 알아보기 위해 Durbin-Waston 통계량 사용 시계열 데이터에서 많이 활용 비상관성 오차들끼리 상관이 없음 정상성(정규성) 오차 분포가 정규분포를 따름 Q-Q plot, Kolmogolov-Sirnov 검정, Shaprio-Wilk 검정 등 활용 가정에 대한 검증 단순선형회귀분석 입력변수와 출력변수 간 선형성을 점검하기 위해 산점도 확인 다중선형회귀분석 선형회귀분석 가정인 선형성, 등분산성, 독립성, 정상성이 모두 만족하는지 확인 2. 단순선형회귀분석 하나의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 통계기법 회귀분석 검토사항 회귀계수가 유의미한가? 해당 계수의 t 통계량의 p-값이 0.05보다 작으면 해당 회귀계수가 통계적으로 유의하다고 볼 수 있음 모형이 설명력을 갖는가? 결정계수(R^2^)를 확인 결정계수는 0~1값을 가지며, 높을수록 추정된 회귀식의 설명력이 높아짐 모형이 데이터를 잘 적합하는가? 잔차를 그래프로 그리고 회귀진단 회귀계수의 추정 최소제곱법, 최소자승법 측정값을 기초로 적당한 제곱합을 만들고 이를 최소로 하는 값을 구해 측정결과를 처리 잔차제곱이 가장 작은 선을 구하는 것 회귀분석의 검정 회귀계수의 검정 회귀계수 β1이 0이면 입력변수 X와 출력변수 y 사이에는 아무런 인과관계가 없음 회귀계수 β1이 0이면 적합된 추정식은 아무 의미가 없음 (귀무가설 β1=0, 대립가설 1≠0) 3. 다중선형회귀분석 다중선형회귀분석(다변량회귀분석) 다중회귀식 Y = β0 + β1X1 + β2X2 + … + βkXk + ε 모형의 통계적 유의성 모형의 통계적 유의성은 F통계량으로 확인 유의수준 5% 하에서 F통계량의 p-값이 0.05보다 작으면 추정된 회귀식은 통계적으로 유의하다 볼 수 있음 F통계량이 크면 p-value가 0.05보다 작아지고 귀무가설을 기각함 → 모형이 유의하다고 결론 내릴 수 있음 회귀계수의 유의성 단변량 회귀분석의 회귀계수 유의성 검토와 같이 t통계량을 통해 확인 모든 회귀계수의 유의성이 통계적으로 검증되어야 선택된 변수 조합으로 모형 확인 가능 모형의 설명력 결정계수(R^2^)나 수정된 결정계수(R^2^α) 확인 모형의 적합성 잔차와 종속변수의 산점도로 모형이 데이터를 잘 적합하고 있는지 확인 데이터가 전제하는 가정을 만족하는가? 선형성, 독립성, 등분산성, 비상관성, 정상성 다중공선성(multicollinearity) 다중회귀분석에서 설명변수 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란함 다중공선성 검사 방법 분산팽창요인(VIF): 4보다 크면 다중공산성 존재한다고 볼 수 있고, 10보다 크면 심각한 문제가 있다고 해석 상태지수: 10 이상이면 문제 있다고 보고, 30보다 크면 심각한 문제가 있다고 해석 다중선형회귀분석에서 다중공선성 문제 발생 시, 문제 있는 변수를 제거하거나 주성분회귀, 능형회귀 모형을 적용하여 문제 해결 4. 회귀분석의 종류 종류 내용 단순회귀 독립변수가 1개이며 종속변수와의 관계가 직선 다중회귀 독립변수가 k개이며 종속변수와의 관계가 선형(1차 함수) 로지스틱 회귀 종속변수가 범주형(2진변수)인 경우에 적용, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장할 수 있음 다항회귀 독립변수와 종속변수와의 관계가 1차 함수 이상인 관계(단, k=1이면 2차 함수 이상) 곡선회귀 독립변수가 1개이며 종속변수와의 관계가 곡선 비선형회귀 회귀식 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형 5. 회귀분석 사례 F-statistic: F-통계량 p-value: 유의수준 5% 하에서 추정되어야 해당 회귀 모형이 통계적으로 유의하다고 할 수 있음 Multiple R-squared: 결정계수, Adjusted R-squared: 수정된 결정계수 (0~1값을 가지며, 높을수록 회귀식의 설명력 높아짐) Pr: 회귀계수들의 p-값 6. 최적회귀방정식 설명변수 선택 상황에 따라 필요한 변수만 선택 y에 영향을 미칠 수 있는 모든 설명변수 x가 y값 예측에 참여 데이터에 설명변수 x 수가 많아지면 관리가 어려워, 가능한 범위 내에서 적은 수의 설명변수만 포함 모형선택(exploratiry analysis) 분석 데이터에 가장 잘 맞는 모형을 찾는 방법 가능한 모든 조합의 회귀분석(All possible regression): 가능한 모든 독립변수 조합에 대한 회귀모형을 생성한 뒤, 가장 적합한 회귀모형 선택 단계적 변수 선택(Stepwise Variable Selection) 전진선택법(forward selection) 절편만 있는 상수모형으로 시작해 중요하다고 생각되는 설명변수부터 모형에 추가 후진제거법(backward selection) 독립변수 후보 모두를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 제거 더 제거할 변수가 없을 때의 모형을 선택 단계선택법(stepwise selection) 전진선택법에 이해 변수를 추가하며, 새롭게 추가된 변수에 기인해 기존 변수 중요도가 약화되면 해당변수를 제거 단계별로 추가 또는 제거되는 변수 여부를 검토하고 더 이상 없을 때 중단 벌점화된 선택기준 모형 복잡도에 벌점을 주는 방법 AIC(Akaike information criterion) BIC(Bayesian information criterion) 모든 후보 모형에 대해 AIC 또는 BIC를 계산하고 값이 최소가 되는 모형을 선택 모형 선택의 일치성(consistency inselection) 자료 수가 늘어날 때 참인 모형이 주어진 모형 선택 기준의 최소값을 갖게 되는 성질 이론적으로 AIC에 대해 일치성이 성립하지 않지만, BIC는 주요 분포에서 이러한 성질이 성립 AIC 활용이 보편화된 방식 추가: RIC(Risk inflation criterion), CIC(Covariance inflation criterion), DIC(Deviation information criterion) 최적회귀방정식 사례: 교재 참고 변수 선택법 예제(유의확률 기반) 변수 선택법 예제(벌점화 전진선택법) 변수 선택법 예제(벌점화 후진제거법) 4절. 시계열 분석1. 시계열 자료 시계열 자료 시간의 흐름에 따라 관찰된 값 시계열 데이터 분석을 통해 미래의 값을 예측하고 경향, 주기, 계절성 등을 파악하여 활용 시계열 자료의 종류 비정상성 시계열 자료 시계열 분석을 실시할 때, 다루기 어려운 자료 정상성 시계열 자료 비정상 시계열을 핸들링해 다루기 쉬운 시계열 자료로 변환한 자료 2. 정상성 평균이 일정할 경우 모든 시점에 대해 일정한 평균을 가짐 평균이 일정하지 않은 시계열은 차분(difference)을 통해 정상화할 수 있음 차분? 현 시점 자료에서 전 시점 자료를 빼는 것 일반차분: 바로 전 시점 자료를 빼는 방법, 계절차분: 여러 시점 전의 자료를 빼는 방법 분산이 일정 분산도 시점에 의존하지 않고 일정해야 함 분산이 일정하지 않을 경우 변환(transformation)을 통해 정상화할 수 있음 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않음 정상 시계열 어떤 시점에서 평균과 분산, 특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값을 가짐 정상 시계열은 항상 그 평균값으로 회귀하려는 경향이 있으며, 그 평균값 주변에서의 변동은 대체로 일정한 폭을 가짐 정상 시계열이 아닌 경우 특정 기간의 시계열 자료로부터 얻은 정보를 다른 시기로 일반화할 수 없음 3. 시계열자료 분석방법 분석방법 회귀분석(계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등 자료 형태에 따른 분석방법 일변량 시계열분석 Box-Jenkins(ARMA), 지수평활법, 시계열 분해법 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심 갖는 경우의 시계열분석 다중 시계열분석 계량경제모형, 전이함수모형, 개입분석, 상태공간분석, 다변량 ARIMA 등 여러 개의 시간(t)에 따른 변수들을 활용하는 시계열 분석 이동평균법 지수평활법 4. 시계열모형교재 참고 5절. 다차원척도법1. 다차원척도법(Multidimensional Scaling) 객체간 근접성을 시각화하는 통계기법 군집분석과 같이 개체를 대상으로 변수들을 측정한 후, 개체 사이의 유사성/비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석 방법 개체들을 2차원 또는 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석 방법 2. 다차원척도법 목적 데이터 속에 잠재해 있는 패턴, 구조를 찾아냄 찾아낸 구조를 소수 차원의 공간에 기하학적으로 표현 데이터 축소 목적으로 다차원척도법을 이용 → 데이터에 포함되는 정보를 끄집어내기 위한 탐색수단 다차원척도법에 의해 얻은 결과를, 데이터가 만들어진 현상이나 과정에 고유의 구조로서 의미 부여 3. 다차원척도법 방법 객체들의 거리 계산: 유클리드 거리행렬 활용 관측대상의 상대적 거리 정확도를 높이기 위해 적합 정도를 스트레스값으로 나타냄 각 개체를 공간상에 표현하기 위한 방법: 부적합도 기준으로 STRESS나 S-STRESS 사용 최적모형의 적합은 부적합도를 최소로 하는 반복알고리즘을 이용하며, 이 값이 일정 수준 이하가 될 때 최종적으로 적합된 모형으로 제시 STRESS와 적합도 수준 M은 개체들을 공간상에 표현하기 위한 방법으로 STRESS나 S-STRESS를 부적합도 기준으로 사용 STRESS 적합도 수준 0 완벽(perfect) 0.05 이내 매우 좋은(excellent) 0.05 ~ 0.10 만족(satisfactory) 0.10 ~ 0.15 보통(acceptable, but doubt) 0.15 이상 나쁨(poor) 4. 다차원척도법 종류 계량적 MDS(Metric MDS) 데이터가 구간척도나 비율척도인 경우 활용 N개의 케이스에 대해 P개의 특성변수가 있는 경우, 각 개체들 간 유클리드 거리행렬을 계산하고 개체들 간 비유사성 S(거리제곱 행렬의 선형함수)를 공간상에 표현 비계량적 MDS(nonmetric MDS) 데이터가 순서척도인 경우 활용 개체들 간 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성한 후 적용 6절. 주성분분석1. 주성분분석(Principal Component Analysis) 여러 변수들이 변량을 주성분이라는 서로 상관성이 높은 변수의 선형 결합으로 만들어 기존 상관성이 높은 변수들을 요약, 축소하는 기법 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로는 첫 번째 주성분과는 상관성이 없어서(낮아서) 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만듦 2. 주성분분석의 목적 여러 변수들 간 내재하는 상관관계, 연관성을 이용해 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 함 다중공선성이 존재하는 경우, 상관성 없는(적은) 주성분으로 변수들을 축소하여 모형 개발에 활용 회귀분석이나 의사결정나무 등 모형 개발 시, 입력변수들 간 상관관계가 높은 다중공선성이 존재할 경우 모형이 잘못 만들어져 문제 발생 연관성 높은 변수를 주성분분석을 통해 차원을 축소한 후, 군집분석을 수행하면 군집화 결과와 연산속도 개선 가능 기계에서 나오는 센서데이터를 주성분분석으로 차원 축소 후, 시계열로 분포나 추세 변화를 분석하면 기계의 고장 징후를 사전에 파악하는 데 활용할 수 있음 3. 주성분분석 vs 요인분석 요인분석(Factor Analysis) 등간척도(혹은 비율척도)로 측정한 두 개 이상 변수에 잠재된 공통인자를 찾아내는 기법 공통점 모두 데이터를 축소하는 데 활용 원래 데이터를 활용하여 몇 개의 새로운 변수 생성 가능 차이점 생성된 변수의 수 요인분석은 몇 개라고 지정 없이(2 or 3, 4, 5 …) 만들 수 있음 주성분분석은 제1주성분, 제2주성분, 제3주성분 정도로 활용(대략 4개 이상은 넘지 않음) 생성된 변수 이름 요인분석은 분석자가 요인 이름을 명명 주성분분석은 주로 제1주성분, 제2주성분 등으로 표현 생성된 변수 간 관계 요인분석은 새 변수들은 기본적으로 대등한 관계를 가짐 요인분석은 어떤 것이 더 중요하다는 의미가 없음(분류/예측의 다음 단계로 사용되면 중요성 부여) 주성분분석은 제1주성분이 가장 중요, 그 다음 제2주성분이 중요 분석 방법의 의미 요인분석은 목표변수를 고려하지 않고 데이터가 주어지면 변수를 비슷한 성격으로 묶어서 새로운 (잠재)변수를 만듦 주성분분석은 목표변수를 고려하여 목표변수를 예측/분류하기 위해 원래 변수의 선형 결합으로 이뤄진 몇 개의 주성분(변수)를 찾게 됨 4. 주성분의 선택법 주성분분석 결과에서 누적기여율(cumulative proportion)이 85% 이상이면 주성분 수로 결정할 수 있음 scree plot을 활용하여 고유값(eigenvalue)이 수평을 유지하기 전 단계로 주성분의 수 선택 5. 주성분 분석 사례 교재 참고 5장. 정형 데이터 마이닝1절. 데이터마이닝의 개요1. 데이터마이닝 데이터마이닝 대용량 데이터에서 의미 있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법 통계분석과의 차이점 통계분석은 가설이나 가정에 따른 분석이나 검증을 함 데이터마이닝은 다양한 수리 알고리즘을 이용해 데이터베이스의 데이터로부터 의미 있는 정보를 찾아내는 방법을 통칭 종류 정보를 찾는 방법론에 따라 분석대상, 활용목적, 표현방법에 따라 인공지능, 의사결정나무, K-평균군집화, 연관분석, 회귀분석, 로짓분석, 최근접이웃 시각화분석, 분류, 군집화, 포케스팅 사용분야 병원: 환자 데이터를 이용하여 해당 환자에게 발생 가능성 높은 병 예측 병원: 기존 환자가 응급실에 왔을 때, 어떤 조치를 먼저 해야 하는지 결정 은행: 고객 데이터를 이용해 해당 고객의 우량/불량을 예측하여 대출 여부 판단 공항: 세관 검사에서 입국자 이력과 데이터를 이용해 관세품 반입 여부 예측 2. 데이터마이닝의 분석 방법 Supervised Data Prediction(지도학습) Unsupervised Data Prediction(비지도학습) 의사결정나무, 인공신경망, 일반화 선형 모형, 회귀분석, 로지스틱 회귀분석, 사례기반 추론, 최근접 이웃법 OLAP, 연관성 규칙발견, 군집분석, SOM 3. 분석 목적에 따른 작업 유형과 기법 예측(Predictive Modeling): 분류 규칙 설명(Descriptive Modeling): 연관 규칙, 연속 규칙, 데이터 군집화 작업유형 설명 사용기법 분류 규칙(Classification) 가장 많이 사용되는 작업으로 과거 데이터로부터 고객특성을 찾아 분륨형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것, 목표 마케팅 및 고객 신용평가 모형에 활용 회귀분석, 판별분석, 신경망, 의사결정나무 연관규칙(Association) 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업, 제품이나 서비스 교차판매, 매장진열, 첨부우편, 사기적발 등 분야에 활용 동시발생 매트릭스 연속규칙(Sequence) 연관 규칙에 시간 관련 정보가 포함된 형태, 고객 구매이력 속성이 반드시 필요, 목표 마케팅이나 일대일 마케팅에 활용 동시발생 매트릭스 데이터 군집화(Clustering) 고객 레코드를 유사한 특성을 지닌 몇 개의 소그룹으로 분할, 작업 특성이 분류규칙과 유사하나 분석대상 데이터에 결과값이 없음, 판촉활동이나 이벤트 대상 선정에 활용 K-Means Clustering 4. 데이터마이닝 추진단계 목적 설정 데이터 준비 가공 기법 적용 검증 5. 데이터마이닝을 위한 데이터 분할 개요 모델 평가용 테스트 데이터와 구축용 데이터로 분할 구축용 데이터로 모형을 생성하고 테스트 데이터로 모형이 얼마나 적합한지를 판단 데이터 분할 구축용(training data, 50%) 추정용, 훈련용 데이터라고도 불리며 데이터마이닝 모델을 만드는 데 활용 검정용(validation data, 30%) 구축된 모형의 과대추정 또는 과소추정을 미세 조정하는 데 활용 시험용(tast data, 20%) 테스트 데이터나 과거 데이터를 활용하여 모델의 성능을 검증하는 데 활용 데이터 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우 홀드아웃 방법 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용 주로 학습용과 시험용으로 분리하여 사용 교차확인 방법 주어진 데이터를 k개의 하부집단으로 구분 k-1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습 k번 반복 측정한 결과를 평균낸 값을 최종값으로 사용 주로 10-fold 교차분석을 많이 사용 6. 성과분석 오분류에 대한 추정치 정분류율(Accuracy) Accuracy = $\\frac{TN + TP}{TN + TP + FN + FP}$ 오분류율(Error Rate) 1 - Accuracy = $\\frac{FN + FP}{TN + TP + FN + FP}$ 특이도(Specificity) Specificity = $\\frac{TN}{TN + FP}$ (TNR: True Negative Rate) 민감도(Sensitivity) Sensitivity = $\\frac{TP}{TP + FN}$ (TPR: True Positive Rate) 정확도(Precision) Precision = $\\frac{TP}{TP + FP}$ 재현율(Recall): 민감도와 같음 Recall = $\\frac{TP}{TP + FN}$ F1 Score F1 = 2 x $\\frac{Precision x Recall}{Precision + Recall}$ ROCR 패키지로 성과분석 ROC Curve(Receiver Operation Characteristic Curve) 가로축을 FPR(False Positive Rate = 1 - 특이도)값, 세로축을 TPR(Ture Positive Rate, 민감도)값으로 두어 시각화한 그래프 2진 분류(binary classfication)에서 모형 성능을 평가하기 위해 사용되는 척도 그래프가 왼쪽 상단에 가깝게 그려질수록 올바르게 예측한 비율은 높고 잘못 예측한 비율은 낮음을 의미 ROC 곡선 아래 면적을 의미하는 AUROC(Area Under ROC) 값이 클수록(1에 가까울 수록) 모형 성능이 좋다고 평가 TPR: 1인 케이스에 대한 1로 예측한 비율 FPR: 0인 케이스에 대한 1로 잘못 예측한 비율 AUROC를 이용한 정확도의 판단 기준 기준 구분 0.9 - 1.0 excellent (A) 0.8 - 0.9 good 0.7 - 0.8 fair 0.6 - 0.7 poor 0.5 - 0.6 fail 이익도표(Lift chart) 분류모형 성능을 평가하기 위한 척도 (분류된 관측치에 대해 예측이 얼마나 잘 이루어졌는지) 임의로 나눈 등급별로 반응검출율, 반응률, 리프트 등 정보를 산출해 나타내는 도표 기본 향상도에 비해 반응률이 몇 배나 높은지 계산: 향상도(Lift) 각 등급은 예측확률에 따라 매겨진 순위이므로, 상위 등급에서는 더 높은 반응률을 보이는 것이 좋은 모형 2절. 분류분석1. 분류분석과 예측분석 분류분석의 정의 데이터가 어떤 그룹에 속하는지 예측할 때 사용하는 기법 클러스터링과 유사하나, 분류분석은 각 그룹이 정의되어 있음 교사학습(supervised learning)에 해당하는 예측기법 예측분석의 정의 시계열분석처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것 모델링을 하는 입력 데이터가 어떤 것인지에 따라 특성이 다름 여러 개의 다양한 설명변수(독립변수)가 아닌 한 개의 설명변수로 생각하면 됨 분류분석 vs 예측분석 공통점 레코드 특정 속성의 값을 미리 알아맞힐 수 있음 차이점 분류: 레코드(튜플)의 범주형 속성의 값을 맞힘(국/영/수 점수로 내신 등급 맞히기) 예측: 레코드(튜플)의 연속형 속성의 값을 맞힘(카드 회원 가입정보로 연 매출액 알아맞히기) 분류 모델링 신용평가모형(우량, 불량) 사기방지모형(사기, 정상) 이탈모형(이탈, 유지) 고객세분화(VVIP, VIP, GOLD, SILVER, BRONZE) 분류 기법 회귀분석, 로지스틱 회귀분석 의사결정나무, CART, C5.0 베이지안 분류 인공신경망 지지도벡터기계 k 최근접 이웃 규칙기반의 분류와 사례기반추록 2. 로지스틱 회귀분석(Logistic Regression) 반응변수가 범주형인 경우에 적용되는 회귀분석모형 새로운 설명변수(또는 예측변수)가 주어질 때, 반응변수의 각 범주(또는 집단)에 속할 확률이 얼마인지 추정(예측모형)하여, 추정 확률을 기준치에 따라 분류하는 목적(분류모형) 사후확률(Posterior Probability): 모형의 적합을 통해 추정된 확률 exp(β1): 나머지 변수가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지 나타내는 값 표준 로지스틱 분포의 누적함수로 성공 확률을 추정 선형회귀분석 vs 로지스틱 회귀분석 목적 선형회귀분석 로지스틱 회귀분석 종속변수 연속형 변수 (0, 1) 계수 추정법 최소제곱법 최대우도추정법 모형 검정 F-검정, T-검정 카이제곱 검정(x^2^-test) glm() 함수를 활용하여 로지스틱 회귀분석을 실행 3. 의사결정나무 정의 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법 연속적으로 발생하는 의사결정 문제를 시각화 계산결과가 의사결정나무에 직접적으로 나타나서 해석이 간편함 주어진 입력값에 대해 출력값을 예측하는 모형 → 분류나무와 회귀나무 모형 예측력과 해석력 의사결정나무의 활용 세분화 데이터를 비슷한 특성을 갖는 몇 개 그룹으로 분할해 그룹별 특성을 발견하는 것 분류 여러 예측변수에 근거해 관측개체의 목표변수 범주를 몇 개 등급으로 분류하고자 하는 경우에 사용 예측 자료에서 규칙을 찾고 이를 이용해 미래 사건을 예측하고자 하는 경우에 사용 차원축소 및 변수선택 많은 예측변수 중 목표변수에 큰 영향을 미치는 변수를 골라내고자 하는 경우에 사용 교호작용효과의 파악 여러 개 예측변수를 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우 범주의 병합 또는 연속형 변수의 이산화: 범주형 목표변수의 범주를 소수 몇 개로 병합하거나, 연속형 목표변수를 몇 개의 등급으로 이산화하고자 하는 경우 의사결정나무 특징 장점 결과 설명 용이 모형 만들기가 계산적으로 복잡하지 않음 대용량 데이터에서도 빠르게 만들 수 있음 비정상 잡음 데이터도 민감함 없이 분류 가능 한 변수와 상관성 높은, 다른 불필요한 변수가 있어도 크게 영향 받지 않음 설명변수나 목표변수에 수치형변수와 범주형변수 모두 사용 가능 모형 분류 정확도가 높음 단점 새로운 자료에 대한 과대적합이 발생할 가능성이 높음 분류 경계선 부근 자료값에 대해 오차가 큼 설명변수 간 중요도 판단이 어려움 의사결정나무 분석 과정 성장: 적절한 정지규칙을 만족하면 중단 가지치기 타당성 평가 해석 및 예측 나무의 성장 분리규칙(splitting rule) 분리기준(splitting criterion) 이산형 목표변수 기준값 분리기준 카이제곱 통계량 p값 P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 지니지수 지니지수를 감소시키는 예측변수와 그때의 최적분리에 의해 자식마디 선택 엔트로피 지수 엔트로피 지수가 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 연속형 목표변수 기준값 분리기준 분산분석에서 F통계량 P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 분산의 감소량 분산 감소량을 최대화하는 기준의 최적분리에 의해 자식마디 형성 정지규칙 더이상 분리가 일어나지 않고 현재 마디가 끝마디가 되도록 하는 규칙 정지기준: 의사결정나무 깊이를 지정, 끝마디의 레코드 수의 최소 개수를 지정 나무의 가지치기(Pruning) 너무 큰 나무모형은 자료를 과대적합, 너무 작은 나무모형은 과소적합할 위험 나무 크기를 모형 복잡도로 볼 수 있으며, 최적 나무 크기는 자료로부터 추정하게 됨 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정 수(가령 5) 이하일 때 분할을 정지 비용 - 복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 됨 4. 불순도의 여러 가지 측도 목표변수가 범주형 변수인 의사결정나무 분류규칙을 선택 카이제곱 통계량 각 셀에 대한 ((실제도수 - 기대도수)의 제곱 / 기대도수) 합으로 구할 수 있음 기대도수 = 열의 합계 x 합의 합계 / 전체합계 지니지수 노드의 불순도를 나타내는 값 지니지수 값이 클수록 이질적이며 순수도가 낮다고 볼 수 있음 엔트로피 지수 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도 엔트로피 지수 값이 클수록 순수도가 낮다고 볼 수 있음 엔트로피 지수가 가장 작은 예측변수와 이때의 최적분리 규칙에 의해 자식마디 형성 5. 의사결정나무 알고리즘 CART 불순도의 측도로 출력변수가 범주형일 경우 지니지수를 이용, 연속형인 경우 분산을 이용한 이진분리 사용 개별 입력변수뿐 아니라 입력변수의 선형결합 중에서 최적의 분리를 찾을 수 있음 C4.5와 C5.0 CART와는 다르게 각 마디에서 다지분리가 가능 범주형 입력변수에 대하여는 범주 수만큼 분리가 일어남 불순도의 측도로는 엔트로피지수 사용 CHAID 가지치기 하지 않고 적당한 크기에서 나무모형의 성장을 중지 입력변수가 반드시 범주형 변수여야 함 불순도 측도로는 카이제곱 통계량 사용 3절. 앙상블 분석 앙상블 주어진 자료로부터 여러 개 예측모형을 만든 후, 조합하여 하나의 최종 예측 모형을 만드는 방법 다중 모델 조합, 분류기 조합 학습방법의 불안정성 학습자료의 작은 변화에 의해 예측모형이 크게 변하는 경우, 그 학습방법은 불안정함 가장 안정적인 방법 1-nearest neighbor: 가장 가까운 자료만 변하지 않으면 예측모형 변하지 않음 선형회귀모형: 최소제곱법으로 추정해 모형 결정 가장 불안정한 방법: 의사결정나무 앙상블 기법의 종류 배깅 주어진 자료에서 여러 개의 붓스트랩 자료를 생성하고 각 붓스트랩 자료에 예측모형을 만든 후 결합하여 최종 예측모형을 만드는 방법 붓스트랩(bootstrap): 주어진 자료에서 동일한 크기 표본을 랜덤 복원추출로 뽑은 자료 보팅(voting): 여러 개 모형으로부터 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정나무를 활용 훈련자료 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없음 → 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있음 부스팅 예측력 약한 모형을 결합하여 강한 예측모형을 만드는 방법 훈련오차를 빠르고 쉽게 줄일 수 있음 배깅에 비해 많은 경우의 예측오차가 향상 Adaboost: 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 가중치를 설정하고 n개 분류기를 결합하여 최종 분류기 만드는 방법(단, 가중치 합은 1) 랜덤 포레스트(random forest) 분산이 크다는 의사결정나무 특징을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 줌 약한 학습기를 생성한 후, 이를 선형 결합하여 최종 학습기를 만드는 방법 랜덤한 forest에는 많은 트리가 생성됨 정확도 측면에서 좋은 성과 이론적 설명이나 최종 결과 해석이 어렵지만, 예측력이 매우 높음 4절. 인공신경망 분석1. 인공신경망 분석(ANN) 인공신경망이란? 인간 뇌를 기반으로 한 추론 모델 뉴런: 기본적인 정보처리 단위 인간의 뇌를 형상화한 인공신경망 인간 뇌의 특징 100억 개 뉴런과 6조 개 시냅스의 결합체 인간의 뇌: 컴퓨터보다 빠르고, 복잡하고, 비선형적, 병렬적인 정보 처리 시스템 적응성에 따라 잘못된 답에 대한 뉴런 사이 연결은 약화되고, 올바른 답에 대한 연결이 강화됨 인간 뇌 모델링 뉴런은 가중치 있는 링크로 연결되어 있음 뉴런은 여러 입력 신호를 받으나, 출력 신호는 하나만 생성함 인공신경망의 학습 신경망은 가중치를 반복적으로 조정하며 학습 뉴런은 링크로 연결되어 있고, 각 링크에는 수치적인 가중치가 있음 신경망 가중치를 초기화 → 훈련 데이터로 가중치 갱신 → 신경망 구조 선택 → 활용할 학습 알고리즘 결정 → 신경망 훈련 인공신경망 특징 구조 입력 링크에서 여러 신호를 받아 새로운 활성화 수준을 계산하고 출력 링크로 출력 신호를 보냄 입력 신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있음 출력 신호는 문제의 최종적인 해(solution)가 되거나 다른 뉴런에 입력될 수 있음 뉴런의 계산 뉴런은 전이함수, 즉 활성화 함수를 사용 활성화 함수를 이용해 출력을 결정하며, 입력신호의 가중치 합을 계산하여 임계값과 비교 가중치 합이 임계값보다 작으면 뉴련의 출력은 -1, 같거나 크면 +1을 출력 뉴런의 활성화 함수 시그모이드 함수: 로지스틱 회귀분석과 유사하며 0~1의 확률값을 가짐 softmax 함수: 표준화지수 함수로도 불리며, 출력값이 여러 개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수 relu 함수: 입력값이 0 이하는 0, 0 이상은 x값을 가지는 함수, 최근 딥러닝에서 많이 활용 단일 뉴런의 학습(단층 퍼셉트론) 퍼셉트론은 선형 결합기와 하드 리미터로 구성 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눔 초평면을 선형 분리 함수로 정의 신경망 모형 구축 시 고려사항 입력변수 신경망 모형은 복잡성으로 인해 입력 자료 선택에 매우 민감 입력변수가 범주형 또는 연속형 변수일 때 아래 조건이 신경망 모형에 적합 범주형 변수: 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주 빈도가 일정할 때 연속형 변수: 입력변수 값의 범위가 변수간의 큰 차이가 없을 때 연속형 변수의 경우, 분포가 평균을 중심으로 대칭이 아니면 좋지 않은 결과를 도출하므로 아래 방법을 활용 변환: 고객 소득(대부분 평균 미만, 특정 고객 소득이 매우 큰) 로그 변환 범주화: 각 범주 빈도가 비슷해지도록 설정 범주형 변수의 경우 가변수화하여 적용 가능한 경우 모든 범주형 변수는 같은 범위를 갖도록 가변수화 하는 것이 좋음 가중치의 초기값과 다중 최소값 문제 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐 → 초기값 선택은 매우 중요한 문제 가중치가 0이면 시그모이드 함수는 선형, 신경망 모형은 근사적으로 선형모형이 됨 일반적으로 초기값은 0 근처로 랜덤하게 선택 → 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할수록 비선형모형이 됨 참고: 초기값이 0이면 반복해도 값이 전혀 변하지 않고, 너무 크면 좋지 않은 해를 주는 문제점 내포 학습모드 온라인 학습모드(online learning mode) 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀜 일반적으로 속도가 빠름, 훈련자료에 유사값 많은 경우 그 차이가 더 두드러짐 훈련자료가 비정상성과 같이 특이한 성질을 가진 경우가 좋음 국소최솟갑셍서 벗어나기 더 쉬움 확률적 학습모드(probabilistic learning mode) 온라인 학습모드와 같으나, 신경망에 투입되는 관측값의 순서가 랜덤 배치 학습모드(batch learning mode) 전체 훈련자료를 동시에 신경망에 투입 은닉층(hidden layer)과 은닉노드(hidden node)의 수 신경망을 적용할 때, 가장 중요한 부분이 모형의 선택 은닉층과 은닉노드가 많으면 가중치가 많아져서 과대 적합 문제 발생 은닉층과 은닉노드가 적으면 과소적합 문제 발생 은닉층 수가 하나인 신경망: 범용 근사자 → 모든 매끄러운 함수 근사적 표현 가능 은닉노드 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 적용하는 것이 좋음 과대 적합 문제 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번히 발생 알고리즘 조기종료와 가중치 감소 기법으로 해결할 수 있음 모형 적합 과정에서 검증오차가 증가하면 반복을 중지하는 조기종료 시행 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법 활용 5절. 군집분석1. 군집분석 개요 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간 상이성을 규명하는 분석 방법 특성에 따라 고객을 여러 개의 배타적인 집단으로 나눔 결과는 구체적인 군집분석 방법에 따라 차이 날 수 있음 군집 개수나 구조에 관한 가정 없이 데이터 사이 거리를 기준으로 군집화 유도 마케팅 조사에서 소비자의 상품구매행동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립에 활용 특징 요인분석과의 차이 요인분석은 유사한 변수를 함께 묶는 목적 판별분석과의 차이 판별분석은 사전에 집단이 나뉜 자료를 통해 새로운 데이터를 기존 집단에 할당하는 것이 목적 2. 거리 군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 함 아래 각 거리 식은 교재 참고 연속형 변수의 경우 유클리디안 거리 데이터 유사성 측정할 때 많이 사용하는 거리, 통계적 개념 내포 x → 변수 산포 정도가 감안되지 않음 표준화 거리 해당변수 표준편차로 척도 변환 후, 유클리드안 거리를 계산하는 방법 표준화하게 되면 척도 차이, 분산 차이로 인한 왜곡을 피할 수 있음 마할라노비스 거리 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리 두 백터 사이 거리를 산포를 의미하는 표본공분산으로 나눠주어야 함 그룹에 관한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란 체비셰프 거리 맨하탄 거리 유클리디안 거리와 함께 가장 많이 사용되는 거리 맨하탄 도시 건물에서 건물을 가기 위한 최단 거리를 구하기 위해 고안 캔버라 거리 민코우스키 거리 맨하탄 거리와 유클리디안 거리를 한 번에 표현한 공식 L1 거리(맨하탄거리), L2 거리(유클리디안 거리)라고 불림 범주형 변수의 경우 자카드 거리 자카드 계수 코사인 거리 유사도 기준으로 문서를 분류/그룹핑할 때 유용하게 사용 코사인 유사도 두 개체 백터 내적의 코사인 값을 이용하여 측정된 백터간의 유사한 정도 3. 계층적 군집분석 n개의 군집으로 시작해 점차 군집 개수를 줄여가는 방법 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있음 최단연결법(single linkage, nearest neighbor) n*n 거리행렬에서 거리가 가장 가까운 데이터를 묶어서 군집 형성 군집과 군집 또는 데이터와의 거리 계산 시, 최단거리(min)를 거리로 계산하여 거리행렬 수정 진행 수정된 거리행렬에서 거리가 가까운 데이터/군집을 새로운 군집으로 형성 최장연결법(complete linkage, farthest neighbor) 군집과 군집/데이터와 거리 계산 시, 최장거리(max)를 거리로 계산하여 거리행렬을 수정하는 방법 평균연결법(average linkage) 군집과 군집/데이터와 거리 계산 시, 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법 와드연결법(ward linkage) 군집 내 편차들의 제곱합을 고려한 방법 군집간 정보 손실 최소화를 위해 군집화 진행 군집화 거리행렬을 통해 가장 가까운 거리의 객체들간 관계를 규명하고 덴드로그램을 그림 덴드로그램을 보고 군집 개수를 변화해가며 적절한 군집 수 선정 군집 수는 분석 목적에 따라 선정할 수 있지만, 5개 이상은 잘 활용하지 않음 군집화 단계 거리행렬을 기준으로 덴드로그램을 그림 덴드로그램 최상단부터 세로축 개수에 따라 가로선을 그어 군집 개수 선택 각 객체 구성을 고려하여 적절한 군집 수 선정 4. 비계층적 군집분석n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것 K-평균 군집분석의 개념 주어진 데이터를 k개의 클러스터로 묶는 알고리즘 각 클러스터와 거리 차이 분산을 최소화하는 방식으로 동작 K-평균 군집분석 과정 원하는 군집 개수와 초기 값(seed)을 정해 seed 중심으로 군집 형성 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류 각 군집의 seed 값을 다시 계산 모든 개체가 군집으로 할당될 때까지 위 과정 반복 K-평균 군집분석 특징 거리 계산을 통해 군집화가 이루어지므로 연속형 변수에 활용 가능 K개의 초기 중심값은 임의 선택 가능하며 가급적이면 멀리 떨어지는 것이 바람직함 초기 중심값을 임의로 선택할 때, 일렬로 선택하면 군집이 혼합되지 않고 층으로 나눠질 수 있어 주의해야 함 초기 중심값 선정에 따라 결과가 달라질 수 있음 초기 중심으로부터 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적(greedy) 알고리즘이므로 안정된 군집은 보장하나 최적이라는 보장은 없음 장점 단점 알고리즘 단순, 수행 빠름 → 분석 방법 적용 용이, 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음, 내부 구조에 대한 사전정보 없이 의미 있는 자료구조 찾을 수 있음, 다양한 형태 데이터에 적용 가능 군집 수, 가중치와 거리 정의가 어려움, 사전에 주어진 목적이 없으므로 결과 해석이 어려움, 잡음이나 이상값의 영향을 많이 받음, 볼록한 형태가 아닌 군집이 존재할 경우에는 성능 떨어짐, 초기 군집 수 결정이 어려움 5. 혼합 분포 군집(mixture distribution clustering) 개요 모형 기반 군집 방법 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정 하에서 모수와 함께 가중치를 자료로부터 추정하는 방법 사용 K개의 각 모형은 군집을 의미, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류가 이루어짐 혼합모형에서 모수와 가중치의 추정(최대가능도 추정)에는 EM 알고리즘이 사용됨 혼합 분포모형으로 설명할 수 있는 데이터 형태 자료의 분포형태가 다봉형의 형태 교재 참고 EM(Expectation-Maximization) 알고리즘의 진행 과정 각 자료에 대해 Z의 조건부분포(어느 집단에 속할지에 관한)로부터 조건부 기댓값을 구할 수 있음 관측변수 X와 잠재변수 Z를 포함하는 (X,Z)에 대한 로그-가능도함수에 Z 대신 상수값인 Z의 조건부 기댓값을 대입하면 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있음, (M-단계) 갱신된 모수 추정치에 위 과정을 반복하면 수렴하는 값을 얻게 되고 이는 최대 가능도 추정치로 사용될 수 있음 E-단계: 잠재변수 Z의 기대치 계산 M-단계: 잠재변수 Z의 기대치를 이용하여 파라미터 추정 혼합 분포 군집모형의 트깅 K-평균군집 절차와 유사하지만 확률분포를 도입하여 군집 수행 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있음 EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있음 군집 크기가 너무 작으면 추정 정도가 떨어지거나 어려울 수 있음 K-평균군집과 같이 이상치 자료에 민감 → 사전 조치 필요 6. SOM(Self Organizing Map) SOM 자가조직화지도 알고리즘은 코호넨에 의해 제시, 개발 → 코호넨 맵(Ko-honen Maps)이라고도 알려져 있음 SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도 형태로 형상화 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징이 있음 → 실제 공간의 입력변수가 가까이 있으면 지도상에도 가까운 위치에 있음 구성 SOM 모델은 두 개의 인공신경망 층으로 구성되어 있음 입력층(Input layer, 입력벡터를 받는 층) 입력변수 개수와 뉴런 수가 동일하게 존재 입력층 자료는 학습을 통해 경쟁층에 정렬되는데, 이를 지도라 부름 입력층에 있는 각각의 뉴런은 경쟁층에 있는 각각의 뉴런과 완전 연결(fully connected)되어 있음 경쟁층(Competitive layer, 2차원 격자(grid)로 구성된 층) 입력백터 특성에 따라 백터가 한 점으로 클러스터링 되는 층 SOM은 경쟁 학습으로 각각 뉴런이 입력백터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하여 학습 위 과정을 거치며 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨 입력층 표본 백터에 가장 가까운 프로토타입 백터를 BMU(Best-Matching-Unit)라고 하며, 코호넨 승자 독점의 학습 규칙에 따라 위상학적 이웃(topological neighbors)에 대한 연결 강도를 조정 승자 독식 구조로 인해 경쟁층에는 승자 뉴런만이 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열됨 특징 고차원의 데이터를 저차원의 지도 형태로 형상화 → 시각적으로 이해가 쉬움 입력변수의 위치 관계를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가깝게 표현 → 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보임 역전파(Back Propagation): 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스를 사용함으로써 속도가 매우 빠름 → 실시간 학습처리를 할 수 있는 모형 SOM과 신경망 모형의 차이점 구분 신경망 모형 SOM 학습 방법 오차역전파법 경쟁학습방법 구성 입력층, 은닉층, 출력층 입력층, 2차원 격자 형태의 경쟁층 기계 학습 방법의 분류 지도학습(Supervised Learning) 비지도학습(Unsupervised Learning) ###7. 최신 군집분석 기법 교재 참고 6절. 연관분석1. 연관규칙 연관규칙분석(Association Analysis)의 개념 흔히 장바구니분석 또는 서열분석이라고 불림 기업 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건 사이 규칙을 발견하기 위해 적용 장바구니 분석: 장바구니에 무엇이 같이 들어 있는지에 관한 분석 서열 분석: ‘A를 구매한 다음에 B를 구매한다’ 연관규칙의 형태 조건과 반응의 형태(if-then)로 이루어져 있음12345(Item set A) → (Item set B)If A then B: 만일 A가 일어나면 B가 일어난다.- 아메리카노를 마시는 손님 중 10%가 브라우니를 먹는다.- 샌드위치를 먹는 고객의 30%가 탄산수를 함께 마신다. 연관규칙의 측도 산업 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 함 지지도(support) 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율 신뢰도(confidence) 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률 → 연관성 정도 파악 가능 향상도(Lift) A가 구매되지 않았을 때, 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률 증가 비 연관규칙 A → B는 품목 A와 품목 B의 구매가 서로 관련 없는 경우에 향상도가 1이 됨 연관규칙의 절차 최소 지지도보다 큰 집합만을 대상으로 높은 지지도를 갖는 품목 집합을 찾는 것 처음에는 5%로 잡고 규칙이 충분히 도출되는지 보고 다양하게 조절하여 시도 처음부터 너무 낮은 최소 지지도를 선정하는 것은 많은 리소스가 소모 절차 최소 지지도 결정 → 품목 중 최소 지지도 넘는 품목 분류 → 2가지 품목 집합 생성 → 반복 수행해 빈발품목 집함 찾기 연관규칙의 장단점 장점 단점 조건 반응으로 표현되는 연관선 분석 결과를 쉽게 이해할 수 있음(탐색적 방법), 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없어 유용하게 활용, 사용이 편리한 분석 데이터 형태로 거래 내용에 관한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 가짐, 분석을 위한 계산이 간단함 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어남,세분화한 품목을 갖고 연관성 규칙을 찾으면 의미 없는 분석이 될 수 있음, 거래량 적은 품목은 당연히 포함된 거래수가 적을 것이고 규칙 발견 시 제외하기가 쉬움 순차패턴(Sequence Analysis) 동시에 구매될 가능성이 큰 상품군을 찾는 연관성 분석에, 시간이라는 개념을 포함해 순차적으로 구매 가능성이 큰 상품군을 찾는 것 연관성분석에서의 데이터 형태에서 각각의 고객으로부터 발생한 구매시점에 대한 정보가 포함됨 2. 기존 연관성분석의 이슈 대용량 데이터에 관한 연관성 분석 불가능 시간이 많이 걸리거나 기존 시스템에서 실행 시, 시스템 다운 현상 발생 가능 3. 최근 연관성분성 동향 1세대 알고리즘인 Apriori나 2세대인 FP-Growth에서 발전하여 3세대의 FPV를 이용해 메모리를 효율적으로 사용 → SKU 레벨인 연관성분석을 성공적으로 적용 거래내역에 포함된 모든 품목 개수가 n개일 때, 품목의 전체집합에서 추출할 수 있는 품목 부분집합 개수는 2^n^-1(공집합 제외)개, 가능한 모든 연관규칙 개서는 3^n^ - 2^n+1^ + 1개 Aprirori: 모든 가능한 품목 부분집합 개수를 줄이는 방식으로 작동 FP-Growth: 거래내역 안에 포함된 품목 개수를 줄여 비교하는 횟수를 줄이는 방식으로 작동 Aprirori 알고리즘 빈발항목집합: 최소 지지도보다 큰 지지도 값을 갖는 품목 집합 모든 품목집합에 대한 지지도를 전부 계산하지 않고, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것 1994년에 발표된 알고리즘으로 구현과 이해가 쉬우나, 지지도 낮은 후보 집합 생성 시 아이템 개수가 많아지면 계산 복잡도가 증가하는 문제 발생 FP-Growth 알고리즘 후보 빈발항목집합을 생성하지 않고, FP-Tree(Frequent Pattern Tree)를 만든 후 분할정복 방식으로 Apriori 알고리즘보다 더 빠르게 빈발항목집합을 추출할 수 있는 방법 Apriori 알고리즘의 약점을 보안하기 위해 고안 → 데이터베이스 스캔 횟수가 작고 빠르게 분석 가능 4. 연관성분석 활용방안 장바구니 분석의 경우는 실시간 상품추천을 통한 교차판매에 응용 순차패턴 분석은 A를 구매한 사람인데 B를 구매하지 않은 경우, B를 추천하는 교차판매 캠페인에 사용 5. 연관성분석 예제 교재 참고","link":"/2020/11/22/Part03_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_2/"}],"tags":[{"name":"ADsP","slug":"ADsP","link":"/tags/ADsP/"},{"name":"ADSP자격증","slug":"ADSP자격증","link":"/tags/ADSP%EC%9E%90%EA%B2%A9%EC%A6%9D/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"}],"categories":[]}